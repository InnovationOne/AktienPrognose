{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1812cd1-c21b-4e85-99d2-d3f2b16b6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step0-setup]\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- Reproduzierbares Setup, Importe, Logging, Pfade ---\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SciPy / Stats für Signifikanztests\n",
    "\n",
    "try:\n",
    "    from statsmodels.stats.contingency_tables import mcnemar as sm_mcnemar\n",
    "except Exception:\n",
    "    sm_mcnemar = None  # Fallback später\n",
    "try:\n",
    "    from scipy import stats\n",
    "except Exception:\n",
    "    stats = None  # Fallback später\n",
    "\n",
    "# YAML für Feature-Set-Config\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None  # Falls nicht vorhanden, nur Heuristik verwenden\n",
    "\n",
    "# Sklearn Metriken (nur Funktionen, KEINE seaborn)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# --- Seeds (deterministisch soweit möglich) ---\n",
    "\n",
    "GLOBAL_SEED: int = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# --- Logging konfigurieren ---\n",
    "\n",
    "LOG_FMT = \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FMT)\n",
    "logger = logging.getLogger(\"baselines10\")\n",
    "\n",
    "# --- Artefakt-Pfade einrichten ---\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        # Bevorzugt: Projekt hat 'config' und 'src' am Root\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "        # Fallback: es existiert 'artifacts/data' (z. B. bei deinem Pfad)\n",
    "        if (p / \"artifacts\" / \"data\").exists():\n",
    "            return p\n",
    "    raise AssertionError(\"Project root not found – expected 'config'+'src' or 'artifacts/data' somewhere above.\")\n",
    "\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "DATA_DIR = ARTIFACTS / \"data\"\n",
    "CONF_DIR = ARTIFACTS / \"config\"\n",
    "FORECASTS_DIR = ARTIFACTS / \"forecasts\"\n",
    "METRICS_DIR = ARTIFACTS / \"metrics\"\n",
    "REPORTS_DIR = ARTIFACTS / \"reports\"\n",
    "\n",
    "for p in [DATA_DIR, CONF_DIR, FORECASTS_DIR, METRICS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Feste Zeitfenster (Train/Test) ---\n",
    "\n",
    "TRAIN_START = pd.Timestamp(\"2009-02-28\")\n",
    "TRAIN_END = pd.Timestamp(\"2019-12-31\")\n",
    "TEST_START = pd.Timestamp(\"2020-01-31\")\n",
    "TEST_END = pd.Timestamp(\"2025-05-31\")\n",
    "\n",
    "# --- Feste Vorgaben ---\n",
    "\n",
    "HORIZON_MONTHS = 1  # 1-Schritt in die Zukunft (t -> t+1)\n",
    "VAL_WINDOW = 12  # Validierungsfenster 12 Monate\n",
    "EMBARGO = 1  # Embargo 1 Monat\n",
    "N_FOLDS = 5  # Expanding 5-Fold\n",
    "\n",
    "# --- Eingänge ---\n",
    "\n",
    "FEATURES_PARQUET = DATA_DIR / \"features_monthly.parquet\"\n",
    "RAW_PARQUET = DATA_DIR / \"raw_data.parquet\"\n",
    "FEATURE_GROUPS_YAML = CONF_DIR / \"feature_groups.yaml\"\n",
    "\n",
    "# --- Modell-/Feature-Set-Listen ---\n",
    "\n",
    "BASELINE_MODELS = [\"always_up\", \"majority_class\", \"persistence\", \"sma_crossover\"]\n",
    "FEATURE_SETS = [\"TECH\", \"MACRO\", \"INTEGRATED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc466db7-2f80-41e7-b542-d6db7846a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 13:59:10,662 | INFO | Feature-Sets heuristisch bestimmt: TECH=5, MACRO=9, INTEGRATED=14\n",
      "2025-08-24 13:59:10,664 | INFO | Feature-Gruppen gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\config\\feature_groups.yaml\n",
      "2025-08-24 13:59:10,666 | INFO | Train 2009-02-28 → 2019-12-31 | n=131\n",
      "2025-08-24 13:59:10,666 | INFO | Test  2020-01-31 → 2025-05-31 | n=65\n"
     ]
    }
   ],
   "source": [
    "# %% [step1-daten_und_featuresets]\n",
    "\n",
    "# Daten laden, Spalten prüfen, Feature-Sets laden/erstellen (YAML oder Heuristik)\n",
    "\n",
    "def _ensure_datetime_index(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Sichert, dass ein DatetimeIndex vorliegt (sonst Fehler).\"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(f\"{name} benötigt einen DatetimeIndex.\")\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def _read_parquet_safely(path: Path, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Robustes Laden von Parquet mit klaren Fehlermeldungen.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Fehlende Datei: {path} ({name})\")\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fehler beim Laden von {path}: {e}\") from e\n",
    "    return _ensure_datetime_index(df, name)\n",
    "\n",
    "def _infer_price_column(df_raw: pd.DataFrame) -> str:\n",
    "    \"\"\"Heuristik zur Wahl der Preis-Spalte (S&P 500 o.ä.), bevorzugt GSPC/SP500/adjclose.\"\"\"\n",
    "    candidates_priority = [\n",
    "        \"SP500\",\n",
    "        \"^GSPC_adjclose\",\n",
    "        \"^GSPC\",\n",
    "        \"GSPC\",\n",
    "        \"sp500\",\n",
    "        \"S&P500\",\n",
    "        \"Adj Close\",\n",
    "        \"Adj_Close\",\n",
    "        \"Close\",\n",
    "        \"close\",\n",
    "    ]\n",
    "    # Direkte Treffer\n",
    "    for c in candidates_priority:\n",
    "        if c in df_raw.columns:\n",
    "            return c\n",
    "    # Heuristik: größte Autokorrelation (Preis-Level), numerische Spalten\n",
    "    num_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "    if not num_cols:\n",
    "        raise RuntimeError(\"Keine numerischen Spalten in raw_data gefunden.\")\n",
    "    ac_scores = {}\n",
    "    for c in num_cols:\n",
    "        s = df_raw[c].dropna()\n",
    "        if len(s) < 24:\n",
    "            continue\n",
    "        ac = s.autocorr(lag=1)\n",
    "        ac_scores[c] = ac if not np.isnan(ac) else -999.0\n",
    "    if not ac_scores:\n",
    "        raise RuntimeError(\"Konnte keine geeignete Preis-Spalte heuristisch bestimmen.\")\n",
    "    best = max(ac_scores, key=ac_scores.get)\n",
    "    logger.warning(f\"Preisspalte heuristisch gewählt: {best}\")\n",
    "    return best\n",
    "\n",
    "def _load_or_infer_feature_sets(df_features: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"Lädt TECH/MACRO aus YAML, oder leitet sie heuristisch aus Spaltennamen ab.\"\"\"\n",
    "    target_cols = {\"y_direction_next\", \"y_return_next_pct\"}\n",
    "    available = [c for c in df_features.columns if c not in target_cols]\n",
    "\n",
    "    # YAML vorhanden?\n",
    "    if FEATURE_GROUPS_YAML.exists() and yaml is not None:\n",
    "        try:\n",
    "            with open(FEATURE_GROUPS_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = yaml.safe_load(f) or {}\n",
    "            tech = cfg.get(\"TECH_FEATURES\", [])\n",
    "            macro = cfg.get(\"MACRO_FEATURES\", [])\n",
    "            # Plausibilisierung\n",
    "            assert isinstance(tech, list) and isinstance(macro, list)\n",
    "            # Filter auf vorhandene\n",
    "            tech = [c for c in tech if c in available]\n",
    "            macro = [c for c in macro if c in available and c not in tech]\n",
    "            logger.info(f\"Feature-Sets aus YAML geladen: TECH={len(tech)}, MACRO={len(macro)}\")\n",
    "            if not tech or not macro:\n",
    "                logger.warning(\"YAML unvollständig – ergänze per Heuristik.\")\n",
    "                raise AssertionError\n",
    "            return {\"TECH\": tech, \"MACRO\": macro, \"INTEGRATED\": sorted(set(tech + macro))}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Feature-Gruppen YAML ungültig/fehlend, nutze Heuristik: {e}\")\n",
    "\n",
    "    # Heuristik: TECH Indikatoren vs. MACRO Indikatoren\n",
    "    tech_patterns = [\"SMA\", \"EMA\", \"MA\", \"Momentum\", \"Mom\", \"Volatility\", \"Vol\", \"Return_Lag\", \"RSI\", \"MACD\", \"Bollinger\", \"BB\", \"ATR\", \"Stoch\"]\n",
    "    macro_patterns = [\"CPI\", \"Inflat\", \"Unemployment\", \"VIX\", \"EPU\", \"FSI\", \"Fed\", \"Funds\", \"Delta\", \"USD\", \"EUR\", \"WTI\", \"Gold\", \"oil\", \"Brent\", \"DGS\", \"Yield\", \"Rate\"]\n",
    "\n",
    "    tech: List[str] = []\n",
    "    macro: List[str] = []\n",
    "    for c in available:\n",
    "        uc = c.upper()\n",
    "        if any(pat.upper() in uc for pat in tech_patterns):\n",
    "            tech.append(c)\n",
    "        elif any(pat.upper() in uc for pat in macro_patterns):\n",
    "            macro.append(c)\n",
    "        else:\n",
    "            # Unklassifizierte eher zu MACRO (konservativ)\n",
    "            macro.append(c)\n",
    "\n",
    "    # Deduplizieren & Plausibilisierung\n",
    "    tech = sorted(set(tech))\n",
    "    macro = [c for c in sorted(set(macro)) if c not in tech]\n",
    "    if not tech:\n",
    "        # Fallback: nimm ein paar offensichtliche technische von den ersten Spalten\n",
    "        tech = [c for c in available if \"SMA\" in c or \"Mom\" in c or \"Vol\" in c][:5]\n",
    "    if not macro:\n",
    "        macro = [c for c in available if c not in tech]\n",
    "\n",
    "    integ = sorted(set(tech + macro))\n",
    "    logger.info(f\"Feature-Sets heuristisch bestimmt: TECH={len(tech)}, MACRO={len(macro)}, INTEGRATED={len(integ)}\")\n",
    "\n",
    "    # Speichern (für Reproduzierbarkeit)\n",
    "    if yaml is not None:\n",
    "        try:\n",
    "            with open(FEATURE_GROUPS_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "                yaml.safe_dump(\n",
    "                    {\"TECH_FEATURES\": tech, \"MACRO_FEATURES\": macro},\n",
    "                    f,\n",
    "                    sort_keys=False,\n",
    "                    allow_unicode=True,\n",
    "                )\n",
    "            logger.info(f\"Feature-Gruppen gespeichert: {FEATURE_GROUPS_YAML}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Konnte Feature-Gruppen nicht speichern: {e}\")\n",
    "\n",
    "    return {\"TECH\": tech, \"MACRO\": macro, \"INTEGRATED\": integ}\n",
    "\n",
    "# Laden der Artefakte\n",
    "\n",
    "df_features: pd.DataFrame = _read_parquet_safely(FEATURES_PARQUET, \"features_monthly\")\n",
    "df_raw: pd.DataFrame = _read_parquet_safely(RAW_PARQUET, \"raw_data\")\n",
    "\n",
    "# Targets prüfen\n",
    "\n",
    "required_targets = [\"y_direction_next\", \"y_return_next_pct\"]\n",
    "missing_targets = [c for c in required_targets if c not in df_features.columns]\n",
    "if missing_targets:\n",
    "    raise KeyError(f\"Zielspalten fehlen in features_monthly: {missing_targets}\")\n",
    "\n",
    "# Preisreihe bestimmen\n",
    "\n",
    "price_col = _infer_price_column(df_raw)\n",
    "s_price = df_raw[price_col].astype(float)\n",
    "\n",
    "# Auf gemeinsame Monatsachse beschränken\n",
    "\n",
    "common_idx = df_features.index.intersection(s_price.index)\n",
    "df_features = df_features.loc[common_idx].copy()\n",
    "s_price = s_price.loc[common_idx].copy()\n",
    "\n",
    "# Feature-Sets laden/ableiten\n",
    "\n",
    "FEATURE_GROUPS = _load_or_infer_feature_sets(df_features)\n",
    "\n",
    "# Indexe für Train/Test schneiden (inklusive)\n",
    "\n",
    "df_train = df_features.loc[(df_features.index >= TRAIN_START) & (df_features.index <= TRAIN_END)].copy()\n",
    "df_test = df_features.loc[(df_features.index >= TEST_START) & (df_features.index <= TEST_END)].copy()\n",
    "s_price_train = s_price.loc[df_train.index]\n",
    "s_price_test = s_price.loc[df_test.index]\n",
    "\n",
    "logger.info(f\"Train {df_train.index.min().date()} → {df_train.index.max().date()} | n={len(df_train)}\")\n",
    "logger.info(f\"Test  {df_test.index.min().date()} → {df_test.index.max().date()} | n={len(df_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9297befe-6a4e-47ce-be45-3cbb0c55269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 13:59:10,673 | INFO | CV-Folds (Train/Val Längen): 66/12, 79/12, 92/12, 105/12, 118/12\n"
     ]
    }
   ],
   "source": [
    "# %% [step2-tscv_splitter]\n",
    "\n",
    "# Expanding TSCV mit Validierungsfenster=12 und Embargo=1\n",
    "\n",
    "def generate_expanding_cv_indices(\n",
    "    idx: pd.DatetimeIndex,\n",
    "    n_folds: int = 5,\n",
    "    val_len: int = 12,\n",
    "    embargo: int = 1,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Erzeuge Expanding CV-Folds (ohne Überlappung der Validierung), rückwärts geplant:\n",
    "    - Letzter Fold endet am letzten Trainingsmonat und umfasst val_len Monate.\n",
    "    - Davor: Lücke von 'embargo' Monaten, dann wieder val_len Monate, etc.\n",
    "    - Train-Fenster: vom ersten bis zum Anfang der Val-Periode minus Embargo.\n",
    "    Returns: Liste von (train_idx_positions, val_idx_positions).\n",
    "    \"\"\"\n",
    "    n = len(idx)\n",
    "    if n < (n_folds * val_len + (n_folds - 1) * embargo + 24):\n",
    "        logger.warning(\"Trainingsfenster ist knapp; Folds könnten sehr kurze Train-Segmente ergeben.\")\n",
    "\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    end_pos = n - 1\n",
    "    for _ in range(n_folds):\n",
    "        val_start = end_pos - val_len + 1\n",
    "        val_end = end_pos\n",
    "        if val_start < 0:\n",
    "            break\n",
    "        train_end = val_start - embargo - 1\n",
    "        train_start = 0\n",
    "        if train_end - train_start + 1 < 12:  # minimal 12 Monate Training\n",
    "            break\n",
    "        train_idx = np.arange(train_start, train_end + 1, dtype=int)\n",
    "        val_idx = np.arange(val_start, val_end + 1, dtype=int)\n",
    "        folds.append((train_idx, val_idx))\n",
    "        # Nächste Val endet vor Embargo und aktueller Val\n",
    "        end_pos = val_start - embargo - 1\n",
    "        if end_pos < val_len - 1:\n",
    "            break\n",
    "\n",
    "    folds = list(reversed(folds))  # zeitlich aufsteigend zurückgeben\n",
    "    if len(folds) < n_folds:\n",
    "        logger.warning(f\"Nur {len(folds)} Folds erzeugt (gewünscht: {n_folds}).\")\n",
    "    return folds\n",
    "\n",
    "# Test-Folds auf Trainingsindex erstellen (nur Indizes)\n",
    "\n",
    "TRAIN_IDX = df_train.index\n",
    "CV_FOLDS = generate_expanding_cv_indices(TRAIN_IDX, n_folds=N_FOLDS, val_len=VAL_WINDOW, embargo=EMBARGO)\n",
    "logger.info(\"CV-Folds (Train/Val Längen): \" + \", \".join([f\"{len(tr)}/{len(v)}\" for tr, v in CV_FOLDS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8729c8be-bd25-43a2-a884-d013e41a9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step3-utils_metrics_tests]\n",
    "\n",
    "# Utils: Renditen, Buy&Hold, Metriken, DM/McNemar, Bootstrap (optional)\n",
    "\n",
    "def monthly_returns_pct(price: pd.Series) -> pd.Series:\n",
    "    \"\"\"Einfache Monatsrendite in %.\"\"\"\n",
    "    r = 100.0 * (price / price.shift(1) - 1.0)\n",
    "    return r\n",
    "\n",
    "def strategy_returns_long_cash(\n",
    "    pred_dir: pd.Series,  # 0/1 pro Monat (Entscheidung am Monatsende t)\n",
    "    next_month_returns_pct: pd.Series,  # y_return_next_pct (Rendite des Folgemonats in %)\n",
    ") -> pd.Series:\n",
    "    \"\"\"Long/Cash-Strategie: bei 1 investiert, sonst 0% Rendite; Kosten=0.\"\"\"\n",
    "    # Index-Ausrichtung: pred_dir und next_month_returns_pct beide auf gleiche Achse (Monat t).\n",
    "    pred = pred_dir.astype(float).reindex(next_month_returns_pct.index).fillna(0.0)\n",
    "    ret = (pred * next_month_returns_pct).astype(float)\n",
    "    return ret\n",
    "\n",
    "def safe_std(a: np.ndarray, ddof: int = 1) -> float:\n",
    "    \"\"\"Numerisch robuste Standardabweichung (0, wenn <2 Beobachtungen).\"\"\"\n",
    "    if a.size < 2:\n",
    "        return 0.0\n",
    "    return float(np.std(a, ddof=ddof))\n",
    "\n",
    "def sharpe_ratio(returns_pct: pd.Series) -> float:\n",
    "    \"\"\"Sharpe (monatlich, risikofrei ~0 angenommen).\"\"\"\n",
    "    r = returns_pct.dropna().values\n",
    "    s = safe_std(r, ddof=1)\n",
    "    if s == 0.0:\n",
    "        return np.nan\n",
    "    return float(np.mean(r) / s)\n",
    "\n",
    "def sortino_ratio(returns_pct: pd.Series) -> float:\n",
    "    \"\"\"Sortino (Downside-Std als Nenner).\"\"\"\n",
    "    r = returns_pct.dropna().values\n",
    "    downside = r[r < 0.0]\n",
    "    ds = safe_std(downside, ddof=1) if downside.size >= 2 else 0.0\n",
    "    if ds == 0.0:\n",
    "        return np.nan\n",
    "    return float(np.mean(r) / ds)\n",
    "\n",
    "def rachev_ratio(returns_pct: pd.Series, alpha: float = 0.05) -> float:\n",
    "    \"\"\"Rachev-Ratio ≈ VaR-Gewinnquantil / |VaR-Verlustquantil| (einfacher Proxy).\"\"\"\n",
    "    r = returns_pct.dropna().values\n",
    "    if r.size < 10:\n",
    "        return np.nan\n",
    "    top = np.quantile(r, 1.0 - alpha)\n",
    "    bot = np.quantile(r, alpha)\n",
    "    denom = abs(bot) if bot != 0 else np.nan\n",
    "    if denom == 0 or np.isnan(denom):\n",
    "        return np.nan\n",
    "    return float(top / denom)\n",
    "\n",
    "def diebold_mariano(\n",
    "    loss_a: np.ndarray,\n",
    "    loss_b: np.ndarray,\n",
    "    h: int = 1,\n",
    "    alternative: str = \"two_sided\",\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Einfache DM-Implementierung (Newey-West Varianz, Lag=h-1).\n",
    "    loss_a, loss_b: Verlustreihen gleicher Länge (z.B. 0/1-Loss).\n",
    "    Returns: (DM-Stat, p-Wert)\n",
    "    \"\"\"\n",
    "    if loss_a.shape[0] != loss_b.shape[0]:\n",
    "        raise ValueError(\"DM: Verlustreihen ungleicher Länge.\")\n",
    "    d = loss_a - loss_b\n",
    "    T = d.shape[0]\n",
    "    if T < 10:\n",
    "        return (np.nan, np.nan)\n",
    "    d_bar = float(np.mean(d))\n",
    "\n",
    "    # Newey-West Varianzschätzer mit Lag = h-1 (bei h=1 -> 0)\n",
    "    q = max(h - 1, 0)\n",
    "    gamma0 = float(np.var(d, ddof=1))\n",
    "    s = gamma0\n",
    "    for lag in range(1, q + 1):\n",
    "        cov = float(np.cov(d[:-lag], d[lag:], ddof=1)[0, 1])\n",
    "        w = 1.0 - lag / (q + 1)\n",
    "        s += 2.0 * w * cov\n",
    "    var_hat = s / T\n",
    "    if var_hat <= 0:\n",
    "        return (np.nan, np.nan)\n",
    "    dm_stat = d_bar / math.sqrt(var_hat)\n",
    "\n",
    "    # p-Wert\n",
    "    if stats is None:\n",
    "        return (dm_stat, np.nan)\n",
    "    if alternative == \"two_sided\":\n",
    "        pval = 2.0 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "    elif alternative == \"greater\":\n",
    "        pval = 1.0 - stats.norm.cdf(dm_stat)\n",
    "    else:  # \"less\"\n",
    "        pval = stats.norm.cdf(dm_stat)\n",
    "    return (float(dm_stat), float(pval))\n",
    "\n",
    "def mcnemar_test(\n",
    "    y_true: Sequence[int], y_pred_a: Sequence[int], y_pred_b: Sequence[int]\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    McNemar-Test für gepaarte binäre Klassifikatoren.\n",
    "    Gibt (stat, p) zurück (mit Kontinuitätskorrektur, falls statsmodels verfügbar).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    a = (np.asarray(y_pred_a).astype(int) == y_true).astype(int)\n",
    "    b = (np.asarray(y_pred_b).astype(int) == y_true).astype(int)\n",
    "    # Kontingenztafel: b01 (a falsch, b richtig), b10 (a richtig, b falsch)\n",
    "    b01 = int(((a == 0) & (b == 1)).sum())\n",
    "    b10 = int(((a == 1) & (b == 0)).sum())\n",
    "\n",
    "    # statsmodels bevorzugt, sonst Approximation\n",
    "    if sm_mcnemar is not None:\n",
    "        table = np.array([[0, b01], [b10, 0]], dtype=int)\n",
    "        try:\n",
    "            res = sm_mcnemar(table, exact=False, correction=True)\n",
    "            return float(res.statistic), float(res.pvalue)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Approx. Chi^2 mit Kontinuitätskorrektur\n",
    "    if b01 + b10 == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    stat = (abs(b01 - b10) - 1) ** 2 / (b01 + b10)\n",
    "    if stats is None:\n",
    "        return (float(stat), np.nan)\n",
    "    p = 1.0 - stats.chi2.cdf(stat, df=1)\n",
    "    return (float(stat), float(p))\n",
    "\n",
    "def plot_cumulative_returns(curves: Dict[str, pd.Series], out_path: Path) -> None:\n",
    "    \"\"\"Kumulierte Renditekurven (eine Figure).\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, s in curves.items():\n",
    "        base = (1.0 + s.fillna(0.0) / 100.0).cumprod()\n",
    "        plt.plot(base.index, base.values, label=name)\n",
    "    plt.title(\"Kumulierte Renditen – Strategien vs. Buy&Hold\")\n",
    "    plt.xlabel(\"Datum\")\n",
    "    plt.ylabel(\"Wachstumsfaktor\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_metric_bars(metrics_df: pd.DataFrame, metric_cols: List[str], out_path: Path) -> None:\n",
    "    \"\"\"Balkenvergleich ausgewählter Metriken über Modelle×Feature-Set (eine Figure).\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Einfache Aggregation: Mittel über Feature-Sets pro Modell (oder nutze alle Einträge)\n",
    "    df_plot = metrics_df.copy()\n",
    "    df_plot[\"label\"] = df_plot[\"model\"] + \" (\" + df_plot[\"feature_set\"] + \")\"\n",
    "    x = np.arange(len(df_plot[\"label\"]))\n",
    "    width = 0.8 / len(metric_cols)\n",
    "    for i, m in enumerate(metric_cols):\n",
    "        vals = df_plot[m].astype(float).values\n",
    "        plt.bar(x + i * width, vals, width=width, label=m)\n",
    "    plt.xticks(x + width * (len(metric_cols) - 1) / 2, df_plot[\"label\"], rotation=45, ha=\"right\")\n",
    "    plt.title(\"Vergleich zentraler Metriken\")\n",
    "    plt.ylabel(\"Wert\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix_figure(cm: np.ndarray, out_path: Path, title: str = \"Confusion-Matrix\") -> None:\n",
    "    \"\"\"Einfache CM-Visualisierung ohne seaborn (eine Figure).\"\"\"\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [\"Down(0)\", \"Up(1)\"])\n",
    "    plt.yticks(tick_marks, [\"Down(0)\", \"Up(1)\"])\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{cm[i, j]:.0f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4733f79-7e45-4961-8cfd-427e03561942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 13:59:10,711 | INFO | OOF always_up: F1=0.800, AUC=0.5\n",
      "2025-08-24 13:59:10,719 | INFO | OOF majority_class: F1=0.800, AUC=nan\n",
      "2025-08-24 13:59:10,728 | INFO | OOF persistence: F1=0.723, AUC=nan\n",
      "2025-08-24 13:59:10,864 | INFO | OOF sma_crossover: F1=0.774, AUC=nan\n",
      "2025-08-24 13:59:10,872 | INFO | Global bestes SMA-Paar: fast=2, slow=9\n"
     ]
    }
   ],
   "source": [
    "# %% [step4-baselines_oof_tuning]\n",
    "\n",
    "# Baselines + TSCV-Tuning (SMA) + OOF-Threshold/Kalibrierung (falls zutreffend)\n",
    "\n",
    "@dataclass\n",
    "class OOFResult:\n",
    "    \"\"\"OOF-Ergebnisstruktur für ein Baseline-Modell.\"\"\"\n",
    "    model: str\n",
    "    feature_set: str\n",
    "    oof_index: List[pd.Timestamp]\n",
    "    y_true: List[int]\n",
    "    y_pred: List[int]\n",
    "    proba_up: List[Optional[float]]\n",
    "    details_path: Optional[Path] = None\n",
    "    threshold: Optional[float] = None\n",
    "    oof_f1: Optional[float] = None\n",
    "    oof_auc: Optional[float] = None\n",
    "\n",
    "def majority_label(y: pd.Series) -> int:\n",
    "    \"\"\"Mehrheitsklasse (0/1) aus y bestimmen (bei Gleichstand -> 1 bevorzugen).\"\"\"\n",
    "    vc = y.value_counts()\n",
    "    if 1 in vc and 0 in vc and vc[1] == vc[0]:\n",
    "        return 1\n",
    "    return int(vc.idxmax())\n",
    "\n",
    "def sma_signal(price: pd.Series, fast: int, slow: int) -> pd.Series:\n",
    "    \"\"\"SMA-Crossover-Signal am Zeitpunkt t (1 wenn SMA_fast > SMA_slow).\"\"\"\n",
    "    sma_f = price.rolling(fast, min_periods=fast).mean()\n",
    "    sma_s = price.rolling(slow, min_periods=slow).mean()\n",
    "    sig = (sma_f > sma_s).astype(int)\n",
    "    return sig\n",
    "\n",
    "def build_oof_for_baseline(\n",
    "    name: str,\n",
    "    df_tr: pd.DataFrame,\n",
    "    fs_cols: List[str],\n",
    "    s_price_tr: pd.Series,\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]],\n",
    ") -> OOFResult:\n",
    "    \"\"\"\n",
    "    Erzeuge OOF-Vorhersagen auf dem Trainingszeitraum für ein Baseline-Modell.\n",
    "    Für SMA: Tuning der (fast, slow)-Paare per F1 im CV; OOF aus best-param pro Fold.\n",
    "    Andere Baselines deterministisch ohne Probas.\n",
    "    \"\"\"\n",
    "    y_true_all: List[int] = []\n",
    "    y_pred_all: List[int] = []\n",
    "    proba_all: List[Optional[float]] = []\n",
    "    idx_all: List[pd.Timestamp] = []\n",
    "\n",
    "    # Kandidaten für SMA\n",
    "    sma_fast_grid = [2, 3, 4, 6]\n",
    "    sma_slow_grid = [9, 12, 18]\n",
    "    per_fold_params: List[Tuple[int, int, float]] = []  # (fast, slow, f1)\n",
    "\n",
    "    for fold_id, (tr_idx, val_idx) in enumerate(folds, start=1):\n",
    "        tr_dates = df_tr.index[tr_idx]\n",
    "        val_dates = df_tr.index[val_idx]\n",
    "\n",
    "        y_tr = df_tr.loc[tr_dates, \"y_direction_next\"].astype(int)\n",
    "        y_val = df_tr.loc[val_dates, \"y_direction_next\"].astype(int)\n",
    "\n",
    "        if name == \"always_up\":\n",
    "            y_pred = np.ones_like(y_val.values)\n",
    "            proba = [1.0] * len(y_val)\n",
    "        elif name == \"majority_class\":\n",
    "            maj = majority_label(y_tr)\n",
    "            y_pred = np.full_like(y_val.values, fill_value=maj)\n",
    "            proba = [np.nan] * len(y_val)\n",
    "        elif name == \"persistence\":\n",
    "            # Persistenz: nutze Rendite r_t (aktueller Monat) -> Richtung für t+1\n",
    "            # Align: Label bezieht sich auf t (Vorhersage für t+1); Prädiktor = Sign(r_t) am t\n",
    "            r = monthly_returns_pct(s_price_tr)\n",
    "            sig = (r >= 0).astype(int).reindex(val_dates).fillna(1).values\n",
    "            y_pred = sig\n",
    "            proba = [np.nan] * len(y_val)\n",
    "        elif name == \"sma_crossover\":\n",
    "            # Hyperparameter-Tuning per F1 auf Val\n",
    "            best_f, best_s, best_f1 = None, None, -1.0\n",
    "            price_subset = s_price_tr.loc[tr_dates.union(val_dates)]\n",
    "            for f in sma_fast_grid:\n",
    "                for s in sma_slow_grid:\n",
    "                    if f >= s:\n",
    "                        continue\n",
    "                    sig = sma_signal(price_subset, f, s)\n",
    "                    yhat_val = sig.reindex(val_dates).fillna(0).astype(int).values\n",
    "                    f1 = f1_score(y_val.values, yhat_val, zero_division=0)\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1 = f1\n",
    "                        best_f, best_s = f, s\n",
    "            if best_f is None:\n",
    "                best_f, best_s = 3, 12\n",
    "                best_f1 = 0.0\n",
    "            per_fold_params.append((best_f, best_s, best_f1))\n",
    "            # OOF mit bestem Paar\n",
    "            sig_best = sma_signal(s_price_tr, best_f, best_s)\n",
    "            y_pred = sig_best.reindex(val_dates).fillna(0).astype(int).values\n",
    "            proba = [np.nan] * len(y_val)\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekanntes Baseline-Modell: {name}\")\n",
    "\n",
    "        # Sammeln\n",
    "        y_true_all.extend(y_val.values.tolist())\n",
    "        y_pred_all.extend(y_pred.tolist())\n",
    "        proba_all.extend(proba)\n",
    "        idx_all.extend(val_dates.tolist())\n",
    "\n",
    "    # OOF-Metriken\n",
    "    oof_f1 = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    try:\n",
    "        # AUC nur sinnvoll mit Wahrscheinlichkeiten – hier meist NaN\n",
    "        if all(isinstance(p, float) and (0.0 <= p <= 1.0) for p in proba_all):\n",
    "            oof_auc = roc_auc_score(y_true_all, proba_all)\n",
    "        else:\n",
    "            oof_auc = np.nan\n",
    "    except Exception:\n",
    "        oof_auc = np.nan\n",
    "\n",
    "    # CV-Details speichern (nur für SMA sinnvoll)\n",
    "    details_path = None\n",
    "    if name == \"sma_crossover\":\n",
    "        details_path = METRICS_DIR / f\"cv_details_{name}.csv\"\n",
    "        pd.DataFrame(per_fold_params, columns=[\"fast\", \"slow\", \"f1\"]).to_csv(details_path, index=False)\n",
    "\n",
    "    return OOFResult(\n",
    "        model=name,\n",
    "        feature_set=\"INTERNAL\",  # OOF unabhängig vom Feature-Set\n",
    "        oof_index=idx_all,\n",
    "        y_true=y_true_all,\n",
    "        y_pred=y_pred_all,\n",
    "        proba_up=proba_all,\n",
    "        details_path=details_path,\n",
    "        threshold=0.5,\n",
    "        oof_f1=float(oof_f1),\n",
    "        oof_auc=float(oof_auc) if not np.isnan(oof_auc) else np.nan,\n",
    "    )\n",
    "\n",
    "# OOF für alle Baselines (einmal, unabhängig von Feature-Set)\n",
    "\n",
    "OOF_RESULTS: Dict[str, OOFResult] = {}\n",
    "for m in BASELINE_MODELS:\n",
    "    OOF_RESULTS[m] = build_oof_for_baseline(\n",
    "        name=m,\n",
    "        df_tr=df_train,\n",
    "        fs_cols=FEATURE_GROUPS[\"INTEGRATED\"],\n",
    "        s_price_tr=s_price_train,\n",
    "        folds=CV_FOLDS,\n",
    "    )\n",
    "    logger.info(f\"OOF {m}: F1={OOF_RESULTS[m].oof_f1:.3f}, AUC={OOF_RESULTS[m].oof_auc}\")\n",
    "\n",
    "# SMA-Gesamtbestes Paar aus OOF (mittels CV-Details ermitteln)\n",
    "\n",
    "def select_global_sma_params(s_price_tr: pd.Series) -> Tuple[int, int]:\n",
    "    \"\"\"Wähle global bestes (fast, slow) anhand OOF (mittels CV-Details); Fallback 3/12.\"\"\"\n",
    "    details_path = OOF_RESULTS[\"sma_crossover\"].details_path\n",
    "    if details_path is None or not details_path.exists():\n",
    "        return 3, 12\n",
    "    df = pd.read_csv(details_path)\n",
    "    if df.empty:\n",
    "        return 3, 12\n",
    "    # Einfach: wähle das Paar des Folds mit höchstem F1 (könnte auch Voting/Mehrheit)\n",
    "    best_row = df.iloc[df[\"f1\"].idxmax()]\n",
    "    return int(best_row[\"fast\"]), int(best_row[\"slow\"])\n",
    "\n",
    "SMA_FAST_BEST, SMA_SLOW_BEST = select_global_sma_params(s_price_train)\n",
    "logger.info(f\"Global bestes SMA-Paar: fast={SMA_FAST_BEST}, slow={SMA_SLOW_BEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edffb829-f419-4978-be96-4c17189e3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 15:06:41,002 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_always_up_TECH.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,038 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_majority_class_TECH.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,076 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_persistence_TECH.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,115 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_sma_crossover_TECH.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,132 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_always_up_MACRO.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,167 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_majority_class_MACRO.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,204 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_persistence_MACRO.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,242 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_sma_crossover_MACRO.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,260 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_always_up_INTEGRATED.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,294 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_majority_class_INTEGRATED.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,332 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_persistence_INTEGRATED.csv (65 Zeilen)\n",
      "2025-08-24 15:06:41,372 | INFO | Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\baseline_sma_crossover_INTEGRATED.csv (65 Zeilen)\n"
     ]
    }
   ],
   "source": [
    "# %% [step5-walk_forward_test]\n",
    "\n",
    "# Walk-Forward-Test (expanding origin), Vorhersage je Testmonat (Horizon=1)\n",
    "\n",
    "def run_walk_forward_baseline(\n",
    "    name: str,\n",
    "    feature_set: str,\n",
    "    df_tr: pd.DataFrame,\n",
    "    df_te: pd.DataFrame,\n",
    "    s_price_tr: pd.Series,\n",
    "    s_price_te: pd.Series,\n",
    "    horizon: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Führt Walk-Forward für ein Baseline-Modell aus.\n",
    "    Gibt DataFrame mit Schema:\n",
    "    [\"date\",\"y_true_dir\",\"y_pred_dir\",\"proba_up\",\"model\",\"model_class\",\"feature_set\",\n",
    "    \"horizon_months\",\"train_start\",\"train_end\",\"test_month\",\"seed\"]\n",
    "    zurück.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Gesamtdaten (für expanding train)\n",
    "    full_idx = df_tr.index.append(df_te.index)\n",
    "\n",
    "    # Rolling-origin: für jeden Testmonat t train bis t-1, pred für t\n",
    "    for t in df_te.index:\n",
    "        train_end = full_idx[full_idx.get_loc(t) - 1] if t in full_idx else df_tr.index.max()\n",
    "        train_slice = df_tr.loc[df_tr.index <= train_end]\n",
    "\n",
    "        # y_true am t (Richtung für t+1)\n",
    "        y_true = int(df_te.loc[t, \"y_direction_next\"])\n",
    "\n",
    "        # Vorhersage je Modell\n",
    "        if name == \"always_up\":\n",
    "            y_pred = 1\n",
    "            proba = 1.0\n",
    "        elif name == \"majority_class\":\n",
    "            maj = majority_label(train_slice[\"y_direction_next\"].astype(int))\n",
    "            y_pred = maj\n",
    "            proba = np.nan\n",
    "        elif name == \"persistence\":\n",
    "            # nutze r_t am Zeitpunkt t aus Preisreihe\n",
    "            r = monthly_returns_pct(pd.concat([s_price_tr, s_price_te]))\n",
    "            y_pred = int((r.loc[t] >= 0.0) if not pd.isna(r.loc[t]) else 1)\n",
    "            proba = np.nan\n",
    "        elif name == \"sma_crossover\":\n",
    "            sig = sma_signal(pd.concat([s_price_tr, s_price_te]), SMA_FAST_BEST, SMA_SLOW_BEST)\n",
    "            y_pred = int(sig.loc[t]) if not pd.isna(sig.loc[t]) else 0\n",
    "            proba = np.nan\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekanntes Modell {name}\")\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"date\": t,\n",
    "                \"y_true_dir\": y_true,\n",
    "                \"y_pred_dir\": y_pred,\n",
    "                \"proba_up\": proba,\n",
    "                \"model\": name,\n",
    "                \"model_class\": \"baseline\",\n",
    "                \"feature_set\": feature_set,\n",
    "                \"horizon_months\": horizon,\n",
    "                \"train_start\": df_tr.index.min(),\n",
    "                \"train_end\": train_end,\n",
    "                \"test_month\": t,\n",
    "                \"seed\": GLOBAL_SEED,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame.from_records(records).set_index(\"date\")\n",
    "    return out\n",
    "\n",
    "# Walk-Forward für alle Modelle × Feature-Sets (Baselines nutzen Features nicht, aber Schema verlangt es)\n",
    "\n",
    "ALL_FORECASTS: List[pd.DataFrame] = []\n",
    "for fs in FEATURE_SETS:\n",
    "    for m in BASELINE_MODELS:\n",
    "        fc = run_walk_forward_baseline(\n",
    "            name=m,\n",
    "            feature_set=fs,\n",
    "            df_tr=df_train,\n",
    "            df_te=df_test,\n",
    "            s_price_tr=s_price_train,\n",
    "            s_price_te=s_price_test,\n",
    "            horizon=HORIZON_MONTHS,\n",
    "        )\n",
    "        # Speichern je Modell×FS\n",
    "        out_path = FORECASTS_DIR / f\"baseline_{m}_{fs}.csv\"\n",
    "        fc.reset_index().to_csv(out_path, index=False)\n",
    "        logger.info(f\"Forecast gespeichert: {out_path} ({len(fc)} Zeilen)\")\n",
    "        ALL_FORECASTS.append(fc)\n",
    "\n",
    "# Kombinierte Vorhersagen-Tabelle\n",
    "\n",
    "df_forecasts_all = pd.concat(ALL_FORECASTS, axis=0).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75fd6530-d41e-4e98-8cfa-8a88fba4a879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,812 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_always_up_TECH.json\n",
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,819 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_majority_class_TECH.json\n",
      "2025-08-24 15:06:51,825 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_persistence_TECH.json\n",
      "2025-08-24 15:06:51,832 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_sma_crossover_TECH.json\n",
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,840 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_always_up_MACRO.json\n",
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,850 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_majority_class_MACRO.json\n",
      "2025-08-24 15:06:51,857 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_persistence_MACRO.json\n",
      "2025-08-24 15:06:51,864 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_sma_crossover_MACRO.json\n",
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,873 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_always_up_INTEGRATED.json\n",
      "C:\\Users\\gamer\\anaconda3neu\\envs\\ki-aktienprognose-env\\Lib\\site-packages\\statsmodels\\stats\\contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n",
      "2025-08-24 15:06:51,878 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_majority_class_INTEGRATED.json\n",
      "2025-08-24 15:06:51,885 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_persistence_INTEGRATED.json\n",
      "2025-08-24 15:06:51,894 | INFO | Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\baseline_sma_crossover_INTEGRATED.json\n"
     ]
    }
   ],
   "source": [
    "# %% [step6-metriken_signifikanz_json]\n",
    "\n",
    "# Metriken & Signifikanz (DM/McNemar) je Modell×FS, JSON speichern\n",
    "\n",
    "def evaluate_model_from_forecasts(\n",
    "    fc: pd.DataFrame,\n",
    "    s_return_next_test: pd.Series,\n",
    "    ref_model_name: str = \"always_up\",\n",
    "    ref_fc: Optional[pd.DataFrame] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Berechnet alle geforderten Metriken und Tests für ein Vorhersage-DF.\"\"\"\n",
    "    y_true = fc[\"y_true_dir\"].astype(int).values\n",
    "    y_pred = fc[\"y_pred_dir\"].astype(int).values\n",
    "    proba = fc[\"proba_up\"].values\n",
    "\n",
    "    # Klassifikationsmetriken\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        if np.all(np.isfinite(proba)) and np.nanmin(proba) >= 0.0 and np.nanmax(proba) <= 1.0:\n",
    "            auc = roc_auc_score(y_true, proba)\n",
    "            brier = brier_score_loss(y_true, proba)\n",
    "        else:\n",
    "            auc, brier = np.nan, np.nan\n",
    "    except Exception:\n",
    "        auc, brier = np.nan, np.nan\n",
    "\n",
    "    # Finanzmetriken (Long/Cash, Signal t → Rendite t+1)\n",
    "    sr = strategy_returns_long_cash(fc[\"y_pred_dir\"], s_return_next_test.reindex(fc.index))\n",
    "    sharpe = sharpe_ratio(sr)\n",
    "    sortino = sortino_ratio(sr)\n",
    "    rachev = rachev_ratio(sr)\n",
    "\n",
    "    # Signifikanztests vs. Always-Up (0/1-Loss)\n",
    "    if ref_fc is None:\n",
    "        dm_stat = np.nan\n",
    "        dm_p = np.nan\n",
    "        mcn_stat = np.nan\n",
    "        mcn_p = np.nan\n",
    "    else:\n",
    "        loss_model = (y_pred != y_true).astype(int)\n",
    "        y_pred_ref = ref_fc[\"y_pred_dir\"].astype(int).reindex(fc.index).fillna(1).values\n",
    "        loss_ref = (y_pred_ref != y_true).astype(int)\n",
    "        dm_stat, dm_p = diebold_mariano(loss_model, loss_ref, h=1, alternative=\"two_sided\")\n",
    "        mcn_stat, mcn_p = mcnemar_test(y_true, y_pred, y_pred_ref)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"auc\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "        \"brier\": float(brier) if not np.isnan(brier) else np.nan,\n",
    "        \"sharpe\": float(sharpe) if not np.isnan(sharpe) else np.nan,\n",
    "        \"sortino\": float(sortino) if not np.isnan(sortino) else np.nan,\n",
    "        \"rachev\": float(rachev) if not np.isnan(rachev) else np.nan,\n",
    "        \"dm_stat\": float(dm_stat) if not np.isnan(dm_stat) else np.nan,\n",
    "        \"dm_pvalue\": float(dm_p) if not np.isnan(dm_p) else np.nan,\n",
    "        \"mcnemar_stat\": float(mcn_stat) if not np.isnan(mcn_stat) else np.nan,\n",
    "        \"mcnemar_p\": float(mcn_p) if not np.isnan(mcn_p) else np.nan,\n",
    "        \"n_obs\": int(len(fc)),\n",
    "    }\n",
    "\n",
    "# Buy&Hold (S&P) Renditen für Test\n",
    "\n",
    "s_ret_test = df_test[\"y_return_next_pct\"].astype(float)\n",
    "\n",
    "# Referenz Always-Up pro Feature-Set laden (für Tests)\n",
    "\n",
    "REF_FORECASTS: Dict[str, pd.DataFrame] = {\n",
    "    fs: df_forecasts_all[(df_forecasts_all[\"model\"] == \"always_up\") & (df_forecasts_all[\"feature_set\"] == fs)]\n",
    "    for fs in FEATURE_SETS\n",
    "}\n",
    "\n",
    "# Metriken pro Modell×FS berechnen und JSON speichern\n",
    "\n",
    "METRICS_ROWS: List[Dict[str, Any]] = []\n",
    "for fs in FEATURE_SETS:\n",
    "    ref_fc = REF_FORECASTS[fs]\n",
    "    for m in BASELINE_MODELS:\n",
    "        fc = df_forecasts_all[(df_forecasts_all[\"model\"] == m) & (df_forecasts_all[\"feature_set\"] == fs)]\n",
    "        met = evaluate_model_from_forecasts(fc, s_ret_test, ref_model_name=\"always_up\", ref_fc=ref_fc)\n",
    "\n",
    "        row = {\n",
    "            \"model\": m,\n",
    "            \"model_class\": \"baseline\",\n",
    "            \"feature_set\": fs,\n",
    "            \"task\": \"direction_1m\",\n",
    "            **met,\n",
    "            \"train_start\": str(TRAIN_START.date()),\n",
    "            \"train_end\": str(TRAIN_END.date()),\n",
    "            \"test_start\": str(TEST_START.date()),\n",
    "            \"test_end\": str(TEST_END.date()),\n",
    "            \"seed\": GLOBAL_SEED,\n",
    "            \"coef_path\": None,\n",
    "            \"permimp_path\": None,\n",
    "            \"cv_details_path\": str(OOF_RESULTS[m].details_path) if OOF_RESULTS[m].details_path else None,\n",
    "            \"threshold\": OOF_RESULTS[m].threshold,\n",
    "            \"oof_f1\": OOF_RESULTS[m].oof_f1,\n",
    "            \"oof_auc\": OOF_RESULTS[m].oof_auc,\n",
    "        }\n",
    "        METRICS_ROWS.append(row)\n",
    "\n",
    "        # JSON pro Modell×FS\n",
    "        out_json = METRICS_DIR / f\"baseline_{m}_{fs}.json\"\n",
    "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(row, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Metrics gespeichert: {out_json}\")\n",
    "\n",
    "# Gesamttabelle\n",
    "\n",
    "df_metrics = pd.DataFrame(METRICS_ROWS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c36249-7027-444b-8505-677f795852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step7-plots]\n",
    "\n",
    "# Plots: (1) Kumulierte Rendite vs. Buy&Hold, (2) Balkenvergleich Metriken, (3) aggregierte Confusion-Matrix (bestes Modell)\n",
    "\n",
    "# (1) Kumulierte Renditen – pro Feature-Set zusammen mit Buy&Hold\n",
    "\n",
    "for fs in FEATURE_SETS:\n",
    "    curves: Dict[str, pd.Series] = {}\n",
    "    # Buy&Hold: immer investiert → nutze y_return_next_pct\n",
    "    curves[\"Buy&Hold\"] = s_ret_test\n",
    "    for m in BASELINE_MODELS:\n",
    "        fc = df_forecasts_all[(df_forecasts_all[\"model\"] == m) & (df_forecasts_all[\"feature_set\"] == fs)]\n",
    "        strat = strategy_returns_long_cash(fc[\"y_pred_dir\"], s_ret_test.reindex(fc.index))\n",
    "        curves[m] = strat\n",
    "    out_path = REPORTS_DIR / f\"10_cum_returns_{fs}.png\"\n",
    "    plot_cumulative_returns(curves, out_path)\n",
    "\n",
    "# (2) Balkendiagramm zentraler Metriken (z. B. F1, Accuracy, Sharpe) – je Feature-Set\n",
    "\n",
    "for fs in FEATURE_SETS:\n",
    "    sub = df_metrics[df_metrics[\"feature_set\"] == fs].copy()\n",
    "    sub = sub.sort_values(by=[\"f1\", \"accuracy\"], ascending=False)\n",
    "    out_path = REPORTS_DIR / f\"10_metric_bars_{fs}.png\"\n",
    "    plot_metric_bars(sub, metric_cols=[\"f1\", \"accuracy\", \"sharpe\"], out_path=out_path)\n",
    "\n",
    "# (3) Aggregierte Confusion-Matrix: für bestes Modell (nach F1) über alle Feature-Sets\n",
    "\n",
    "best_row = df_metrics.sort_values(by=[\"f1\", \"accuracy\"], ascending=False).iloc[0]\n",
    "best_model = best_row[\"model\"]\n",
    "best_fs = best_row[\"feature_set\"]\n",
    "fc_best = df_forecasts_all[(df_forecasts_all[\"model\"] == best_model) & (df_forecasts_all[\"feature_set\"] == best_fs)]\n",
    "cm = confusion_matrix(fc_best[\"y_true_dir\"].astype(int), fc_best[\"y_pred_dir\"].astype(int), labels=[0, 1])\n",
    "cm_path = REPORTS_DIR / f\"10_confusion_matrix_{best_model}_{best_fs}.png\"\n",
    "plot_confusion_matrix_figure(cm, cm_path, title=f\"Confusion-Matrix ({best_model}, {best_fs})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ebe7fa-d52a-4e7b-9254-b37b0b1a0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 15:07:00,296 | INFO | Übersicht gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\reports\\10_overview_metrics.csv, C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\reports\\10_overview_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# %% [step8-uebersicht]\n",
    "\n",
    "# Übersicht 10: Gesamttabelle Baselines×Feature-Sets (CSV + PNG-Rendering)\n",
    "\n",
    "# CSV\n",
    "\n",
    "overview_csv = REPORTS_DIR / \"10_overview_metrics.csv\"\n",
    "df_metrics.to_csv(overview_csv, index=False)\n",
    "\n",
    "# PNG-Rendering einer Tabelle mit ausgewählten Spalten\n",
    "\n",
    "cols_to_show = [\n",
    "    \"model\", \"feature_set\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\", \"brier\", \"sharpe\", \"sortino\", \"rachev\",\n",
    "    \"dm_stat\", \"dm_pvalue\", \"mcnemar_stat\", \"mcnemar_p\",\n",
    "]\n",
    "table_df = df_metrics[cols_to_show].copy()\n",
    "table_df = table_df.sort_values(by=[\"f1\", \"accuracy\"], ascending=False)\n",
    "\n",
    "# Matplotlib Table (eine Figure)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.axis(\"off\")\n",
    "tbl = ax.table(\n",
    "    cellText=table_df.round(3).values.tolist(),\n",
    "    colLabels=table_df.columns.tolist(),\n",
    "    loc=\"center\",\n",
    ")\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(8)\n",
    "tbl.scale(1.0, 1.4)\n",
    "plt.tight_layout()\n",
    "overview_png = REPORTS_DIR / \"10_overview_metrics.png\"\n",
    "plt.savefig(overview_png, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "logger.info(f\"Übersicht gespeichert: {overview_csv}, {overview_png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26db167-2225-4ccf-929a-4295b092d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
