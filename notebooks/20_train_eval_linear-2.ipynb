{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c267e60e-a2ac-407b-b549-f3ff6ebaacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# Helper: find project root by looking for specified directories\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Find the project root directory (contains 'config' and 'src' folders).\"\"\"\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Project root not found (expected 'config' and 'src' directories).\")\n",
    "\n",
    "# Helper: persist a dictionary as a JSON file (with indentation for readability)\n",
    "def _persist_json(obj: dict, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Convert any numpy types to native types for JSON serialization\n",
    "    obj_converted = {k: (float(v) if isinstance(v, np.generic) else v) for k, v in obj.items()}\n",
    "    path.write_text(json.dumps(obj_converted, indent=2), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569e2bbe-47e7-47ba-8968-c5571a7c7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2009-02-28 → 2025-05-31 (n=196)\n",
      "Train period: 2009-02-28 → 2019-12-31 | n=131\n",
      "Test  period: 2020-01-31 → 2025-05-31 | n=65\n"
     ]
    }
   ],
   "source": [
    "# Define paths for data and metrics\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "FEATURES_PATH = ARTIFACTS_DIR / \"data\" / \"features_monthly.parquet\"\n",
    "METRICS_DIR = ARTIFACTS_DIR / \"metrics\"\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Features file not found at {FEATURES_PATH}. Please run notebooks 00-03 to generate it.\")\n",
    "df = pd.read_parquet(FEATURES_PATH)\n",
    "\n",
    "# Ensure the index is a DateTime index and sorted\n",
    "if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"Index must be DatetimeIndex for time-series modeling.\")\n",
    "if not df.index.is_monotonic_increasing:\n",
    "    df = df.sort_index()\n",
    "\n",
    "# Verify required target columns are present\n",
    "required_targets = {\"y_return_next_pct\", \"y_direction_next\"}\n",
    "missing_targets = required_targets - set(df.columns)\n",
    "if missing_targets:\n",
    "    raise KeyError(f\"Missing target columns in features data: {missing_targets}\")\n",
    "\n",
    "# Display the date range of the data for reference\n",
    "print(f\"Data date range: {df.index.min().date()} → {df.index.max().date()} (n={len(df)})\")\n",
    "\n",
    "# Train-test split (using the conventional split: train up to 2019-12, test from 2020-01)\n",
    "train = df[df.index <= \"2019-12-31\"]\n",
    "test  = df[df.index >= \"2020-01-31\"]\n",
    "print(f\"Train period: {train.index.min().date()} → {train.index.max().date()} | n={len(train)}\")\n",
    "print(f\"Test  period: {test.index.min().date()} → {test.index.max().date()} | n={len(test)}\")\n",
    "\n",
    "# Separate features (X) and targets (y) for both regression and classification\n",
    "X_train = train.drop(columns=list(required_targets))\n",
    "X_test  = test.drop(columns=list(required_targets))\n",
    "y_train_reg = train[\"y_return_next_pct\"]\n",
    "y_test_reg  = test[\"y_return_next_pct\"]\n",
    "# Ensure classification target is integer type\n",
    "y_train_clf = train[\"y_direction_next\"].astype(int)\n",
    "y_test_clf  = test[\"y_direction_next\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c88552-ec42-4998-984d-cf039e02968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042fe38726864b1ea8986cd209ccaf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='Ridge α', max=2.0, min=-3.0), FloatLogSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Define the evaluation function that trains models and outputs metrics and plots\n",
    "def evaluate_model(ridge_alpha=1.0, logistic_C=1.0, threshold=0.5):\n",
    "    # Train models with given parameters\n",
    "    linreg = LinearRegression()\n",
    "    ridge = Ridge(alpha=ridge_alpha)\n",
    "    logreg = LogisticRegression(C=logistic_C, solver='lbfgs', max_iter=1000, random_state=0)\n",
    "    linreg.fit(X_train, y_train_reg)\n",
    "    ridge.fit(X_train, y_train_reg)\n",
    "    logreg.fit(X_train, y_train_clf)\n",
    "    # Generate predictions on the test set\n",
    "    y_pred_ols = linreg.predict(X_test)\n",
    "    y_pred_ridge = ridge.predict(X_test)\n",
    "    y_prob = logreg.predict_proba(X_test)[:, 1]  # probability of class 1 (Up) for each test sample\n",
    "    y_pred_class = (y_prob >= threshold).astype(int)\n",
    "    # Compute regression metrics\n",
    "    rmse_ols = np.sqrt(mean_squared_error(y_test_reg, y_pred_ols))\n",
    "    rmse_ridge = np.sqrt(mean_squared_error(y_test_reg, y_pred_ridge))\n",
    "    mae_ols = mean_absolute_error(y_test_reg, y_pred_ols)\n",
    "    mae_ridge = mean_absolute_error(y_test_reg, y_pred_ridge)\n",
    "    r2_ols = r2_score(y_test_reg, y_pred_ols)\n",
    "    r2_ridge = r2_score(y_test_reg, y_pred_ridge)\n",
    "    # Compute classification metrics\n",
    "    acc_log = accuracy_score(y_test_clf, y_pred_class)\n",
    "    f1_log = f1_score(y_test_clf, y_pred_class)\n",
    "    prec_log = precision_score(y_test_clf, y_pred_class, zero_division=0)\n",
    "    rec_log = recall_score(y_test_clf, y_pred_class, zero_division=0)\n",
    "    try:\n",
    "        auc_log = roc_auc_score(y_test_clf, y_prob)\n",
    "    except ValueError:\n",
    "        # If only one class present in y_test (unlikely in our case), ROC AUC is undefined\n",
    "        auc_log = float('nan')\n",
    "    # Print out the metrics\n",
    "    print(\"Regression performance on test set:\")\n",
    "    print(f\"  RMSE (OLS) = {rmse_ols:.4f},   RMSE (Ridge, α={ridge_alpha}) = {rmse_ridge:.4f}\")\n",
    "    print(f\"  MAE  (OLS) = {mae_ols:.4f},   MAE  (Ridge, α={ridge_alpha}) = {mae_ridge:.4f}\")\n",
    "    print(f\"  R^2  (OLS) = {r2_ols:.4f},   R^2  (Ridge, α={ridge_alpha}) = {r2_ridge:.4f}\")\n",
    "    print(\"\\nClassification performance on test set:\")\n",
    "    print(f\"  Accuracy = {acc_log:.4f},   F1-score = {f1_log:.4f}\")\n",
    "    print(f\"  Precision = {prec_log:.4f},   Recall = {rec_log:.4f},   ROC AUC = {auc_log:.4f}\")\n",
    "    # Plot Actual vs Predicted returns and ROC curve\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    # Left plot: time series of actual vs predicted returns\n",
    "    axes[0].plot(y_test_reg.index, y_test_reg, label=\"Actual\", color='C0')\n",
    "    axes[0].plot(y_test_reg.index, y_pred_ols, label=\"Predicted OLS\", color='C1')\n",
    "    axes[0].plot(y_test_reg.index, y_pred_ridge, label=f\"Predicted Ridge (α={ridge_alpha})\", color='C2')\n",
    "    axes[0].axhline(0.0, color='k', linestyle='--', linewidth=0.8)  # reference line at 0% return\n",
    "    axes[0].set_title(\"SP500 Monthly Returns: Actual vs Predicted\")\n",
    "    axes[0].set_xlabel(\"Date\")\n",
    "    axes[0].set_ylabel(\"Monthly Return (%)\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "    for label in axes[0].get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "    # Right plot: ROC curve for classification (if applicable)\n",
    "    axes[1].set_title(\"ROC Curve (Logistic Regression)\")\n",
    "    axes[1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[1].set_ylabel(\"True Positive Rate\")\n",
    "    # Plot ROC curve only if both classes are present in test set\n",
    "    if len(np.unique(y_test_clf)) == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_test_clf, y_prob)\n",
    "        axes[1].plot(fpr, tpr, label=f\"Model (AUC = {auc_log:.2f})\", color='C1')\n",
    "        axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Chance\")\n",
    "        axes[1].legend(loc=\"lower right\")\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, \"Only one class present in test data\", ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widgets for parameters\n",
    "alpha_slider = widgets.FloatLogSlider(value=1.0, base=10, min=-3, max=2, step=0.1, description=\"Ridge α\")\n",
    "c_slider = widgets.FloatLogSlider(value=1.0, base=10, min=-3, max=3, step=0.1, description=\"Logistic C\")\n",
    "threshold_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description=\"Threshold\")\n",
    "\n",
    "# Display the interactive controls and output\n",
    "widgets.interact(evaluate_model, ridge_alpha=alpha_slider, logistic_C=c_slider, threshold=threshold_slider);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda8ddc1-5e04-49c3-8ac8-9451bc4d3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REG_RMSE_ols: 5.1870\n",
      "REG_RMSE_ridge: 5.1701\n",
      "REG_MAE_ols: 4.3686\n",
      "REG_MAE_ridge: 4.3741\n",
      "REG_R2_ols: -0.0164\n",
      "REG_R2_ridge: -0.0098\n",
      "CLF_Acc_logistic: 0.4462\n",
      "CLF_F1_logistic: 0.5000\n",
      "CLF_Prec_logistic: 0.5806\n",
      "CLF_Recall_logistic: 0.4390\n",
      "CLF_AUC_logistic: 0.4654\n",
      "Metrics saved to: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear.json\n"
     ]
    }
   ],
   "source": [
    "# Set final chosen hyperparameters (modify these based on your exploration if desired)\n",
    "final_ridge_alpha = 1.0\n",
    "final_logistic_C = 1.0\n",
    "final_threshold = 0.5\n",
    "\n",
    "# Train final models with chosen parameters on the full training set\n",
    "linreg_final = LinearRegression()\n",
    "ridge_final = Ridge(alpha=final_ridge_alpha)\n",
    "logreg_final = LogisticRegression(C=final_logistic_C, solver='lbfgs', max_iter=1000, random_state=0)\n",
    "linreg_final.fit(X_train, y_train_reg)\n",
    "ridge_final.fit(X_train, y_train_reg)\n",
    "logreg_final.fit(X_train, y_train_clf)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_ols_final = linreg_final.predict(X_test)\n",
    "y_pred_ridge_final = ridge_final.predict(X_test)\n",
    "y_prob_final = logreg_final.predict_proba(X_test)[:, 1]\n",
    "y_pred_class_final = (y_prob_final >= final_threshold).astype(int)\n",
    "\n",
    "# Compute metrics for final model\n",
    "metrics = {\n",
    "    # Regression metrics (OLS vs Ridge)\n",
    "    \"REG_RMSE_ols\": np.sqrt(mean_squared_error(y_test_reg, y_pred_ols_final)),\n",
    "    \"REG_RMSE_ridge\": np.sqrt(mean_squared_error(y_test_reg, y_pred_ridge_final)),\n",
    "    \"REG_MAE_ols\": mean_absolute_error(y_test_reg, y_pred_ols_final),\n",
    "    \"REG_MAE_ridge\": mean_absolute_error(y_test_reg, y_pred_ridge_final),\n",
    "    \"REG_R2_ols\": r2_score(y_test_reg, y_pred_ols_final),\n",
    "    \"REG_R2_ridge\": r2_score(y_test_reg, y_pred_ridge_final),\n",
    "    # Classification metrics (Logistic Regression)\n",
    "    \"CLF_Acc_logistic\": accuracy_score(y_test_clf, y_pred_class_final),\n",
    "    \"CLF_F1_logistic\": f1_score(y_test_clf, y_pred_class_final, zero_division=0),\n",
    "    \"CLF_Prec_logistic\": precision_score(y_test_clf, y_pred_class_final, zero_division=0),\n",
    "    \"CLF_Recall_logistic\": recall_score(y_test_clf, y_pred_class_final, zero_division=0),\n",
    "    \"CLF_AUC_logistic\": roc_auc_score(y_test_clf, y_prob_final) if len(np.unique(y_test_clf)) == 2 else float('nan')\n",
    "}\n",
    "\n",
    "# Print the metrics for verification\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float) or isinstance(v, np.floating):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# Save metrics to a JSON file in the artifacts/metrics directory\n",
    "out_path = METRICS_DIR / \"linear.json\"\n",
    "_persist_json(metrics, out_path)\n",
    "print(f\"Metrics saved to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
