{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb69eb7-6b63-4c07-b6cd-a77a468e2ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ROOT=C:\\Users\\gamer\\Desktop\\AktienPrognose | DATA_DIR=C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\data\n"
     ]
    }
   ],
   "source": [
    "# Step0: Setup (robuste Pfade, Imports, Logging, Feature-Set-YAML)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml, pickle, logging, warnings\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.stats import norm\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, log_loss\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(\"arimax\")\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    chain = [start, *start.parents]\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\" / \"features_monthly.parquet\").exists():\n",
    "            return p\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\").exists():\n",
    "            return p\n",
    "    for p in chain:\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "    raise AssertionError(\"Project root not found – expected 'artifacts/data' or 'config'+'src' somewhere above.\")\n",
    "\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "DATA_DIR = ARTIFACTS / \"data\"\n",
    "CONF_DIR = ARTIFACTS / \"config\"\n",
    "FORECASTS_DIR = ARTIFACTS / \"forecasts\"\n",
    "METRICS_DIR = ARTIFACTS / \"metrics\"\n",
    "MODELS_DIR = ARTIFACTS / \"models\"\n",
    "REPORTS_DIR = ARTIFACTS / \"reports\"\n",
    "for p in [DATA_DIR, CONF_DIR, FORECASTS_DIR, METRICS_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"ROOT={ROOT} | DATA_DIR={DATA_DIR}\")\n",
    "\n",
    "# Feature-Set-Konfig laden/erstellen\n",
    "FEATS_CFG = CONF_DIR / \"features_config.yaml\"\n",
    "if not FEATS_CFG.exists():\n",
    "    default_cfg_feats = {\n",
    "        \"TECH\": [\"3M_SMA_Return\",\"12M_SMA_Return\",\"3M_Momentum\",\"Volatility_6M\",\"Return_Lag1\"],\n",
    "        \"MACRO\": [\"FedFunds_Delta_bps\",\"Inflation_YoY_pct\",\"UnemploymentRate\",\"VIX\",\"EPU_US\",\"FSI\",\"Gold_USD_oz\",\"WTI_Spot\",\"USD_per_EUR\"],\n",
    "    }\n",
    "    default_cfg_feats[\"INTEGRATED\"] = default_cfg_feats[\"TECH\"] + default_cfg_feats[\"MACRO\"]\n",
    "    with FEATS_CFG.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(default_cfg_feats, f, sort_keys=False)\n",
    "\n",
    "with FEATS_CFG.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    cfg_feats = yaml.safe_load(f) or {}\n",
    "\n",
    "TECH_FEATS = cfg_feats.get(\"TECH\", [])\n",
    "MACRO_FEATS = cfg_feats.get(\"MACRO\", [])\n",
    "INTEGRATED_FEATS = cfg_feats.get(\"INTEGRATED\", TECH_FEATS + MACRO_FEATS)\n",
    "\n",
    "TARGET_RET = \"y_return_next_pct\"\n",
    "TARGET_DIR = \"y_direction_next\"\n",
    "\n",
    "def _sappend(s: pd.Series, item: pd.Series) -> pd.Series:\n",
    "    return pd.concat([s, item])\n",
    "\n",
    "def _dappend(df: pd.DataFrame, row_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df, row_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a57fd3e-18e1-41e0-8c3d-970042791f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Daten/Featuresets (Load, Align, Splits, Sanity)\n",
    "features_path = DATA_DIR / \"features_monthly.parquet\"\n",
    "raw_path = DATA_DIR / \"raw_data.parquet\"\n",
    "\n",
    "assert features_path.exists(), f\"Missing file: {features_path}\"\n",
    "features_df = pd.read_parquet(features_path)\n",
    "features_df.index = pd.to_datetime(features_df.index)\n",
    "features_df = features_df.sort_index()\n",
    "\n",
    "# raw optional\n",
    "raw_df = None\n",
    "if raw_path.exists():\n",
    "    raw_df = pd.read_parquet(raw_path)\n",
    "    raw_df.index = pd.to_datetime(raw_df.index)\n",
    "    raw_df = raw_df.sort_index()\n",
    "\n",
    "TRAIN_START, TRAIN_END = \"2009-02-28\", \"2019-12-31\"\n",
    "TEST_START, TEST_END   = \"2020-01-31\", \"2025-05-31\"\n",
    "\n",
    "train_df = features_df.loc[TRAIN_START:TRAIN_END].copy()\n",
    "test_df  = features_df.loc[TEST_START:TEST_END].copy()\n",
    "\n",
    "def _X(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return df[cols].astype(float)\n",
    "\n",
    "X_train_sets = {\n",
    "    \"TECH\": _X(train_df, TECH_FEATS).dropna(),\n",
    "    \"MACRO\": _X(train_df, MACRO_FEATS).dropna(),\n",
    "    \"INTEGRATED\": _X(train_df, INTEGRATED_FEATS).dropna(),\n",
    "}\n",
    "X_test_sets = {\n",
    "    \"TECH\": _X(test_df, TECH_FEATS).dropna(),\n",
    "    \"MACRO\": _X(test_df, MACRO_FEATS).dropna(),\n",
    "    \"INTEGRATED\": _X(test_df, INTEGRATED_FEATS).dropna(),\n",
    "}\n",
    "\n",
    "def _align_targets(df: pd.DataFrame, idx: pd.Index) -> Tuple[pd.Series, pd.Series]:\n",
    "    yr = df.reindex(idx)[TARGET_RET].astype(float)\n",
    "    yd = df.reindex(idx)[TARGET_DIR].astype(int)\n",
    "    return yr, yd\n",
    "\n",
    "y_train_all = train_df[TARGET_RET].astype(float)\n",
    "y_train_dir_all = train_df[TARGET_DIR].astype(int)\n",
    "y_test_all = test_df[TARGET_RET].astype(float)\n",
    "y_test_dir_all = test_df[TARGET_DIR].astype(int)\n",
    "\n",
    "# Sanity: sicherstellen, dass alle Sets Targets haben\n",
    "for fs, Xtr in X_train_sets.items():\n",
    "    _yr, _yd = _align_targets(train_df, Xtr.index)\n",
    "    assert _yr.notna().all() and _yd.notna().all(), f\"Targets contain NaN in {fs} train\"\n",
    "for fs, Xte in X_test_sets.items():\n",
    "    _yr, _yd = _align_targets(test_df, Xte.index)\n",
    "    assert _yr.notna().all() and _yd.notna().all(), f\"Targets contain NaN in {fs} test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63f9853-986d-4b38-82e6-dffe6f0d616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: TSCV-Splitter (5 Folds, Val=12M, Embargo=1M)\n",
    "from typing import Iterable\n",
    "\n",
    "def build_folds() -> List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp]]:\n",
    "    years = [2015, 2016, 2017, 2018, 2019]\n",
    "    folds = []\n",
    "    for y in years:\n",
    "        train_end = pd.Timestamp(f\"{y-1}-11-30\")  # bis Nov Vorjahr (Embargo: Dez)\n",
    "        val_start = pd.Timestamp(f\"{y}-01-31\")    # Val: Jan..Dez\n",
    "        val_end   = pd.Timestamp(f\"{y}-12-31\")\n",
    "        folds.append((train_end, val_start, val_end))\n",
    "    return folds\n",
    "\n",
    "tscv_folds = build_folds()\n",
    "\n",
    "def slice_xy(X: pd.DataFrame, y: pd.Series, start: pd.Timestamp, end: pd.Timestamp) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    idx = (X.index >= start) & (X.index <= end)\n",
    "    return X.loc[idx], y.loc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed9e1c7-5d90-4160-ac5d-26d7223678a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 (Fix): ARIMAX OOF → Kalibrierung → OOF-Threshold  | use .forecast(steps=...) to avoid exog shape mismatch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "param_grid = [(p,d,q) for p in [0,1,2] for d in [0,1,2] for q in [0,1,2]]\n",
    "\n",
    "calibrators, thresholds = {}, {}\n",
    "best_orders_cv, oof_store = {}, {}\n",
    "\n",
    "for fs, Xtr in X_train_sets.items():\n",
    "    ytr_reg, ytr_dir = _align_targets(train_df, Xtr.index)\n",
    "\n",
    "    fold_rows = []\n",
    "    oof_frames = []\n",
    "\n",
    "    for (train_end, val_start, val_end) in tscv_folds:\n",
    "        X_train_fold = Xtr.loc[:train_end]\n",
    "        y_train_fold = ytr_reg.loc[:train_end]\n",
    "        # validation window based on available X rows\n",
    "        X_val_fold = Xtr.loc[(Xtr.index >= val_start) & (Xtr.index <= val_end)]\n",
    "        y_val_fold = ytr_reg.reindex(X_val_fold.index)\n",
    "\n",
    "        if len(X_val_fold) == 0:\n",
    "            continue\n",
    "\n",
    "        best_aic, best_order, best_pred, best_f1 = np.inf, None, None, -np.inf\n",
    "\n",
    "        for (p,d,q) in param_grid:\n",
    "            try:\n",
    "                res = ARIMA(endog=y_train_fold, exog=X_train_fold, order=(p,d,q)).fit()\n",
    "                aic = res.aic\n",
    "                # IMPORTANT: forecast with exact step-count to match exog rows\n",
    "                pred = res.forecast(steps=len(X_val_fold), exog=X_val_fold)\n",
    "                pred.index = X_val_fold.index\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            f1 = f1_score((y_val_fold>=0).astype(int), (pred>=0).astype(int))\n",
    "            if (aic < best_aic) or (np.isclose(aic, best_aic) and f1 > best_f1):\n",
    "                best_aic, best_order, best_pred, best_f1 = aic, (p,d,q), pred.copy(), f1\n",
    "\n",
    "        if best_pred is None:\n",
    "            # fallback AR(1)\n",
    "            res = ARIMA(endog=y_train_fold, exog=X_train_fold, order=(1,0,0)).fit()\n",
    "            best_order, best_aic = (1,0,0), res.aic\n",
    "            best_pred = res.forecast(steps=len(X_val_fold), exog=X_val_fold)\n",
    "            best_pred.index = X_val_fold.index\n",
    "\n",
    "        fold_rows.append((f\"{val_start.date()}_{val_end.date()}\", best_order, float(best_aic)))\n",
    "        oof_frames.append(pd.DataFrame({\n",
    "            \"date\": best_pred.index,\n",
    "            \"y_point\": best_pred.values,\n",
    "            \"y_true_dir\": (y_val_fold>=0).astype(int).values\n",
    "        }))\n",
    "\n",
    "    best_orders_cv[fs] = fold_rows\n",
    "    oof_df = pd.concat(oof_frames, axis=0).set_index(\"date\").sort_index()\n",
    "\n",
    "    cal = LogisticRegression(max_iter=2000)\n",
    "    cal.fit(oof_df[[\"y_point\"]].values, oof_df[\"y_true_dir\"].values)\n",
    "    calibrators[fs] = cal\n",
    "\n",
    "    oof_proba = cal.predict_proba(oof_df[[\"y_point\"]].values)[:,1]\n",
    "    y_true_oof = oof_df[\"y_true_dir\"].values\n",
    "    best_th, best_f1 = 0.5, -np.inf\n",
    "    for th in np.linspace(0.01, 0.99, 99):\n",
    "        y_hat = (oof_proba >= th).astype(int)\n",
    "        f1 = f1_score(y_true_oof, y_hat)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_th = f1, th\n",
    "    thresholds[fs] = float(best_th)\n",
    "    oof_store[fs] = oof_df.assign(y_proba=oof_proba)\n",
    "\n",
    "    pd.DataFrame(fold_rows, columns=[\"val_window\",\"best_order\",\"best_aic\"]).to_csv(\n",
    "        METRICS_DIR / f\"cv_details_arima_x_{fs}.csv\", index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b9cb4a-c192-49c7-92f0-2095f04a93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Walk-Forward-Test (expanding origin) + Modelle speichern\n",
    "best_orders_full: Dict[str, Tuple[int,int,int]] = {}\n",
    "forecasts: Dict[str, pd.DataFrame] = {}\n",
    "last_models: Dict[str, object] = {}\n",
    "\n",
    "for fs, Xtr in X_train_sets.items():\n",
    "    ytr_reg, _ = _align_targets(train_df, Xtr.index)\n",
    "    # Best (p,d,q) auf Full-Train per AIC\n",
    "    best_aic = np.inf; best_order = None\n",
    "    for (p,d,q) in param_grid:\n",
    "        try:\n",
    "            res = ARIMA(endog=ytr_reg, exog=Xtr, order=(p,d,q)).fit()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if res.aic < best_aic:\n",
    "            best_aic, best_order = res.aic, (p,d,q)\n",
    "    if best_order is None:\n",
    "        best_order = (1,0,0)\n",
    "    best_orders_full[fs] = best_order\n",
    "\n",
    "    Xte = X_test_sets[fs].copy()\n",
    "    yte_ret, yte_dir = _align_targets(test_df, Xte.index)\n",
    "\n",
    "    train_y = ytr_reg.copy()\n",
    "    train_X = Xtr.copy()\n",
    "    dates = Xte.index\n",
    "    y_point_preds = []\n",
    "\n",
    "    for dt in dates:\n",
    "        res = ARIMA(endog=train_y, exog=train_X, order=best_order).fit()\n",
    "        y_point = res.predict(start=dt, end=dt, exog=Xte.loc[dt:dt]).iloc[0]\n",
    "        y_point_preds.append(y_point)\n",
    "        if dt in yte_ret.index:\n",
    "            train_y = _sappend(train_y, pd.Series({dt: yte_ret.loc[dt]}))\n",
    "            train_X = _dappend(train_X, Xte.loc[dt:dt])\n",
    "        last_models[fs] = res\n",
    "\n",
    "    y_point_preds = np.array(y_point_preds, dtype=float)\n",
    "    proba = calibrators[fs].predict_proba(y_point_preds.reshape(-1,1))[:,1]\n",
    "    y_pred = (proba >= thresholds[fs]).astype(int)\n",
    "\n",
    "    fc_df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"y_true\": yte_dir.values.astype(int),\n",
    "        \"y_pred\": y_pred.astype(int),\n",
    "        \"y_proba\": proba.astype(float),\n",
    "        \"y_point\": y_point_preds.astype(float)\n",
    "    }).set_index(\"date\")\n",
    "    forecasts[fs] = fc_df\n",
    "    fc_df.to_csv(FORECASTS_DIR / f\"arima_x_{fs}.csv\")\n",
    "\n",
    "    with open(MODELS_DIR / f\"arimax_last_{fs}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(last_models[fs], f)\n",
    "    with open(MODELS_DIR / f\"calibrator_{fs}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(calibrators[fs], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90da17cb-9f66-4339-948c-e3852d6814b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: Metriken, Signifikanz (DM 0/1-Loss vs Always-Up & vs Linear-Logit; McNemar vs Always-Up), JSON\n",
    "import json\n",
    "\n",
    "def dm_test_01_loss(y_true: np.ndarray, y_pred_a: np.ndarray, y_pred_b: np.ndarray) -> float:\n",
    "    la = (y_true != y_pred_a).astype(int)\n",
    "    lb = (y_true != y_pred_b).astype(int)\n",
    "    d = la - lb\n",
    "    dm_stat = np.mean(d) / (np.std(d, ddof=1) / np.sqrt(len(d)))\n",
    "    p = 2 * (1 - norm.cdf(abs(dm_stat)))\n",
    "    return float(p)\n",
    "\n",
    "metrics_summary = []\n",
    "\n",
    "for fs, fc in forecasts.items():\n",
    "    y_true = fc[\"y_true\"].values.astype(int)\n",
    "    y_pred = fc[\"y_pred\"].values.astype(int)\n",
    "    y_proba = fc[\"y_proba\"].values.astype(float)\n",
    "\n",
    "    F1 = f1_score(y_true, y_pred)\n",
    "    AUC = roc_auc_score(y_true, y_proba)\n",
    "    ACC = accuracy_score(y_true, y_pred)\n",
    "    PREC = precision_score(y_true, y_pred, zero_division=0)\n",
    "    REC = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Baselines\n",
    "    always_up = np.ones_like(y_true, dtype=int)\n",
    "\n",
    "    # Linear-Logit baseline (train auf full train)\n",
    "    Xtr = X_train_sets[fs]; _, ytr_dir = _align_targets(train_df, Xtr.index)\n",
    "    logit = LogisticRegression(max_iter=2000)\n",
    "    logit.fit(Xtr.values, ytr_dir.values)\n",
    "    Xte = X_test_sets[fs].reindex(fc.index)\n",
    "    logit_proba = logit.predict_proba(Xte.values)[:,1]\n",
    "    logit_pred = (logit_proba >= 0.5).astype(int)\n",
    "\n",
    "    p_dm_vs_always = dm_test_01_loss(y_true, y_pred, always_up)\n",
    "    p_dm_vs_logit  = dm_test_01_loss(y_true, y_pred, logit_pred)\n",
    "\n",
    "    b01 = np.sum((y_pred == y_true) & (always_up != y_true))\n",
    "    b10 = np.sum((y_pred != y_true) & (always_up == y_true))\n",
    "    tbl = [[0, b01],[b10, 0]]\n",
    "    mcn = mcnemar(tbl, exact=False, correction=True)\n",
    "\n",
    "    met = {\n",
    "        \"F1\": float(F1), \"AUC\": float(AUC), \"Accuracy\": float(ACC),\n",
    "        \"Precision\": float(PREC), \"Recall\": float(REC),\n",
    "        \"threshold\": float(thresholds[fs]),\n",
    "        \"oof_f1\": float(f1_score(oof_store[fs][\"y_true_dir\"], (oof_store[fs][\"y_proba\"]>=thresholds[fs]).astype(int))),\n",
    "        \"oof_auc\": float(roc_auc_score(oof_store[fs][\"y_true_dir\"], oof_store[fs][\"y_proba\"])),\n",
    "        \"cv_details_path\": str((METRICS_DIR / f\"cv_details_arima_x_{fs}.csv\").as_posix()),\n",
    "        \"calibrator_path\": str((MODELS_DIR / f\"calibrator_{fs}.pkl\").as_posix()),\n",
    "        \"dm_p_vs_always\": float(p_dm_vs_always),\n",
    "        \"dm_p_vs_logit\": float(p_dm_vs_logit),\n",
    "        \"mcnemar_p_vs_always\": float(mcn.pvalue),\n",
    "    }\n",
    "    metrics_summary.append({\"FeatureSet\": fs, **met})\n",
    "\n",
    "    with open(MODELS_DIR / f\"logit_baseline_{fs}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(logit, f)\n",
    "    with open(METRICS_DIR / f\"arima_x_{fs}.json\", \"w\") as f:\n",
    "        json.dump(met, f, indent=2)\n",
    "\n",
    "summary_df = pd.DataFrame(metrics_summary).set_index(\"FeatureSet\")\n",
    "summary_df.to_csv(REPORTS_DIR / \"21_summary_table.csv\")\n",
    "\n",
    "# einfache Heatmap ohne seaborn\n",
    "vals = summary_df[[\"F1\",\"AUC\",\"Accuracy\",\"Precision\",\"Recall\"]].values\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.imshow(vals, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(5), [\"F1\",\"AUC\",\"ACC\",\"PREC\",\"REC\"])\n",
    "plt.yticks(range(len(summary_df.index)), summary_df.index)\n",
    "plt.title(\"ARIMAX Metrics by Feature Set\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / \"21_summary_table.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7430aa10-a493-47cc-a59f-f5e5746c34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step6 (Fix): Use .forecast(steps=...) and align indices to avoid exog shape mismatch\n",
    "LAST_VAL_YEAR = 2019\n",
    "last_train_end = pd.Timestamp(f\"{LAST_VAL_YEAR-1}-11-30\")\n",
    "last_val_start = pd.Timestamp(f\"{LAST_VAL_YEAR}-01-31\")\n",
    "last_val_end   = pd.Timestamp(f\"{LAST_VAL_YEAR}-12-31\")\n",
    "\n",
    "for fs in X_train_sets.keys():\n",
    "    # Coefficients of last walk-forward fit\n",
    "    res_last = last_models[fs]\n",
    "    coef = pd.Series(res_last.params, name=\"coef\")\n",
    "    coef.index.name = \"param\"\n",
    "    coef.to_csv(REPORTS_DIR / f\"21_coeffs_{fs}.csv\")\n",
    "\n",
    "    # Permutation Importance (Δ log-loss) im letzten Val-Fenster\n",
    "    Xtr = X_train_sets[fs]\n",
    "    ytr_reg, _ = _align_targets(train_df, Xtr.index)\n",
    "\n",
    "    X_train_fold = Xtr.loc[:last_train_end]\n",
    "    y_train_fold = ytr_reg.loc[:last_train_end]\n",
    "    X_val_fold   = Xtr.loc[last_val_start:last_val_end]\n",
    "    if X_val_fold.empty:\n",
    "        continue\n",
    "    y_val_dir    = train_df.loc[X_val_fold.index, TARGET_DIR].astype(int)\n",
    "\n",
    "    order = best_orders_full.get(fs, (1,0,0))\n",
    "    try:\n",
    "        res_fold = ARIMA(endog=y_train_fold, exog=X_train_fold, order=order).fit()\n",
    "    except Exception:\n",
    "        res_fold = ARIMA(endog=y_train_fold, exog=X_train_fold, order=(1,0,0)).fit()\n",
    "\n",
    "    # IMPORTANT: forecast with exact step-count to match exog rows\n",
    "    y_point_val = res_fold.forecast(steps=len(X_val_fold), exog=X_val_fold)\n",
    "    y_point_val.index = X_val_fold.index\n",
    "\n",
    "    base_proba = calibrators[fs].predict_proba(y_point_val.values.reshape(-1,1))[:,1]\n",
    "    base_ll = log_loss(y_val_dir.values, base_proba, labels=[0,1])\n",
    "\n",
    "    deltas = []\n",
    "    for feat in X_val_fold.columns:\n",
    "        X_perm = X_val_fold.copy()\n",
    "        X_perm[feat] = np.random.permutation(X_perm[feat].values)\n",
    "        y_point_perm = res_fold.forecast(steps=len(X_perm), exog=X_perm)\n",
    "        y_point_perm.index = X_perm.index\n",
    "        proba_perm = calibrators[fs].predict_proba(y_point_perm.values.reshape(-1,1))[:,1]\n",
    "        ll_perm = log_loss(y_val_dir.values, proba_perm, labels=[0,1])\n",
    "        deltas.append((feat, float(ll_perm - base_ll)))\n",
    "\n",
    "    imp_df = pd.DataFrame(deltas, columns=[\"feature\",\"delta_logloss\"]).sort_values(\"delta_logloss\", ascending=False).head(20)\n",
    "    imp_df.to_csv(REPORTS_DIR / f\"21_importance_{fs}.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.barh(imp_df[\"feature\"][::-1], imp_df[\"delta_logloss\"][::-1])\n",
    "    plt.xlabel(\"Δ log-loss (higher = more important)\")\n",
    "    plt.title(f\"Permutation Importance (last train window) – {fs}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / f\"importance_{fs}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b017df-b3e9-4f26-8b48-b898ee153e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7: Plots (Trefferquote über Zeit, kumulierte Rendite)\n",
    "for fs, fc in forecasts.items():\n",
    "    dates = fc.index\n",
    "    y_true = fc[\"y_true\"].values.astype(int)\n",
    "    y_pred = fc[\"y_pred\"].values.astype(int)\n",
    "\n",
    "    hits = (y_true == y_pred).astype(int)\n",
    "    cum_hit = np.cumsum(hits) / (np.arange(len(hits)) + 1)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(dates, cum_hit, marker=\"o\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(f\"Cumulative Hit Rate – {fs}\")\n",
    "    plt.ylabel(\"Hit rate\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / f\"21_hitrate_{fs}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    y_ret = test_df.loc[dates, TARGET_RET].values.astype(float)\n",
    "    strat_ret = np.where(y_pred==1, y_ret, 0.0)\n",
    "    cum_ret = np.cumprod(1.0 + strat_ret/100.0) - 1.0\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(dates, cum_ret, marker=\"o\")\n",
    "    plt.title(f\"Cumulative Return – {fs}\")\n",
    "    plt.ylabel(\"Cumulative return\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / f\"21_cumret_{fs}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baf5d11c-e3b9-4c4b-9641-7cc74fe2c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Persistenz (Thresholds, Orders, Pfade)\n",
    "persist = {\n",
    "    \"thresholds\": thresholds,\n",
    "    \"best_orders_full\": {k: list(v) for k,v in best_orders_full.items()},\n",
    "    \"best_orders_cv\": {k: [(w, list(o), a) for (w,o,a) in v] for k,v in best_orders_cv.items()},\n",
    "    \"paths\": {\n",
    "        \"forecasts_dir\": str(FORECASTS_DIR),\n",
    "        \"metrics_dir\": str(METRICS_DIR),\n",
    "        \"models_dir\": str(MODELS_DIR),\n",
    "        \"reports_dir\": str(REPORTS_DIR),\n",
    "        \"features_cfg\": str(FEATS_CFG),\n",
    "        \"features_parquet\": str(features_path),\n",
    "        \"raw_parquet\": str(raw_path) if raw_path.exists() else None,\n",
    "    }\n",
    "}\n",
    "with open(MODELS_DIR / \"arimax_persistence.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(persist, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edde512-30d8-4a65-85e1-a2c307aec84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step9: Übersicht 21 (Summary & Probabilites-Plot)\n",
    "summary = pd.read_csv(REPORTS_DIR / \"21_summary_table.csv\")\n",
    "summary[\"Rank_F1\"]  = summary[\"F1\"].rank(ascending=False, method=\"min\")\n",
    "summary[\"Rank_AUC\"] = summary[\"AUC\"].rank(ascending=False, method=\"min\")\n",
    "summary.to_csv(REPORTS_DIR / \"21_summary_table_ranked.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for fs, fc in forecasts.items():\n",
    "    plt.plot(fc.index, fc[\"y_proba\"].values, label=fs)\n",
    "plt.legend()\n",
    "plt.title(\"P(up) over Test Period\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / \"21_probabilities_over_time.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
