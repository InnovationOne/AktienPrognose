{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc10df4-1d8e-44b7-b675-d62661fc2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step0 Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, brier_score_loss, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create artifact directories if not existing\n",
    "root = Path.cwd()\n",
    "(art_dir := root / \"artifacts\").mkdir(exist_ok=True)\n",
    "(art_data := art_dir / \"data\").mkdir(exist_ok=True)\n",
    "(art_forecasts := art_dir / \"forecasts\").mkdir(exist_ok=True)\n",
    "(art_metrics := art_dir / \"metrics\").mkdir(exist_ok=True)\n",
    "(art_reports := art_dir / \"reports\").mkdir(exist_ok=True)\n",
    "\n",
    "# load actual returns and directions from feature data\n",
    "features_file = art_data / \"features_monthly.csv\"\n",
    "if features_file.exists():\n",
    "    df_features = pd.read_csv(features_file, parse_dates=[0])\n",
    "    # Identify return and direction columns\n",
    "    if \"y_return\" in df_features.columns:\n",
    "        df_features[\"return\"] = df_features[\"y_return\"] / 100.0  # convert percent to fraction\n",
    "        df_features[\"direction\"] = df_features[\"y_direction\"]\n",
    "    elif \"y_return_next_pct\" in df_features.columns:\n",
    "        df_features[\"return\"] = df_features[\"y_return_next_pct\"] / 100.0\n",
    "        df_features[\"direction\"] = df_features[\"y_direction_next\"]\n",
    "    else:\n",
    "        raise KeyError(\"Return column not found in features data.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Features file not found: {features_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fc310-c456-4853-b25e-3d83b0667624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Load forecast CSVs, validate and merge\n",
    "models = [\"linear\", \"arimax\", \"rf\", \"lstm\", \"baseline\"]\n",
    "feature_sets = [\"F1\", \"F3\"]\n",
    "df_forecasts = {}\n",
    "\n",
    "for fs in feature_sets:\n",
    "    merged = None\n",
    "    used_models = []\n",
    "    for model in models:\n",
    "        file = art_forecasts / f\"{model}_{fs}.csv\"\n",
    "        if file.exists():\n",
    "            df = pd.read_csv(file)\n",
    "            # identify date column (first column)\n",
    "            date_col = df.columns[0]\n",
    "            df = df.rename(columns={date_col: \"date\"})\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "            # rename prediction columns\n",
    "            if \"y_pred_proba_up\" in df.columns:\n",
    "                df = df.rename(columns={\"y_pred_proba_up\": f\"{model}_proba\"})\n",
    "            if \"y_pred_dir\" in df.columns:\n",
    "                df = df.rename(columns={\"y_pred_dir\": f\"{model}_dir\"})\n",
    "            if f\"{model}_dir\" in df.columns:\n",
    "                df[f\"{model}_dir\"] = df[f\"{model}_dir\"].astype(int)\n",
    "            # keep only date, proba, dir\n",
    "            cols_to_keep = [\"date\"] + [col for col in df.columns if col.endswith(\"_proba\") or col.endswith(\"_dir\")]\n",
    "            df = df[cols_to_keep]\n",
    "            if merged is None:\n",
    "                merged = df.copy()\n",
    "            else:\n",
    "                merged = pd.merge(merged, df, on=\"date\", how=\"outer\")\n",
    "            used_models.append(model)\n",
    "    if merged is None:\n",
    "        raise FileNotFoundError(f\"No forecast files found for feature set {fs}\")\n",
    "    merged = merged.sort_values(\"date\").reset_index(drop=True)\n",
    "    df_forecasts[fs] = {\"df\": merged, \"models\": used_models}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb2665-091b-45bf-a613-ea805152e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Compute ensemble forecasts\n",
    "ensemble_results = {}\n",
    "for fs in feature_sets:\n",
    "    data = df_forecasts[fs][\"df\"]\n",
    "    models_used = df_forecasts[fs][\"models\"]\n",
    "    # Probability Averaging\n",
    "    prob_cols = [f\"{m}_proba\" for m in models_used if f\"{m}_proba\" in data.columns]\n",
    "    data[\"ensemble_mean_proba\"] = data[prob_cols].mean(axis=1, skipna=True)\n",
    "    data[\"ensemble_mean_dir\"] = (data[\"ensemble_mean_proba\"] >= 0.5).astype(int)\n",
    "    ensemble_results[(fs, \"mean\")] = {\n",
    "        \"proba\": data[\"ensemble_mean_proba\"],\n",
    "        \"dir\": data[\"ensemble_mean_dir\"],\n",
    "        \"models_used\": models_used\n",
    "    }\n",
    "    # Weighted Averaging by F1\n",
    "    weights_f1 = {}\n",
    "    total_f1 = 0.0\n",
    "    for model in models_used:\n",
    "        metrics_file = art_metrics / f\"{model}_{fs}.json\"\n",
    "        if metrics_file.exists():\n",
    "            m = json.load(open(metrics_file))\n",
    "            oof_f1 = m.get(\"oof_f1\", 0.0)\n",
    "            weights_f1[model] = oof_f1\n",
    "            total_f1 += oof_f1\n",
    "    weights_f1 = {m: (w/total_f1 if total_f1>0 else 0) for m, w in weights_f1.items()}\n",
    "    data[\"ensemble_w_f1_proba\"] = 0.0\n",
    "    for model, w in weights_f1.items():\n",
    "        col = f\"{model}_proba\"\n",
    "        if col in data.columns:\n",
    "            data[\"ensemble_w_f1_proba\"] += data[col] * w\n",
    "    data[\"ensemble_w_f1_dir\"] = (data[\"ensemble_w_f1_proba\"] >= 0.5).astype(int)\n",
    "    ensemble_results[(fs, \"w_f1\")] = {\n",
    "        \"proba\": data[\"ensemble_w_f1_proba\"],\n",
    "        \"dir\": data[\"ensemble_w_f1_dir\"],\n",
    "        \"models_used\": list(weights_f1.keys()),\n",
    "        \"weights\": weights_f1,\n",
    "        \"oof_weight_metric\": \"f1\"\n",
    "    }\n",
    "    # Weighted Averaging by AUC\n",
    "    weights_auc = {}\n",
    "    total_auc = 0.0\n",
    "    for model in models_used:\n",
    "        metrics_file = art_metrics / f\"{model}_{fs}.json\"\n",
    "        if metrics_file.exists():\n",
    "            m = json.load(open(metrics_file))\n",
    "            oof_auc = m.get(\"oof_auc\", 0.0)\n",
    "            weights_auc[model] = oof_auc\n",
    "            total_auc += oof_auc\n",
    "    weights_auc = {m: (w/total_auc if total_auc>0 else 0) for m, w in weights_auc.items()}\n",
    "    data[\"ensemble_w_auc_proba\"] = 0.0\n",
    "    for model, w in weights_auc.items():\n",
    "        col = f\"{model}_proba\"\n",
    "        if col in data.columns:\n",
    "            data[\"ensemble_w_auc_proba\"] += data[col] * w\n",
    "    data[\"ensemble_w_auc_dir\"] = (data[\"ensemble_w_auc_proba\"] >= 0.5).astype(int)\n",
    "    ensemble_results[(fs, \"w_auc\")] = {\n",
    "        \"proba\": data[\"ensemble_w_auc_proba\"],\n",
    "        \"dir\": data[\"ensemble_w_auc_dir\"],\n",
    "        \"models_used\": list(weights_auc.keys()),\n",
    "        \"weights\": weights_auc,\n",
    "        \"oof_weight_metric\": \"auc\"\n",
    "    }\n",
    "    # Majority Vote\n",
    "    dir_cols = [f\"{m}_dir\" for m in models_used if f\"{m}_dir\" in data.columns]\n",
    "    def majority_vote(row):\n",
    "        votes = row[dir_cols].dropna().astype(int)\n",
    "        if votes.size == 0:\n",
    "            return 0\n",
    "        ones = votes.sum()\n",
    "        return 1 if ones > votes.size / 2 else 0\n",
    "    data[\"ensemble_vote_dir\"] = data.apply(majority_vote, axis=1)\n",
    "    data[\"ensemble_vote_proba\"] = data[dir_cols].mean(axis=1, skipna=True)\n",
    "    ensemble_results[(fs, \"vote\")] = {\n",
    "        \"proba\": data[\"ensemble_vote_proba\"],\n",
    "        \"dir\": data[\"ensemble_vote_dir\"],\n",
    "        \"models_used\": models_used\n",
    "    }\n",
    "# Save ensemble forecast CSVs\n",
    "for fs in feature_sets:\n",
    "    data = df_forecasts[fs][\"df\"]\n",
    "    method_keys = {\"mean\": \"ensemble_mean\", \"w_f1\": \"ensemble_w_f1\", \"w_auc\": \"ensemble_w_auc\", \"vote\": \"ensemble_vote\"}\n",
    "    for method_key, prefix in method_keys.items():\n",
    "        name = f\"ensemble_{method_key}_{fs}.csv\"\n",
    "        df_out = pd.DataFrame({\n",
    "            \"date\": data[\"date\"],\n",
    "            \"y_pred_proba_up\": data[f\"{prefix}_proba\"],\n",
    "            \"y_pred_dir\": data[f\"{prefix}_dir\"]\n",
    "        })\n",
    "        df_out.to_csv(art_forecasts / name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dbaf9-2fe2-4f52-ad09-8586ac05c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Compute metrics for each ensemble, save JSON\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# prepare actuals mapping by date\n",
    "df_feat = df_features.copy()\n",
    "date_col = df_feat.columns[0]\n",
    "df_feat = df_feat.rename(columns={date_col: \"date\"})\n",
    "df_feat[\"date\"] = pd.to_datetime(df_feat[\"date\"])\n",
    "df_feat = df_feat.set_index(\"date\")\n",
    "y_true_series = df_feat[\"direction\"]\n",
    "\n",
    "for (fs, method_key), ens in ensemble_results.items():\n",
    "    df_ens = df_forecasts[fs][\"df\"]\n",
    "    dates = pd.to_datetime(df_ens[\"date\"])\n",
    "    y_true = y_true_series.reindex(dates).values\n",
    "    y_pred = ens[\"dir\"].values\n",
    "    y_proba = ens[\"proba\"].values\n",
    "    mask = ~np.isnan(y_pred) & ~np.isnan(y_true)\n",
    "    y_true = y_true[mask].astype(int)\n",
    "    y_pred = y_pred[mask].astype(int)\n",
    "    y_proba = y_proba[mask].astype(float)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    returns = df_feat[\"return\"].reindex(dates)[mask].fillna(0).values\n",
    "    strat_returns = returns * y_pred\n",
    "    if strat_returns.std() > 0:\n",
    "        sharpe = strat_returns.mean() / strat_returns.std() * np.sqrt(12)\n",
    "    else:\n",
    "        sharpe = float(\"nan\")\n",
    "    neg_returns = strat_returns[strat_returns < 0]\n",
    "    if neg_returns.size > 0 and np.std(neg_returns, ddof=0) > 0:\n",
    "        sortino = strat_returns.mean() / np.std(neg_returns, ddof=0) * np.sqrt(12)\n",
    "    else:\n",
    "        sortino = float(\"nan\")\n",
    "    if strat_returns.size > 0:\n",
    "        try:\n",
    "            top5 = np.percentile(strat_returns, 95)\n",
    "            bot5 = np.percentile(strat_returns, 5)\n",
    "            tail_top = strat_returns[strat_returns >= top5]\n",
    "            tail_bot = strat_returns[strat_returns <= bot5]\n",
    "            if tail_bot.size > 0:\n",
    "                rachev = tail_top.mean() / abs(tail_bot.mean()) if abs(tail_bot.mean()) > 0 else float(\"nan\")\n",
    "            else:\n",
    "                rachev = float(\"nan\")\n",
    "        except Exception:\n",
    "            rachev = float(\"nan\")\n",
    "    else:\n",
    "        rachev = float(\"nan\")\n",
    "    if \"baseline_dir\" in df_ens.columns:\n",
    "        y_base = df_ens[\"baseline_dir\"].reindex(dates).values[mask].astype(int)\n",
    "        n01 = int(np.sum((y_pred == y_true) & (y_base != y_true)))\n",
    "        n10 = int(np.sum((y_pred != y_true) & (y_base == y_true)))\n",
    "        if n01 + n10 > 0:\n",
    "            mcnemar_stat = (abs(n01 - n10) - 0)**2 / (n01 + n10)\n",
    "            mcnemar_p = 1 - chi2.cdf(mcnemar_stat, 1)\n",
    "        else:\n",
    "            mcnemar_stat = None\n",
    "            mcnemar_p = None\n",
    "    else:\n",
    "        mcnemar_stat = None\n",
    "        mcnemar_p = None\n",
    "    dm_stat = None\n",
    "    dm_p = None\n",
    "    metrics_dict = {\n",
    "        \"featureset\": fs,\n",
    "        \"ensemble_method\": method_key,\n",
    "        \"models_used\": ens.get(\"models_used\", []),\n",
    "        \"n_models\": len(ens.get(\"models_used\", [])),\n",
    "        \"weighting_scheme\": (\"equal\" if method_key==\"mean\" else (\"majority_vote\" if method_key==\"vote\" else \"weighted\")),\n",
    "        \"oof_weight_metric\": ens.get(\"oof_weight_metric\", None),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"brier\": brier,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"rachev\": rachev,\n",
    "        \"DM_stat\": dm_stat,\n",
    "        \"DM_pvalue\": dm_p,\n",
    "        \"McNemar_stat\": mcnemar_stat,\n",
    "        \"McNemar_pvalue\": mcnemar_p\n",
    "    }\n",
    "    json_name = art_metrics / f\"ensemble_{method_key}_{fs}.json\"\n",
    "    with open(json_name, \"w\") as f:\n",
    "        json.dump(metrics_dict, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49e354-b012-4d07-b8cc-4d81015d0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4: Plots for each ensemble\n",
    "for fs in feature_sets:\n",
    "    data = df_forecasts[fs][\"df\"]\n",
    "    dates = pd.to_datetime(data[\"date\"])\n",
    "    # find best model by F1\n",
    "    best_model = None\n",
    "    best_f1 = -1\n",
    "    for model in df_forecasts[fs][\"models\"]:\n",
    "        metrics_file = art_metrics / f\"{model}_{fs}.json\"\n",
    "        if metrics_file.exists():\n",
    "            m = json.load(open(metrics_file))\n",
    "            if m.get(\"f1\",0) > best_f1:\n",
    "                best_f1 = m[\"f1\"]\n",
    "                best_model = model\n",
    "    if best_model is None:\n",
    "        continue\n",
    "    actual_returns = df_feat[\"return\"].reindex(dates).fillna(0).values\n",
    "    cum_bh = np.cumprod(1 + actual_returns) - 1\n",
    "    best_dir = data[f\"{best_model}_dir\"].values\n",
    "    cum_best = np.cumprod(1 + actual_returns * best_dir) - 1\n",
    "    for method_key in [\"mean\", \"w_f1\", \"w_auc\", \"vote\"]:\n",
    "        ens = ensemble_results[(fs, method_key)]\n",
    "        # ROC Curve\n",
    "        y_true = df_feat[\"direction\"].reindex(dates).fillna(0).astype(int).values\n",
    "        y_proba = ens[\"proba\"].values\n",
    "        mask = ~np.isnan(y_proba)\n",
    "        y_true_m = y_true[mask]\n",
    "        y_proba_m = y_proba[mask]\n",
    "        if len(np.unique(y_true_m)) > 1:\n",
    "            fpr, tpr, _ = roc_curve(y_true_m, y_proba_m)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f\"{method_key} (AUC={roc_auc_score(y_true_m,y_proba_m):.2f})\")\n",
    "            plt.plot([0,1], [0,1], 'k--', lw=0.5)\n",
    "            plt.title(f\"ROC Curve - Ensemble {method_key} {fs}\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.legend()\n",
    "            plt.savefig(art_reports / f\"90_ROC_{method_key}_{fs}.png\")\n",
    "            plt.close()\n",
    "        # Cumulative Returns\n",
    "        ens_dir = ens[\"dir\"].values\n",
    "        strat_returns = actual_returns * ens_dir\n",
    "        cum_ens = np.cumprod(1 + strat_returns) - 1\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(dates, cum_bh, label=\"Buy & Hold\")\n",
    "        plt.plot(dates, cum_best, label=f\"Best Model: {best_model}\")\n",
    "        plt.plot(dates, cum_ens, label=f\"Ensemble {method_key}\")\n",
    "        plt.title(f\"Cumulative Returns - Ensemble {method_key} {fs}\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(art_reports / f\"90_CumRet_{method_key}_{fs}.png\")\n",
    "        plt.close()\n",
    "        # Bar metrics (Accuracy, Precision, Recall, F1) vs best model and baseline\n",
    "        metrics_json = {}\n",
    "        ens_metrics_file = art_metrics / f\"ensemble_{method_key}_{fs}.json\"\n",
    "        if ens_metrics_file.exists():\n",
    "            metrics_json[\"ensemble\"] = json.load(open(ens_metrics_file))\n",
    "        best_metrics_file = art_metrics / f\"{best_model}_{fs}.json\"\n",
    "        if best_metrics_file.exists():\n",
    "            metrics_json[\"best_model\"] = json.load(open(best_metrics_file))\n",
    "        base_metrics_file = art_metrics / f\"baseline_{fs}.json\"\n",
    "        if base_metrics_file.exists():\n",
    "            metrics_json[\"baseline\"] = json.load(open(base_metrics_file))\n",
    "        if \"ensemble\" in metrics_json and \"best_model\" in metrics_json and \"baseline\" in metrics_json:\n",
    "            labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "            ens_vals = [metrics_json[\"ensemble\"].get(l.lower(), 0) for l in labels]\n",
    "            best_vals = [metrics_json[\"best_model\"].get(l.lower(), 0) for l in labels]\n",
    "            base_vals = [metrics_json[\"baseline\"].get(l.lower(), 0) for l in labels]\n",
    "            x = np.arange(len(labels))\n",
    "            width = 0.25\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.bar(x - width, ens_vals, width, label=\"Ensemble\")\n",
    "            plt.bar(x, best_vals, width, label=\"Best Model\")\n",
    "            plt.bar(x + width, base_vals, width, label=\"Baseline\")\n",
    "            plt.xticks(x, labels)\n",
    "            plt.ylabel(\"Score\")\n",
    "            plt.title(f\"Metrics Comparison - Ensemble {method_key} {fs}\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(art_reports / f\"90_MetricsBar_{method_key}_{fs}.png\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd10a1-3557-457a-b18f-1a8c12cda1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5: Overview table for all ensembles\n",
    "overview_rows = []\n",
    "metrics_files = art_metrics.glob(\"ensemble_*.json\")\n",
    "for jf in metrics_files:\n",
    "    m = json.load(open(jf))\n",
    "    row = {\n",
    "        \"featureset\": m.get(\"featureset\"),\n",
    "        \"method\": m.get(\"ensemble_method\"),\n",
    "        \"accuracy\": m.get(\"accuracy\"),\n",
    "        \"precision\": m.get(\"precision\"),\n",
    "        \"recall\": m.get(\"recall\"),\n",
    "        \"f1\": m.get(\"f1\"),\n",
    "        \"auc\": m.get(\"auc\"),\n",
    "        \"brier\": m.get(\"brier\"),\n",
    "        \"sharpe\": m.get(\"sharpe\"),\n",
    "        \"sortino\": m.get(\"sortino\"),\n",
    "        \"rachev\": m.get(\"rachev\")\n",
    "    }\n",
    "    overview_rows.append(row)\n",
    "if overview_rows:\n",
    "    df_overview = pd.DataFrame(overview_rows)\n",
    "    df_overview = df_overview.sort_values([\"featureset\", \"method\"]).reset_index(drop=True)\n",
    "    csv_file = art_reports / \"90_uebersicht_ensembles.csv\"\n",
    "    df_overview.to_csv(csv_file, index=False)\n",
    "    fig, ax = plt.subplots(figsize=(8,2))\n",
    "    ax.axis('off')\n",
    "    table_data = df_overview.round(3)\n",
    "    mpl_table = ax.table(cellText=table_data.values, colLabels=table_data.columns, loc='center')\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(8)\n",
    "    mpl_table.scale(1.2, 1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(art_reports / \"90_uebersicht_ensembles.png\")\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
