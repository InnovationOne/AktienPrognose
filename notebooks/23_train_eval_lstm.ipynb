{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3840e86e-abb6-4504-a941-05549b828db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: src-Paket importierbar.\n",
      "INFO: ROOT=C:\\Users\\gamer\\Desktop\\AktienPrognose | DATA_DIR=C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow-Version: 2.20.0\n",
      "Built with CUDA: False\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "Phys. GPUs: []\n",
      "Log. GPUs : []\n"
     ]
    }
   ],
   "source": [
    "# Step0 (GPU-Diagnose & robustes Setup): Seeds, GPU-Check, Pfade, 'src'-Import\n",
    "import os, sys, random, logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# deterministische Seeds\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Logging & TensorFlow-Verbosity reduzieren\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(\"lstm_notebook\")\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# GPU-Diagnose (zeigt, ob TF mit CUDA gebaut wurde und welche Geräte sichtbar sind)\n",
    "print(\"TensorFlow-Version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Phys. GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Log. GPUs :\", tf.config.list_logical_devices(\"GPU\"))\n",
    "\n",
    "# Speicherwachstum setzen (falls GPU vorhanden)\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Projekt-Root finden (verhindert notebooks\\artifacts-Falle)\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    chain = [start, *start.parents]\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\" / \"features_monthly.parquet\").exists():\n",
    "            return p\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\").exists():\n",
    "            return p\n",
    "    for p in chain:\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "    raise AssertionError(\"Project root not found – 'artifacts/data' oder 'config'+'src' erwartet.\")\n",
    "\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "ARTIFACTS     = ROOT / \"artifacts\"\n",
    "DATA_DIR      = ARTIFACTS / \"data\"\n",
    "CONF_DIR      = ROOT / \"config\"\n",
    "FORECASTS_DIR = ARTIFACTS / \"forecasts\"\n",
    "METRICS_DIR   = ARTIFACTS / \"metrics\"\n",
    "REPORTS_DIR   = ARTIFACTS / \"reports\"\n",
    "for p in [DATA_DIR, CONF_DIR, FORECASTS_DIR, METRICS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pfade für 'src' registrieren\n",
    "if str(ROOT) not in sys.path: sys.path.insert(0, str(ROOT))\n",
    "if str(ROOT / \"src\") not in sys.path: sys.path.insert(0, str(ROOT / \"src\"))\n",
    "try:\n",
    "    import importlib; importlib.import_module(\"src\")\n",
    "    logger.info(\"src-Paket importierbar.\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"'src' nicht importierbar (Diagnose): {e}\")\n",
    "\n",
    "logger.info(f\"ROOT={ROOT} | DATA_DIR={DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5198345-b29c-49ad-bf03-9c8188e42bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Geladen: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\data\\features_monthly.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten (erste 20): ['Return_Lag1', '3M_SMA_Return', '12M_SMA_Return', '3M_Momentum', 'Volatility_6M', 'FedFunds_Delta_bps', 'Inflation_YoY_pct', 'UnemploymentRate', 'VIX', 'EPU_US', 'FSI', 'Gold_USD_oz', 'WTI_Spot', 'USD_per_EUR', 'y_return_next_pct', 'y_direction_next']\n",
      "Zeitraum: 2009-02-28 → 2025-05-31 | n= 196\n"
     ]
    }
   ],
   "source": [
    "# Step1 (final): Features laden (Parquet/CSV-Fallback), Datumsindex setzen, Feature-Sets definieren\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Feature-Spalten definieren ---\n",
    "tech_cols = [\"Return_Lag1\", \"3M_SMA_Return\", \"12M_SMA_Return\", \"3M_Momentum\", \"Volatility_6M\"]\n",
    "macro_cols = [\"FedFunds_Delta_bps\", \"Inflation_YoY_pct\", \"UnemploymentRate\", \"VIX\", \"EPU_US\", \"FSI\", \"Gold_USD_oz\", \"WTI_Spot\", \"USD_per_EUR\"]\n",
    "integrated_cols = tech_cols + macro_cols\n",
    "\n",
    "def load_features_df(data_dir: Path) -> pd.DataFrame:\n",
    "    pq  = data_dir / \"features_monthly.parquet\"\n",
    "    csv = data_dir / \"features_monthly.csv\"\n",
    "    if pq.exists():\n",
    "        df_ = pd.read_parquet(pq)\n",
    "        logger.info(f\"Geladen: {pq}\")\n",
    "        return df_\n",
    "    if csv.exists():\n",
    "        df_ = pd.read_csv(csv)\n",
    "        logger.info(f\"Geladen: {csv}\")\n",
    "        return df_\n",
    "    avail = sorted([p.name for p in data_dir.glob(\"*\")])\n",
    "    raise FileNotFoundError(f\"Kein features_monthly.[parquet|csv] in {data_dir}. Gefunden: {avail}\")\n",
    "\n",
    "df = load_features_df(DATA_DIR)\n",
    "\n",
    "# --- Datumsindex sicherstellen ---\n",
    "if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df = df.set_index(\"date\").sort_index()\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "\n",
    "print(\"Spalten (erste 20):\", list(df.columns)[:20])\n",
    "print(\"Zeitraum:\", df.index.min().date(), \"→\", df.index.max().date(), \"| n=\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e99340-6c0c-40cd-87c7-60ad77723112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2009-02-28 → 2019-12-31 | n= 131\n",
      "Test : 2020-01-31 → 2025-05-31 | n= 65\n"
     ]
    }
   ],
   "source": [
    "# Step2: Zeitliche Aufteilung in Trainings- und Test-Daten\n",
    "from src.utils.splits import time_split, split_Xy\n",
    "\n",
    "# Konventioneller Split: Training bis 2019-12, Test ab 2020-01\n",
    "train_df, test_df = time_split(df, train_end=\"2019-12-31\", test_start=\"2020-01-31\")\n",
    "Xtr_full, ytr_reg, ytr_clf = split_Xy(train_df, y_reg=\"y_return_next_pct\", y_clf=\"y_direction_next\")\n",
    "Xte_full, yte_reg, yte_clf = split_Xy(test_df,  y_reg=\"y_return_next_pct\", y_clf=\"y_direction_next\")\n",
    "\n",
    "print(\"Train:\", train_df.index.min().date(), \"→\", train_df.index.max().date(), \"| n=\", len(train_df))\n",
    "print(\"Test :\", test_df.index.min().date(), \"→\", test_df.index.max().date(), \"| n=\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30581453-84f2-451d-a53b-6ccd27def98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Feature-Set: TECH\n",
      "k=6: OOF AUC=0.591 | OOF F1=0.800 | thr=0.00\n",
      "k=9: OOF AUC=0.633 | OOF F1=0.800 | thr=0.00\n",
      "k=12: OOF AUC=0.681 | OOF F1=0.800 | thr=0.00\n",
      "--> Gewähltes k: 12\n",
      "\n",
      ">>> Feature-Set: MACRO\n",
      "k=6: OOF AUC=0.616 | OOF F1=0.800 | thr=0.00\n",
      "k=9: OOF AUC=0.559 | OOF F1=0.800 | thr=0.00\n",
      "k=12: OOF AUC=0.609 | OOF F1=0.821 | thr=0.65\n",
      "--> Gewähltes k: 6\n",
      "\n",
      ">>> Feature-Set: INTEGRATED\n",
      "k=6: OOF AUC=0.616 | OOF F1=0.800 | thr=0.00\n",
      "k=9: OOF AUC=0.455 | OOF F1=0.800 | thr=0.00\n",
      "k=12: OOF AUC=0.450 | OOF F1=0.800 | thr=0.00\n",
      "--> Gewähltes k: 6\n"
     ]
    }
   ],
   "source": [
    "# Step3 (SAFE): TSCV, OOF, Platt-Kalibrierung (pure NumPy), Threshold, bestes k, finales Train\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# --- Stabilitäts-Tweaks (kein SciPy) ---\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "try:\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# --- Sequenzbau ---\n",
    "def build_sequences_matrix(X: np.ndarray, y: np.ndarray, k: int):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(k-1, len(X)):\n",
    "        Xs.append(X[i-k+1:i+1])\n",
    "        ys.append(y[i])\n",
    "    return np.asarray(Xs, dtype=np.float32), np.asarray(ys, dtype=np.int32)\n",
    "\n",
    "# --- LSTM-Factory ---\n",
    "def build_lstm_model(t_steps: int, n_feats: int) -> tf.keras.Model:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(t_steps, n_feats)),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "# --- Platt-Kalibrierung (pure NumPy): p -> sigmoid(a*p + b) ---\n",
    "def platt_fit(p: np.ndarray, y: np.ndarray, lr=0.05, l2=1e-4, max_iter=2000):\n",
    "    # p und y als float32 / int32\n",
    "    p = p.astype(np.float32).clip(1e-6, 1-1e-6)\n",
    "    y = y.astype(np.float32)\n",
    "    a = 0.0\n",
    "    b = 0.0\n",
    "    for it in range(max_iter):\n",
    "        z = a * p + b\n",
    "        q = 1.0 / (1.0 + np.exp(-z))\n",
    "        # Gradienten (Log-Loss) + L2 nur auf 'a'\n",
    "        grad_a = np.mean((q - y) * p) + l2 * a\n",
    "        grad_b = np.mean(q - y)\n",
    "        a -= lr * grad_a\n",
    "        b -= lr * grad_b\n",
    "        # primitive Konvergenzprüfung (optional)\n",
    "        if it % 400 == 0:\n",
    "            loss = -np.mean(y*np.log(q+1e-12) + (1-y)*np.log(1-q+1e-12)) + 0.5*l2*(a*a)\n",
    "    return float(a), float(b)\n",
    "\n",
    "def platt_predict(p: np.ndarray, ab: tuple[float,float]) -> np.ndarray:\n",
    "    a, b = ab\n",
    "    p = p.astype(np.float32).clip(1e-6, 1-1e-6)\n",
    "    z = a * p + b\n",
    "    q = 1.0 / (1.0 + np.exp(-z))\n",
    "    return q.astype(np.float32)\n",
    "\n",
    "# --- Wrapper: kalibrierte Probas (kann später auch sklearn-Calibrator akzeptieren) ---\n",
    "def calibrate_proba(p_raw: np.ndarray, calibrator):\n",
    "    if isinstance(calibrator, tuple) and len(calibrator) == 2:\n",
    "        return platt_predict(p_raw, calibrator)\n",
    "    # Fallback, falls mal ein sklearn-Calibrator übergeben wird\n",
    "    return calibrator.predict_proba(p_raw.reshape(-1,1))[:,1]\n",
    "\n",
    "# --- Hyperparameter & Container ---\n",
    "ks = [6, 9, 12]\n",
    "feature_sets = {\n",
    "    \"TECH\": tech_cols,\n",
    "    \"MACRO\": macro_cols,\n",
    "    \"INTEGRATED\": integrated_cols\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for fs_name, cols in feature_sets.items():\n",
    "    print(f\"\\n>>> Feature-Set: {fs_name}\")\n",
    "    Xtr = Xtr_full[cols].values.astype(np.float32)\n",
    "    ytr = ytr_clf.values.astype(np.int32)\n",
    "\n",
    "    best = {\"k\": None, \"auc\": -np.inf, \"calibrator\": None, \"threshold\": None,\n",
    "            \"oof_f1\": None, \"oof_auc\": None, \"history\": None}\n",
    "\n",
    "    for k in ks:\n",
    "        y_oof, p_oof = [], []\n",
    "\n",
    "        # Expanding 5-Fold: Val=12, Embargo=1\n",
    "        initial_train_end = max(k-1, 65)\n",
    "        embargo = 1\n",
    "        val_size = 12\n",
    "        last_history = None\n",
    "\n",
    "        for fold in range(5):\n",
    "            train_end_idx = initial_train_end + fold*(val_size+embargo)\n",
    "            val_start_idx = train_end_idx + 1 + embargo\n",
    "            val_end_idx   = min(val_start_idx + val_size - 1, len(Xtr)-1)\n",
    "            if val_start_idx >= len(Xtr) or (val_end_idx - (k-1)) < 0:\n",
    "                break\n",
    "\n",
    "            X_tr = Xtr[:train_end_idx+1]\n",
    "            y_tr = ytr[:train_end_idx+1]\n",
    "            X_val = Xtr[val_start_idx:val_end_idx+1]\n",
    "            y_val = ytr[val_start_idx:val_end_idx+1]\n",
    "\n",
    "            Xs_tr, ys_tr = build_sequences_matrix(X_tr, y_tr, k)\n",
    "\n",
    "            # Val-Sequenzen erhalten k-1 Historie\n",
    "            if len(X_tr) >= (k-1):\n",
    "                X_val_aug = np.vstack([X_tr[-(k-1):], X_val])\n",
    "                y_val_aug = np.hstack([y_tr[-(k-1):], y_val])\n",
    "            else:\n",
    "                X_val_aug, y_val_aug = X_val, y_val\n",
    "            Xs_val, ys_val = build_sequences_matrix(X_val_aug, y_val_aug, k)\n",
    "\n",
    "            # Skalierung (nur auf Training)\n",
    "            scaler = StandardScaler().fit(Xs_tr.reshape(-1, Xs_tr.shape[-1]))\n",
    "            Xs_tr  = scaler.transform(Xs_tr.reshape(-1, Xs_tr.shape[-1])).reshape(Xs_tr.shape).astype(np.float32)\n",
    "            Xs_val = scaler.transform(Xs_val.reshape(-1, Xs_val.shape[-1])).reshape(Xs_val.shape).astype(np.float32)\n",
    "\n",
    "            # Modell\n",
    "            model = build_lstm_model(k, len(cols))\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "            # Warmup, um retracing zu vermeiden\n",
    "            _ = model.predict(Xs_tr[:1], verbose=0)\n",
    "\n",
    "            history = model.fit(\n",
    "                Xs_tr, ys_tr,\n",
    "                epochs=15, batch_size=16,\n",
    "                validation_data=(Xs_val, ys_val),\n",
    "                callbacks=[es],\n",
    "                verbose=0\n",
    "            )\n",
    "            last_history = history.history\n",
    "\n",
    "            p_val = model.predict(Xs_val, verbose=0).flatten().astype(np.float32)\n",
    "            y_oof.extend(ys_val.tolist())\n",
    "            p_oof.extend(p_val.tolist())\n",
    "\n",
    "            # Aufräumen pro Fold\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        if len(y_oof) == 0:\n",
    "            continue\n",
    "\n",
    "        y_oof = np.asarray(y_oof, dtype=np.int32)\n",
    "        p_oof = np.asarray(p_oof, dtype=np.float32)\n",
    "\n",
    "        # Platt (pure NumPy)\n",
    "        platt_ab = platt_fit(p_oof, y_oof)\n",
    "        p_oof_cal = platt_predict(p_oof, platt_ab)\n",
    "\n",
    "        # F1-optimaler Threshold\n",
    "        thresholds = np.linspace(0, 1, 101, dtype=np.float32)\n",
    "        f1s = [f1_score(y_oof, (p_oof_cal >= t).astype(int), zero_division=0) for t in thresholds]\n",
    "        best_idx = int(np.nanargmax(f1s))\n",
    "        th = float(thresholds[best_idx])\n",
    "        oof_f1 = float(f1s[best_idx])\n",
    "        oof_auc = float(roc_auc_score(y_oof, p_oof) if len(np.unique(y_oof))>1 else 0.0)\n",
    "\n",
    "        print(f\"k={k}: OOF AUC={oof_auc:.3f} | OOF F1={oof_f1:.3f} | thr={th:.2f}\")\n",
    "\n",
    "        if oof_auc > best[\"auc\"]:\n",
    "            best.update({\n",
    "                \"k\": k, \"auc\": oof_auc, \"calibrator\": platt_ab,\n",
    "                \"threshold\": th, \"oof_f1\": oof_f1, \"oof_auc\": oof_auc, \"history\": last_history\n",
    "            })\n",
    "\n",
    "    # Final: Train mit bestem k\n",
    "    k_best = best[\"k\"]\n",
    "    print(f\"--> Gewähltes k: {k_best}\")\n",
    "    Xs_train_all, ys_train_all = build_sequences_matrix(Xtr, ytr, k_best)\n",
    "    scaler_final = StandardScaler().fit(Xs_train_all.reshape(-1, Xs_train_all.shape[-1]))\n",
    "    Xs_train_all = scaler_final.transform(Xs_train_all.reshape(-1, Xs_train_all.shape[-1])).reshape(Xs_train_all.shape).astype(np.float32)\n",
    "\n",
    "    model_final = build_lstm_model(k_best, len(cols))\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    _ = model_final.predict(Xs_train_all[:1], verbose=0)\n",
    "    history_final = model_final.fit(\n",
    "        Xs_train_all, ys_train_all,\n",
    "        epochs=15, batch_size=16,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[es], verbose=0\n",
    "    )\n",
    "\n",
    "    results[fs_name] = {\n",
    "        \"model\": model_final,\n",
    "        \"scaler\": scaler_final,\n",
    "        \"calibrator\": best[\"calibrator\"],   # (a,b)\n",
    "        \"threshold\": best[\"threshold\"],\n",
    "        \"oof_f1\": best[\"oof_f1\"],\n",
    "        \"oof_auc\": best[\"oof_auc\"],\n",
    "        \"best_k\": k_best,\n",
    "        \"history\": best[\"history\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555a95c7-8b88-46b2-91fa-fa07e580cd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Walk-Forward-Test für TECH\n",
      "Test F1: 0.774, Test AUC: 0.502\n",
      "\n",
      ">>> Walk-Forward-Test für MACRO\n",
      "Test F1: 0.774, Test AUC: 0.565\n",
      "\n",
      ">>> Walk-Forward-Test für INTEGRATED\n",
      "Test F1: 0.774, Test AUC: 0.530\n"
     ]
    }
   ],
   "source": [
    "# Step4 (patched): Walk-Forward-Test nutzt den neuen calibrate_proba()-Wrapper\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "for fs_name, res in results.items():\n",
    "    print(f\"\\n>>> Walk-Forward-Test für {fs_name}\")\n",
    "    cols = feature_sets[fs_name]\n",
    "    k = res[\"best_k\"]\n",
    "\n",
    "    # Kombiniere k-1 Historie\n",
    "    Xtr_tail = Xtr_full[cols].values.astype(np.float32)[-(k-1):]\n",
    "    Xte_vals = Xte_full[cols].values.astype(np.float32)\n",
    "    X_combined = np.vstack([Xtr_tail, Xte_vals])\n",
    "\n",
    "    # Sequenzen nur für Testbereich\n",
    "    Xs_test = []\n",
    "    for i in range(k-1, len(X_combined)):\n",
    "        Xs_test.append(X_combined[i-k+1:i+1])\n",
    "    Xs_test = np.asarray(Xs_test, dtype=np.float32)[:len(test_df)]\n",
    "\n",
    "    # Skalierung\n",
    "    Xs_test = res[\"scaler\"].transform(Xs_test.reshape(-1, len(cols))).reshape(Xs_test.shape).astype(np.float32)\n",
    "\n",
    "    # Roh-Probas\n",
    "    p_test_raw = res[\"model\"].predict(Xs_test, verbose=0).flatten().astype(np.float32)\n",
    "    # Kalibrierte Probas (NumPy-Platt)\n",
    "    p_test_cal = calibrate_proba(p_test_raw, res[\"calibrator\"]).astype(np.float32)\n",
    "\n",
    "    # Klassenvorhersage & Metriken\n",
    "    y_true = yte_clf.values.astype(int)\n",
    "    y_pred = (p_test_cal >= res[\"threshold\"]).astype(int)\n",
    "    test_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    test_auc = roc_auc_score(y_true, p_test_cal) if len(np.unique(y_true))>1 else 0.0\n",
    "    print(f\"Test F1: {test_f1:.3f}, Test AUC: {test_auc:.3f}\")\n",
    "\n",
    "    res[\"y_true\"] = y_true\n",
    "    res[\"y_pred\"] = y_pred\n",
    "    res[\"p_test\"] = p_test_cal\n",
    "    res[\"test_f1\"] = test_f1\n",
    "    res[\"test_auc\"] = test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83afa8a-cf5c-4984-90ba-84b8e36710cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_TECH.json\n",
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_MACRO.json\n",
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_INTEGRATED.json\n"
     ]
    }
   ],
   "source": [
    "# Step5 (SAFE): Metriken, Signifikanz (DM & McNemar) ohne SciPy, JSON speichern\n",
    "import json\n",
    "import numpy as np\n",
    "from math import erf, sqrt\n",
    "\n",
    "def normal_cdf(z: float) -> float:\n",
    "    return 0.5 * (1.0 + erf(z / sqrt(2.0)))\n",
    "\n",
    "def chi2_sf_df1(x: float) -> float:\n",
    "    # Survival-Funktion Chi^2(df=1) ≈ 2 * (1 - Φ(√x))\n",
    "    if x < 0: \n",
    "        return 1.0\n",
    "    return 2.0 * (1.0 - normal_cdf(sqrt(x)))\n",
    "\n",
    "def dm_test_pvalue(e1: np.ndarray, e2: np.ndarray) -> float:\n",
    "    # Einfache DM-Variante (ohne Newey-West auf Monatsdaten i.d.R. ok)\n",
    "    d = e1 - e2\n",
    "    sd = d.std(ddof=1)\n",
    "    if sd == 0 or len(d) < 3:\n",
    "        return 1.0\n",
    "    t = d.mean() / (sd / sqrt(len(d)))\n",
    "    # zweiseitig, Normal-Approx\n",
    "    return float(2 * (1.0 - normal_cdf(abs(t))))\n",
    "\n",
    "summary_rows = []\n",
    "for fs_name, res in results.items():\n",
    "    y_true = res[\"y_true\"].astype(int)\n",
    "    y_pred = res[\"y_pred\"].astype(int)\n",
    "    p      = res[\"p_test\"].astype(float)\n",
    "\n",
    "    # DM-Test (quadratischer Fehler) vs Always-Up\n",
    "    y_always = np.ones_like(y_true)\n",
    "    e_model  = (y_true - p)**2\n",
    "    e_alw    = (y_true - y_always)**2\n",
    "    dm_p_vs_always = dm_test_pvalue(e_model, e_alw)\n",
    "\n",
    "    # McNemar vs Always-Up (Kontinuitätskorrektur)\n",
    "    n01 = int(np.sum((y_true==1) & (y_pred==0)))  # Modell falsch, Always korrekt\n",
    "    n10 = int(np.sum((y_true==0) & (y_pred==0)))  # Modell korrekt, Always falsch\n",
    "    denom = n01 + n10\n",
    "    if denom == 0:\n",
    "        mcnemar_p = 1.0\n",
    "    else:\n",
    "        chi2_corr = (abs(n01 - n10) - 1)**2 / denom\n",
    "        mcnemar_p = chi2_sf_df1(chi2_corr)\n",
    "\n",
    "    metrics = {\n",
    "        \"threshold\": float(res[\"threshold\"]),\n",
    "        \"oof_f1\": float(res[\"oof_f1\"]),\n",
    "        \"oof_auc\": float(res[\"oof_auc\"]),\n",
    "        \"test_f1\": float(res[\"test_f1\"]),\n",
    "        \"test_auc\": float(res[\"test_auc\"]),\n",
    "        \"dm_p_vs_always\": float(dm_p_vs_always),\n",
    "        \"dm_p_vs_rf\": None,\n",
    "        \"mcnemar_p\": float(mcnemar_p),\n",
    "    }\n",
    "    metrics_path = METRICS_DIR / f\"lstm_clf_{fs_name}.json\"\n",
    "    metrics_path.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"OK: {metrics_path}\")\n",
    "    summary_rows.append({\"featureset\": fs_name, **metrics})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef047c26-ef86-48a4-8f3b-613e2d5658b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_TECH.json\n",
      "aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_MACRO.json\n",
      "aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_INTEGRATED.json\n"
     ]
    }
   ],
   "source": [
    "# Step5b: Metriken-JSON um permimp_path ergänzen (Schema-Ergänzung)\n",
    "import json\n",
    "\n",
    "for fs_name, res in results.items():\n",
    "    metrics_path = METRICS_DIR / f\"lstm_clf_{fs_name}.json\"\n",
    "    if metrics_path.exists():\n",
    "        data = json.loads(metrics_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        data = {}\n",
    "    data[\"permimp_path\"] = res.get(\"permimp_path\")\n",
    "    # Platzhalter für cv_details_path (nicht erzeugt in diesem Notebook)\n",
    "    data.setdefault(\"cv_details_path\", None)\n",
    "    metrics_path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"aktualisiert: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb637a6-96b1-4640-bf2b-70cfaa124be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance für TECH\n",
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_TECH_permimp.csv\n",
      "\n",
      "Permutation Importance für MACRO\n",
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_MACRO_permimp.csv\n",
      "\n",
      "Permutation Importance für INTEGRATED\n",
      "OK: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_INTEGRATED_permimp.csv\n"
     ]
    }
   ],
   "source": [
    "# Step6 (revised): Explainability – Permutations-Importance robust (fix: build_sequences_full)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# deterministische Permutation\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "def build_sequences_full(X: np.ndarray, y: np.ndarray, k: int):\n",
    "    # Sequenzbau identisch zu build_sequences_matrix\n",
    "    Xs, ys = [], []\n",
    "    for i in range(k-1, len(X)):\n",
    "        Xs.append(X[i-k+1:i+1])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys, dtype=int)\n",
    "\n",
    "for fs_name, res in results.items():\n",
    "    print(f\"\\nPermutation Importance für {fs_name}\")\n",
    "    cols = feature_sets[fs_name]\n",
    "    k = res[\"best_k\"]\n",
    "    model = res[\"model\"]\n",
    "    scaler = res[\"scaler\"]\n",
    "\n",
    "    # Letztes Trainingsfenster (gesamtes Training) sequenzieren und skalieren\n",
    "    Xs_train, ys_train = build_sequences_full(Xtr_full[cols].values, ytr_clf.values, k)\n",
    "    Xs_train = scaler.transform(Xs_train.reshape(-1, len(cols))).reshape(Xs_train.shape)\n",
    "\n",
    "    # Basis-AUC\n",
    "    p_base = model.predict(Xs_train, verbose=0).flatten()\n",
    "    auc_base = roc_auc_score(ys_train, p_base) if len(np.unique(ys_train))>1 else 0.0\n",
    "\n",
    "    # Feature-weise Permutation (zeitlich synchron je Sequenz)\n",
    "    importances = []\n",
    "    n_seq = Xs_train.shape[0]\n",
    "    for j, feat in enumerate(cols):\n",
    "        X_perm = Xs_train.copy()\n",
    "        idx = rng.permutation(n_seq)\n",
    "        X_perm[:, :, j] = X_perm[idx, :, j]\n",
    "        p_perm = model.predict(X_perm, verbose=0).flatten()\n",
    "        auc_perm = roc_auc_score(ys_train, p_perm) if len(np.unique(ys_train))>1 else 0.0\n",
    "        importances.append((feat, float(auc_base - auc_perm)))\n",
    "\n",
    "    # Top-20 speichern\n",
    "    importances.sort(key=lambda x: x[1], reverse=True)\n",
    "    top20 = importances[:20]\n",
    "    imp_df = pd.DataFrame(top20, columns=[\"feature\", \"importance\"])\n",
    "    permimp_path = METRICS_DIR / f\"lstm_clf_{fs_name}_permimp.csv\"\n",
    "    imp_df.to_csv(permimp_path, index=False)\n",
    "    results[fs_name][\"permimp_path\"] = str(permimp_path)\n",
    "    print(f\"OK: {permimp_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05eabace-d3da-470f-9c80-01b8dfedc339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plots für TECH\n",
      "\n",
      "Plots für MACRO\n",
      "\n",
      "Plots für INTEGRATED\n"
     ]
    }
   ],
   "source": [
    "# Step7: Plots erzeugen (Lernkurven, Confusion, kumulierte Rendite, Balken-Metriken, PermImp)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for fs_name, res in results.items():\n",
    "    print(f\"\\nPlots für {fs_name}\")\n",
    "    # Lernkurve (Loss train vs Val) vom letzten CV-Fold (falls gespeichert)\n",
    "    if res[\"history\"] is not None:\n",
    "        loss = res[\"history\"][\"loss\"]\n",
    "        val_loss = res[\"history\"][\"val_loss\"]\n",
    "        plt.figure()\n",
    "        plt.plot(loss, label=\"Training\")\n",
    "        plt.plot(val_loss, label=\"Validation\")\n",
    "        plt.title(f\"Lernkurve LSTM ({fs_name})\")\n",
    "        plt.xlabel(\"Epoche\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(ROOT / \"artifacts\" / \"reports\" / f\"23_learning_curve_{fs_name}.png\")\n",
    "        plt.close()\n",
    "    # Confusion Matrix auf Testdaten\n",
    "    cm = confusion_matrix(res[\"y_true\"], res[\"y_pred\"])\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(f\"Konfusionsmatrix ({fs_name})\")\n",
    "    plt.xlabel(\"Vorhergesagt\")\n",
    "    plt.ylabel(\"Tatsächlich\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "    plt.savefig(ROOT / \"artifacts\" / \"reports\" / f\"23_confusion_{fs_name}.png\")\n",
    "    plt.close()\n",
    "    # Kumulative Rendite: Strategie vs Always-Up\n",
    "    returns = test_df[\"y_return_next_pct\"].values / 100.0\n",
    "    strat_factor = 1.0\n",
    "    always_factor = 1.0\n",
    "    strat_cum = []\n",
    "    always_cum = []\n",
    "    for r, pred in zip(returns, res[\"y_pred\"]):\n",
    "        always_factor *= (1 + r)\n",
    "        if pred == 1:\n",
    "            strat_factor *= (1 + r)\n",
    "        strat_cum.append(strat_factor - 1)\n",
    "        always_cum.append(always_factor - 1)\n",
    "    plt.figure()\n",
    "    plt.plot(strat_cum, label=\"Modell\")\n",
    "    plt.plot(always_cum, label=\"Always-Up\")\n",
    "    plt.title(f\"Kumulierte Rendite ({fs_name})\")\n",
    "    plt.xlabel(\"Monat\")\n",
    "    plt.ylabel(\"Rendite (kumuliert)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(ROOT / \"artifacts\" / \"reports\" / f\"23_cumret_{fs_name}.png\")\n",
    "    plt.close()\n",
    "    # Balkendiagramm Metriken (Test-F1 vs Always-Up F1)\n",
    "    # Hier: Vergleich der F1-Score des Modells vs Always-Up\n",
    "    model_f1 = res[\"test_f1\"]\n",
    "    always_f1 = f1_score(res[\"y_true\"], np.ones_like(res[\"y_true\"]), zero_division=0)\n",
    "    labels = [\"Modell\", \"Always-Up\"]\n",
    "    plt.figure()\n",
    "    plt.bar(labels, [model_f1, always_f1], color=[\"C0\",\"C1\"])\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(f\"Test-F1 ({fs_name})\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.savefig(ROOT / \"artifacts\" / \"reports\" / f\"23_bar_metrics_{fs_name}.png\")\n",
    "    plt.close()\n",
    "    # Permutations-Importance Balkendiagramm (Top-20)\n",
    "    imp_df = imp_df = pd.read_csv(ROOT / \"artifacts\" / \"metrics\" / f\"lstm_clf_{fs_name}_permimp.csv\")\n",
    "    imp_df = imp_df.sort_values(by=\"importance\", ascending=False).head(10)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.barh(imp_df[\"feature\"][::-1], imp_df[\"importance\"][::-1], color=\"C2\")\n",
    "    plt.title(f\"Top-10 Perm. Importance ({fs_name})\")\n",
    "    plt.xlabel(\"Wertverlust in AUC\")\n",
    "    plt.savefig(ROOT / \"artifacts\" / \"reports\" / f\"23_permimp_{fs_name}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1559a6c-7d84-4c3e-814c-4e588ebf1976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert (.keras): C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\lstm_clf_TECH.keras\n",
      "Forecasts: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\lstm_clf_TECH.csv\n",
      "Metriken aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_TECH.json\n",
      "Gespeichert (.keras): C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\lstm_clf_MACRO.keras\n",
      "Forecasts: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\lstm_clf_MACRO.csv\n",
      "Metriken aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_MACRO.json\n",
      "Gespeichert (.keras): C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\lstm_clf_INTEGRATED.keras\n",
      "Forecasts: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\lstm_clf_INTEGRATED.csv\n",
      "Metriken aktualisiert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_INTEGRATED.json\n"
     ]
    }
   ],
   "source": [
    "# Step8 (revised): Modelle im Keras-Native-Format speichern (.keras) + Pfad in Metriken aktualisieren\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for fs_name, res in results.items():\n",
    "    # .keras-Format (empfohlen)\n",
    "    model_path_keras = (MODELS_DIR := (ROOT / \"artifacts\" / \"models\"))\n",
    "    model_path_keras.mkdir(parents=True, exist_ok=True)\n",
    "    model_path_keras = model_path_keras / f\"lstm_clf_{fs_name}.keras\"\n",
    "    res[\"model\"].save(model_path_keras)\n",
    "    print(f\"Gespeichert (.keras): {model_path_keras}\")\n",
    "\n",
    "    # Forecasts (CSV)\n",
    "    df_forecast = pd.DataFrame({\n",
    "        \"date\": test_df.index,\n",
    "        \"y_true\": res[\"y_true\"],\n",
    "        \"y_pred_prob\": res[\"p_test\"],\n",
    "        \"y_pred\": res[\"y_pred\"]\n",
    "    }).set_index(\"date\")\n",
    "    forecast_path = ROOT / \"artifacts\" / \"forecasts\" / f\"lstm_clf_{fs_name}.csv\"\n",
    "    forecast_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_forecast.to_csv(forecast_path)\n",
    "    print(f\"Forecasts: {forecast_path}\")\n",
    "\n",
    "    # Metriken-JSON um model_path erweitern/anpassen\n",
    "    metrics_path = ROOT / \"artifacts\" / \"metrics\" / f\"lstm_clf_{fs_name}.json\"\n",
    "    if metrics_path.exists():\n",
    "        data = json.loads(metrics_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        data = {}\n",
    "    data[\"model_path\"] = str(model_path_keras)\n",
    "    metrics_path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Metriken aktualisiert: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9af5ca-3ede-4385-98ed-3bc856d4bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert Übersicht CSV: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\lstm_clf_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Step9: Zusammenfassung aller Feature-Sets (CSV und Übersicht-Plot)\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "# CSV speichern\n",
    "summary_csv = ROOT / \"artifacts\" / \"metrics\" / \"lstm_clf_summary.csv\"\n",
    "summary_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"Gespeichert Übersicht CSV: {summary_csv}\")\n",
    "# Balkendiagramm: F1 und AUC pro Feature-Set\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - width/2, summary_df[\"test_f1\"], width, label=\"F1-Score\")\n",
    "ax.bar(x + width/2, summary_df[\"test_auc\"], width, label=\"AUC\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df[\"featureset\"])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel(\"Wert\")\n",
    "ax.set_title(\"Test F1 vs AUC nach Feature-Set\")\n",
    "ax.legend()\n",
    "plt.savefig(ROOT / \"artifacts\" / \"reports\" / \"23_summary.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
