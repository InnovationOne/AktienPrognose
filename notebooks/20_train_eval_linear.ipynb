{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc0d717-0c0d-4fce-ba5b-890b06733252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 17:53:45,872 | INFO | ROOT=C:\\Users\\gamer\\Desktop\\AktienPrognose | DATA_DIR=C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\data\n"
     ]
    }
   ],
   "source": [
    "# %% [step0-setup]\n",
    "\n",
    "\"\"\"\n",
    "Setup & Konstanten:\n",
    "\n",
    "* Importe, Logging, Pfade, Typen\n",
    "* Globale Parameter (Zeiträume, CV-Größen, Grid, Feature-Gruppen-YAML)\n",
    "* Hilfsfunktionen für IO, Index-Checks, Pfaderstellung\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ---------- Logging ----------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"linear_logit\")\n",
    "\n",
    "# --- Artefakt-Pfade einrichten ---\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    chain = [start, *start.parents]\n",
    "    # 1) Bevorzugt: exakte Daten-Datei vorhanden (verhindert notebooks\\artifacts-Falle)\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\" / \"features_monthly.parquet\").exists():\n",
    "            return p\n",
    "    # 2) Falls nur das artifacts/data-Verzeichnis existiert\n",
    "    for p in chain:\n",
    "        if (p / \"artifacts\" / \"data\").exists():\n",
    "            return p\n",
    "    # 3) Klassische Marker (falls du config/src als Root-Kriterium nutzt)\n",
    "    for p in chain:\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "    raise AssertionError(\"Project root not found – expected 'artifacts/data' or 'config'+'src' somewhere above.\")\n",
    "\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "DATA_DIR = ARTIFACTS / \"data\"\n",
    "CONF_DIR = ARTIFACTS / \"config\"\n",
    "FORECASTS_DIR = ARTIFACTS / \"forecasts\"\n",
    "METRICS_DIR = ARTIFACTS / \"metrics\"\n",
    "REPORTS_DIR = ARTIFACTS / \"reports\"\n",
    "MODELS_DIR = ARTIFACTS / \"models\"  # zusätzlich benötigt\n",
    "\n",
    "for p in [DATA_DIR, CONF_DIR, FORECASTS_DIR, METRICS_DIR, REPORTS_DIR, MODELS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"ROOT={ROOT} | DATA_DIR={DATA_DIR}\")\n",
    "\n",
    "# (Alias-Namen beibehalten, damit restlicher Code unverändert bleiben kann)\n",
    "ARTIFACTS_DIR = ARTIFACTS\n",
    "\n",
    "# ---------- Daten-Pfade ----------\n",
    "\n",
    "FEATURES_PARQUET = DATA_DIR / \"features_monthly.parquet\"\n",
    "RAW_PARQUET = DATA_DIR / \"raw_data.parquet\"\n",
    "FEATURE_GROUPS_YAML = CONF_DIR / \"feature_groups.yaml\"  # wird geladen/erstellt\n",
    "\n",
    "# ---------- Zeiträume ----------\n",
    "\n",
    "TRAIN_START = pd.Timestamp(\"2009-02-28\")\n",
    "TRAIN_END = pd.Timestamp(\"2019-12-31\")\n",
    "TEST_START = pd.Timestamp(\"2020-01-31\")\n",
    "TEST_END = pd.Timestamp(\"2025-05-31\")\n",
    "\n",
    "# ---------- CV-Parameter ----------\n",
    "\n",
    "N_SPLITS = 5\n",
    "VAL_SIZE = 12  # Monate\n",
    "EMBARGO = 1   # Gap\n",
    "\n",
    "# ---------- Grid-Search ----------\n",
    "\n",
    "GRID_C = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "# ---------- Reproduzierbarkeit ----------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c267e60e-a2ac-407b-b549-f3ff6ebaacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 17:53:45,921 | INFO | Feature-Sets aus YAML geladen: TECH=5, MACRO=9\n",
      "2025-08-24 17:53:45,922 | INFO | Train 2009-02-28 → 2019-12-31 | n=131\n",
      "2025-08-24 17:53:45,922 | INFO | Test  2020-01-31 → 2025-05-31 | n=65\n"
     ]
    }
   ],
   "source": [
    "# %% [step1-daten_und_featuresets]\n",
    "\n",
    "\"\"\"\n",
    "Daten laden, Spalten prüfen, Feature-Sets laden/erstellen (YAML oder Heuristik)\n",
    "\n",
    "* Lädt features_monthly & raw_data\n",
    "* Zielspalten prüfen\n",
    "* Preisspalte heuristisch bestimmen\n",
    "* Indexe alignen, Zeitfenster schneiden\n",
    "* Feature-Sets (TECH/MACRO/INTEGRATED) laden bzw. heuristisch ableiten und speichern\n",
    "\"\"\"\n",
    "def _ensure_datetime_index(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Sichert, dass ein DatetimeIndex vorliegt (sonst Fehler).\"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(f\"{name} benötigt einen DatetimeIndex.\")\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def _read_parquet_safely(path: Path, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Robustes Laden von Parquet mit klaren Fehlermeldungen.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Fehlende Datei: {path} ({name})\")\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fehler beim Laden von {path}: {e}\") from e\n",
    "    return _ensure_datetime_index(df, name)\n",
    "\n",
    "\n",
    "def _infer_price_column(df_raw: pd.DataFrame) -> str:\n",
    "    \"\"\"Heuristik zur Wahl der Preis-Spalte (S&P 500 o.ä.), bevorzugt GSPC/SP500/adjclose.\"\"\"\n",
    "    candidates_priority = [\n",
    "        \"SP500\",\n",
    "        \"^GSPC_adjclose\",\n",
    "        \"^GSPC\",\n",
    "        \"GSPC\",\n",
    "        \"sp500\",\n",
    "        \"S&P500\",\n",
    "        \"Adj Close\",\n",
    "        \"Adj_Close\",\n",
    "        \"Close\",\n",
    "        \"close\",\n",
    "    ]\n",
    "    for c in candidates_priority:\n",
    "        if c in df_raw.columns:\n",
    "            return c\n",
    "    num_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "    if not num_cols:\n",
    "        raise RuntimeError(\"Keine numerischen Spalten in raw_data gefunden.\")\n",
    "    ac_scores: Dict[str, float] = {}\n",
    "    for c in num_cols:\n",
    "        s = df_raw[c].dropna()\n",
    "        if len(s) < 24:\n",
    "            continue\n",
    "        ac = s.autocorr(lag=1)\n",
    "        ac_scores[c] = ac if not np.isnan(ac) else -999.0\n",
    "    if not ac_scores:\n",
    "        raise RuntimeError(\"Konnte keine geeignete Preis-Spalte heuristisch bestimmen.\")\n",
    "    best = max(ac_scores, key=ac_scores.get)\n",
    "    logger.warning(f\"Preisspalte heuristisch gewählt: {best}\")\n",
    "    return best\n",
    "\n",
    "\n",
    "def _load_or_infer_feature_sets(df_features: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"Lädt TECH/MACRO aus YAML, oder leitet sie heuristisch aus Spaltennamen ab.\"\"\"\n",
    "    target_cols = {\"y_direction_next\", \"y_return_next_pct\"}\n",
    "    available = [c for c in df_features.columns if c not in target_cols]\n",
    "\n",
    "    # YAML vorhanden?\n",
    "    if FEATURE_GROUPS_YAML.exists():\n",
    "        try:\n",
    "            with open(FEATURE_GROUPS_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = yaml.safe_load(f) or {}\n",
    "            tech = cfg.get(\"TECH_FEATURES\", [])\n",
    "            macro = cfg.get(\"MACRO_FEATURES\", [])\n",
    "            assert isinstance(tech, list) and isinstance(macro, list)\n",
    "            tech = [c for c in tech if c in available]\n",
    "            macro = [c for c in macro if c in available and c not in tech]\n",
    "            logger.info(f\"Feature-Sets aus YAML geladen: TECH={len(tech)}, MACRO={len(macro)}\")\n",
    "            if tech and macro:\n",
    "                return {\"TECH\": tech, \"MACRO\": macro, \"INTEGRATED\": sorted(set(tech + macro))}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Feature-Gruppen YAML ungültig/fehlend, nutze Heuristik: {e}\")\n",
    "\n",
    "    # Heuristik\n",
    "    tech_patterns = [\n",
    "        \"SMA\", \"EMA\", \"MA\", \"Momentum\", \"Mom\", \"Volatility\", \"Vol\",\n",
    "        \"Return_Lag\", \"RSI\", \"MACD\", \"Bollinger\", \"BB\", \"ATR\", \"Stoch\"\n",
    "    ]\n",
    "    macro_patterns = [\n",
    "        \"CPI\", \"Inflat\", \"Unemployment\", \"VIX\", \"EPU\", \"FSI\", \"Fed\",\n",
    "        \"Funds\", \"Delta\", \"USD\", \"EUR\", \"WTI\", \"Gold\", \"oil\", \"Brent\",\n",
    "        \"DGS\", \"Yield\", \"Rate\"\n",
    "    ]\n",
    "    tech: List[str] = []\n",
    "    macro: List[str] = []\n",
    "    for c in available:\n",
    "        uc = c.upper()\n",
    "        if any(pat.upper() in uc for pat in tech_patterns):\n",
    "            tech.append(c)\n",
    "        elif any(pat.upper() in uc for pat in macro_patterns):\n",
    "            macro.append(c)\n",
    "        else:\n",
    "            macro.append(c)\n",
    "\n",
    "    tech = sorted(set(tech))\n",
    "    macro = [c for c in sorted(set(macro)) if c not in tech]\n",
    "    if not tech:\n",
    "        tech = [c for c in available if (\"SMA\" in c or \"Mom\" in c or \"Vol\" in c)][:5]\n",
    "    if not macro:\n",
    "        macro = [c for c in available if c not in tech]\n",
    "\n",
    "    integ = sorted(set(tech + macro))\n",
    "    logger.info(f\"Feature-Sets heuristisch bestimmt: TECH={len(tech)}, MACRO={len(macro)}, INTEGRATED={len(integ)}\")\n",
    "\n",
    "    # Speichern\n",
    "    try:\n",
    "        FEATURE_GROUPS_YAML.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(FEATURE_GROUPS_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(\n",
    "                {\"TECH_FEATURES\": tech, \"MACRO_FEATURES\": macro},\n",
    "                f,\n",
    "                sort_keys=False,\n",
    "                allow_unicode=True,\n",
    "            )\n",
    "        logger.info(f\"Feature-Gruppen gespeichert: {FEATURE_GROUPS_YAML}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Konnte Feature-Gruppen nicht speichern: {e}\")\n",
    "\n",
    "    return {\"TECH\": tech, \"MACRO\": macro, \"INTEGRATED\": integ}\n",
    "\n",
    "\n",
    "# --- Laden der Artefakte ---\n",
    "\n",
    "df_features: pd.DataFrame = _read_parquet_safely(FEATURES_PARQUET, \"features_monthly\")\n",
    "df_raw: pd.DataFrame = _read_parquet_safely(RAW_PARQUET, \"raw_data\")\n",
    "\n",
    "# --- Targets prüfen ---\n",
    "\n",
    "required_targets = [\"y_direction_next\", \"y_return_next_pct\"]\n",
    "missing_targets = [c for c in required_targets if c not in df_features.columns]\n",
    "if missing_targets:\n",
    "    raise KeyError(f\"Zielspalten fehlen in features_monthly: {missing_targets}\")\n",
    "\n",
    "# --- Preisreihe bestimmen ---\n",
    "\n",
    "price_col = _infer_price_column(df_raw)\n",
    "s_price = df_raw[price_col].astype(float)\n",
    "\n",
    "# --- Auf gemeinsame Monatsachse beschränken ---\n",
    "\n",
    "common_idx = df_features.index.intersection(s_price.index)\n",
    "df_features = df_features.loc[common_idx].copy()\n",
    "s_price = s_price.loc[common_idx].copy()\n",
    "\n",
    "# --- Feature-Sets laden/ableiten ---\n",
    "\n",
    "FEATURE_GROUPS = _load_or_infer_feature_sets(df_features)\n",
    "\n",
    "# --- Zeitfenster schneiden ---\n",
    "\n",
    "df_train = df_features.loc[(df_features.index >= TRAIN_START) & (df_features.index <= TRAIN_END)].copy()\n",
    "df_test = df_features.loc[(df_features.index >= TEST_START) & (df_features.index <= TEST_END)].copy()\n",
    "s_price_train = s_price.loc[df_train.index]\n",
    "s_price_test = s_price.loc[df_test.index]\n",
    "\n",
    "logger.info(f\"Train {df_train.index.min().date()} → {df_train.index.max().date()} | n={len(df_train)}\")\n",
    "logger.info(f\"Test  {df_test.index.min().date()} → {df_test.index.max().date()} | n={len(df_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cedf164-e090-4ed5-a401-54c2bf92a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step2-tscv_splitter]\n",
    "\n",
    "\"\"\"\n",
    "Zeitreihen-CV (Expanding, 5-Fold) mit 12M Validierung und 1M Embargo (gap).\n",
    "Hilfsfunktionen:\n",
    "\n",
    "* get_tscv(): TimeSeriesSplit Objekt\n",
    "* split_Xy(): Feature/Target-Splits\n",
    "\"\"\"\n",
    "def get_tscv(n_splits: int = N_SPLITS, test_size: int = VAL_SIZE, gap: int = EMBARGO) -> TimeSeriesSplit:\n",
    "    \"\"\"Erzeugt TimeSeriesSplit mit festem Validierungsfenster und Embargo.\"\"\"\n",
    "    return TimeSeriesSplit(n_splits=n_splits, test_size=test_size, gap=gap)\n",
    "\n",
    "\n",
    "def split_Xy(\n",
    "    df: pd.DataFrame,\n",
    "    y_reg: str = \"y_return_next_pct\",\n",
    "    y_clf: str = \"y_direction_next\",\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Teilt Datensatz in X und Ziele (Reg + Clf).\"\"\"\n",
    "    missing = [c for c in (y_reg, y_clf) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing target column(s): {missing}\")\n",
    "    X = df.drop(columns=[y_reg, y_clf])\n",
    "    yr = df[y_reg]\n",
    "    yc = df[y_clf].astype(\"int8\")\n",
    "    return X, yr, yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f4a60a-ab85-4896-a3d9-0cd6f5187afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step3-grid_oof_kalibrierung_threshold]\n",
    "\n",
    "\"\"\"\n",
    "Grid-Search (LR) → OOF-Scores → Platt-Kalibrierung → Schwellen-Optimierung (F1).\n",
    "Optionale OOF-Evaluierung mit SGDClassifier (nur Vergleich, keine Persistenz).\n",
    "Ergebnis je Feature-Set:\n",
    "\n",
    "* best_C\n",
    "* OOF-Entscheidungsscores, kalibrierte OOF-Probas\n",
    "* globaler Schwellenwert theta (F1-optimal)\n",
    "* CV-Details (cv_results_) als CSV\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score, log_loss, roc_auc_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "@dataclass\n",
    "class OOFResult:\n",
    "    best_C: float\n",
    "    oof_scores: np.ndarray          # decision_function Scores (nur für gültige OOF-Indizes)\n",
    "    oof_proba_cal: np.ndarray       # kalibrierte Wahrscheinlichkeiten (Positivklasse), gültige Indizes\n",
    "    threshold: float                # globales theta (F1-optimal)\n",
    "    cv_details_path: Path\n",
    "    coef_per_fold: pd.DataFrame     # Koeffizienten je Fold (für Explainability)\n",
    "    oof_index: pd.Index             # Index der Beobachtungen, für die OOF-Scores existieren\n",
    "\n",
    "class PlattCalibrator:\n",
    "    \"\"\"Einfacher Platt-Kalibrierer (sigmoid) via Logistische Regression auf Scores.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._lr = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "\n",
    "    def fit(self, scores: ArrayLike, y_true: ArrayLike) -> \"PlattCalibrator\":\n",
    "        x = np.asarray(scores, dtype=float).reshape(-1, 1)\n",
    "        y = np.asarray(y_true, dtype=int)\n",
    "        self._lr.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, scores: ArrayLike) -> np.ndarray:\n",
    "        check_is_fitted(self._lr)\n",
    "        x = np.asarray(scores, dtype=float).reshape(-1, 1)\n",
    "        p = self._lr.predict_proba(x)[:, 1]\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def coef_(self) -> np.ndarray:\n",
    "        check_is_fitted(self._lr)\n",
    "        return self._lr.coef_.copy()\n",
    "\n",
    "    @property\n",
    "    def intercept_(self) -> np.ndarray:\n",
    "        check_is_fitted(self._lr)\n",
    "        return self._lr.intercept_.copy()\n",
    "\n",
    "def _build_lr_pipeline(C: float | None = None) -> Pipeline:\n",
    "    \"\"\"Erzeugt Pipeline(StandardScaler -> LogisticRegression).\"\"\"\n",
    "    clf = LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=10000,\n",
    "        C=1.0 if C is None else C,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", clf),\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def _grid_search_lr(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    feature_names: List[str],\n",
    "    set_name: str,\n",
    ") -> Tuple[Pipeline, Path, pd.DataFrame]:\n",
    "    \"\"\"Grid-Search über C, scoring=neg_log_loss, TSCV.\"\"\"\n",
    "    param_grid = {\"clf__C\": GRID_C}\n",
    "    tscv = get_tscv()\n",
    "    pipe = _build_lr_pipeline()\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_log_loss\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    gs.fit(X[feature_names], y)\n",
    "    best_pipe: Pipeline = gs.best_estimator_\n",
    "    # CV-Details speichern\n",
    "    cv_df = pd.DataFrame(gs.cv_results_)\n",
    "    cv_details_path = METRICS_DIR / f\"linear_logit_{set_name}_cv_results.csv\"\n",
    "    cv_df.to_csv(cv_details_path, index=False)\n",
    "    logger.info(f\"[{set_name}] Best C={gs.best_params_['clf__C']} (CV neg_log_loss={gs.best_score_:.4f})\")\n",
    "    return best_pipe, cv_details_path, cv_df\n",
    "\n",
    "def _oof_scores_and_coefs(\n",
    "    pipe: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    feature_names: List[str],\n",
    ") -> Tuple[np.ndarray, pd.DataFrame, pd.Index]:\n",
    "    \"\"\"\n",
    "    Erzeugt OOF decision_function-Scores via TimeSeriesSplit OHNE cross_val_predict.\n",
    "    Gibt (oof_scores_valid, coef_df, oof_index_valid) zurück.\n",
    "    \"\"\"\n",
    "    tscv = get_tscv()\n",
    "    n = len(X)\n",
    "    oof_scores = np.full(shape=n, fill_value=np.nan, dtype=float)\n",
    "    coefs: List[pd.Series] = []\n",
    "\n",
    "    for fold, (tri, vai) in enumerate(tscv.split(X[feature_names], y), start=1):\n",
    "        X_tr, y_tr = X.iloc[tri][feature_names], y.iloc[tri]\n",
    "        X_va = X.iloc[vai][feature_names]\n",
    "\n",
    "        tmp = _build_lr_pipeline(C=pipe.named_steps[\"clf\"].C)\n",
    "        tmp.fit(X_tr, y_tr)\n",
    "\n",
    "        # decision_function auf Validierungs-Indices\n",
    "        scores_va = tmp.decision_function(X_va)\n",
    "        oof_scores[vai] = scores_va\n",
    "\n",
    "        # Koeffizienten speichern\n",
    "        beta = tmp.named_steps[\"clf\"].coef_.ravel()\n",
    "        coefs.append(pd.Series(beta, index=feature_names, name=f\"fold{fold}\"))\n",
    "\n",
    "    coef_df = pd.concat(coefs, axis=1)\n",
    "\n",
    "    valid_mask = np.isfinite(oof_scores)\n",
    "    oof_index_valid = X.index[valid_mask]\n",
    "    oof_scores_valid = oof_scores[valid_mask]\n",
    "\n",
    "    return oof_scores_valid, coef_df, oof_index_valid\n",
    "\n",
    "def _optimize_threshold_f1(y_true: ArrayLike, proba: ArrayLike) -> Tuple[float, Dict[float, float]]:\n",
    "    \"\"\"Sucht Schwelle theta in [0.01, 0.99], die F1 maximiert.\"\"\"\n",
    "    y = np.asarray(y_true, dtype=int)\n",
    "    p = np.asarray(proba, dtype=float)\n",
    "    grid = np.linspace(0.01, 0.99, 99)\n",
    "    f1_map: Dict[float, float] = {}\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        f1 = f1_score(y, yhat, zero_division=0)\n",
    "        f1_map[float(t)] = float(f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), f1_map\n",
    "\n",
    "def run_oof_training_for_set(\n",
    "    set_name: str,\n",
    "    feature_names: List[str],\n",
    "    df_train: pd.DataFrame,\n",
    ") -> Tuple[OOFResult, PlattCalibrator, Pipeline]:\n",
    "    \"\"\"Führt Grid-Search, OOF, Kalibrierung, Threshold-Bestimmung für ein Feature-Set aus.\"\"\"\n",
    "    X_tr, _, y_tr = split_Xy(df_train)\n",
    "\n",
    "    # Grid Search\n",
    "    best_pipe, cv_details_path, _ = _grid_search_lr(X_tr, y_tr, feature_names, set_name)\n",
    "    best_C = float(best_pipe.named_steps[\"clf\"].C)\n",
    "\n",
    "    # OOF Scores + Fold-Koeffizienten (zeitreihen-sicher)\n",
    "    oof_scores, coef_folds, oof_index = _oof_scores_and_coefs(best_pipe, X_tr, y_tr, feature_names)\n",
    "\n",
    "    # Platt-Kalibrierung auf OOF-Scores (nur gültige Indizes)\n",
    "    calibrator = PlattCalibrator().fit(oof_scores, y_tr.loc[oof_index].values)\n",
    "    oof_proba_cal = calibrator.predict_proba(oof_scores)\n",
    "\n",
    "    # Schwellen-Optimierung (F1) auf validem OOF-Ausschnitt\n",
    "    threshold, _ = _optimize_threshold_f1(y_tr.loc[oof_index].values, oof_proba_cal)\n",
    "\n",
    "    # OOF-Qualitätsmetriken (nur Info-Log)\n",
    "    oof_auc = roc_auc_score(y_tr.loc[oof_index].values, oof_proba_cal)\n",
    "    oof_f1 = f1_score(y_tr.loc[oof_index].values, (oof_proba_cal >= threshold).astype(int), zero_division=0)\n",
    "    logger.info(f\"[{set_name}] OOF AUC={oof_auc:.3f} | OOF F1@{threshold:.2f}={oof_f1:.3f} (valid n={len(oof_index)})\")\n",
    "\n",
    "    res = OOFResult(\n",
    "        best_C=best_C,\n",
    "        oof_scores=oof_scores,\n",
    "        oof_proba_cal=oof_proba_cal,\n",
    "        threshold=threshold,\n",
    "        cv_details_path=cv_details_path,\n",
    "        coef_per_fold=coef_folds,\n",
    "        oof_index=oof_index,\n",
    "    )\n",
    "    return res, calibrator, best_pipe\n",
    "\n",
    "# Optional: SGDClassifier nur zur Validierung (nicht persistiert)\n",
    "def sgd_oof_check(\n",
    "    feature_names: List[str],\n",
    "    df_train: pd.DataFrame,\n",
    "    set_name: str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Optionale OOF-Evaluierung mit SGD (log_loss), ohne cross_val_predict.\"\"\"\n",
    "    X_tr, _, y_tr = split_Xy(df_train)\n",
    "    tscv = get_tscv()\n",
    "    proba = np.full(len(X_tr), np.nan, dtype=float)\n",
    "\n",
    "    for (tri, vai) in tscv.split(X_tr[feature_names], y_tr):\n",
    "        X_tr_i, y_tr_i = X_tr.iloc[tri][feature_names], y_tr.iloc[tri]\n",
    "        X_va_i = X_tr.iloc[vai][feature_names]\n",
    "\n",
    "        sgd = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", SGDClassifier(\n",
    "                loss=\"log_loss\", penalty=\"l2\",\n",
    "                max_iter=2000, random_state=RANDOM_STATE,\n",
    "                class_weight=\"balanced\",\n",
    "            )),\n",
    "        ])\n",
    "        sgd.fit(X_tr_i, y_tr_i)\n",
    "        proba[vai] = sgd.predict_proba(X_va_i)[:, 1]\n",
    "\n",
    "    valid = np.isfinite(proba)\n",
    "    y_true = y_tr.values[valid]\n",
    "    p = proba[valid]\n",
    "    yhat = (p >= 0.5).astype(int)\n",
    "\n",
    "    # NEU: clip statt eps-Argument\n",
    "    p_clipped = np.clip(p, 1e-15, 1.0 - 1e-15)\n",
    "\n",
    "    out = {\n",
    "        \"sgd_oof_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"sgd_oof_f1_0p5\": float(f1_score(y_true, yhat, zero_division=0)),\n",
    "        \"sgd_oof_logloss\": float(log_loss(y_true, p_clipped)),\n",
    "    }\n",
    "    logger.info(f\"[{set_name}] SGD OOF AUC={out['sgd_oof_auc']:.3f} | F1@0.5={out['sgd_oof_f1_0p5']:.3f} (valid n={valid.sum()})\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1160a288-3da0-46c6-be3b-e922945fb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step4-walk_forward_test]\n",
    "\n",
    "\"\"\"\n",
    "Walk-Forward-Test (Expanding Origin):\n",
    "\n",
    "* Für jeden Testmonat t: Train = [TRAIN_START .. t-1], Modell neu fitten (C=best_C), Proba(t)\n",
    "* Kalibrierer aus OOF (fix) anwenden, globalen Threshold anwenden\n",
    "* Forecast-CSV je Feature-Set ausgeben\n",
    "* Kumulierte Renditen (Long/Cash) vorbereiten\n",
    "\"\"\"\n",
    "def monthly_return_pct(price: pd.Series) -> pd.Series:\n",
    "    \"\"\"Einfache Monatsrendite in %.\"\"\"\n",
    "    return 100.0 * (price / price.shift(1) - 1.0)\n",
    "\n",
    "\n",
    "def _compute_strategy_returns(\n",
    "    idx: pd.DatetimeIndex,\n",
    "    pred_dir: pd.Series,          # 0/1 Klassensignal für Monate im Test\n",
    "    s_price: pd.Series,           # Preislevel (Monatsultimo)\n",
    ") -> pd.Series:\n",
    "    \"\"\"Long/Cash-Strategie: investiere in Up-Monaten, sonst 0%.\"\"\"\n",
    "    # Align Renditen mit Testindex\n",
    "    ret = monthly_return_pct(s_price).reindex(idx).astype(float)\n",
    "    strat_ret = ret * pred_dir.astype(int)  # in % Einheiten\n",
    "    return strat_ret\n",
    "\n",
    "\n",
    "def run_walk_forward(\n",
    "    set_name: str,\n",
    "    feature_names: List[str],\n",
    "    df_full: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    s_price_full: pd.Series,\n",
    "    best_C: float,\n",
    "    calibrator: PlattCalibrator,\n",
    "    threshold: float,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Expanding-Origin Walk-Forward auf dem Testfenster, mit globalem Kalibrator & Threshold.\"\"\"\n",
    "    X_full, _, y_full = split_Xy(df_full)\n",
    "    X_tr0, _, y_tr0 = split_Xy(df_train)\n",
    "    X_te0, _, y_te0 = split_Xy(df_test)\n",
    "\n",
    "    test_idx = X_te0.index\n",
    "    preds: List[float] = []\n",
    "    probas_raw: List[float] = []\n",
    "    probas_cal: List[float] = []\n",
    "\n",
    "    # Initiales Train-Ende\n",
    "    all_idx = X_full.index\n",
    "    for t in test_idx:\n",
    "        # Trainingsfenster = alles < t (expanding)\n",
    "        train_mask = all_idx < t\n",
    "        X_tr_t = X_full.loc[train_mask, feature_names]\n",
    "        y_tr_t = y_full.loc[train_mask]\n",
    "\n",
    "        # Fit mit best_C\n",
    "        pipe = _build_lr_pipeline(C=best_C)\n",
    "        pipe.fit(X_tr_t, y_tr_t)\n",
    "\n",
    "        # Decision score & kalibrierte Proba für Monat t\n",
    "        score_t = float(pipe.decision_function(X_te0.loc[[t], feature_names])[0])\n",
    "        proba_t = float(calibrator.predict_proba([score_t])[0])\n",
    "        yhat_t = 1.0 if proba_t >= threshold else 0.0\n",
    "\n",
    "        probas_raw.append(float(1.0 / (1.0 + math.exp(-score_t))))  # Sigmoid(score) rein informativ\n",
    "        probas_cal.append(proba_t)\n",
    "        preds.append(yhat_t)\n",
    "\n",
    "    df_forecast = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": test_idx,\n",
    "            \"y_true\": y_te0.values.astype(int),\n",
    "            \"y_pred\": np.array(preds, dtype=int),\n",
    "            \"proba_cal\": np.array(probas_cal, dtype=float),\n",
    "            \"proba_raw_sigmoid\": np.array(probas_raw, dtype=float),\n",
    "            \"featureset\": set_name,\n",
    "            \"model_class\": \"Linear\",\n",
    "        }\n",
    "    ).set_index(\"date\")\n",
    "\n",
    "    # Strategie-Renditen (Long/Cash) + Buy&Hold\n",
    "    strat_ret = _compute_strategy_returns(df_forecast.index, df_forecast[\"y_pred\"], s_price_full)\n",
    "    bh_ret = monthly_return_pct(s_price_full).reindex(df_forecast.index).astype(float)\n",
    "\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": df_forecast.index,\n",
    "            \"ret_model_pct\": strat_ret.values,\n",
    "            \"ret_bh_pct\": bh_ret.values,\n",
    "        }\n",
    "    ).set_index(\"date\")\n",
    "\n",
    "    # Persist Forecast\n",
    "    out_path = FORECASTS_DIR / f\"linear_logit_{set_name}.csv\"\n",
    "    df_forecast.to_csv(out_path, index=True)\n",
    "    logger.info(f\"[{set_name}] Forecast gespeichert: {out_path}\")\n",
    "\n",
    "    return df_forecast, df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840bee67-c9d3-481e-8adf-cb14dff9d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step5-metriken_signifikanz_json]\n",
    "\n",
    "\"\"\"\n",
    "Metriken (OOF & Test) + Signifikanztests:\n",
    "\n",
    "* OOF: AUC, F1, LogLoss, Brier\n",
    "* Test: AUC, F1, Acc, LogLoss, Brier\n",
    "* DM-Test (0/1-Loss) vs. Always-Up & vs. Persistence\n",
    "* McNemar vs. Always-Up\n",
    "* JSON je Feature-Set speichern\n",
    "\"\"\"\n",
    "def _binary_loss(y_true: ArrayLike, y_pred: ArrayLike) -> np.ndarray:\n",
    "    \"\"\"0/1-Loss je Beobachtung.\"\"\"\n",
    "    y = np.asarray(y_true, dtype=int)\n",
    "    yhat = np.asarray(y_pred, dtype=int)\n",
    "    return (y != yhat).astype(int)\n",
    "\n",
    "def diebold_mariano_01loss(e1: ArrayLike, e2: ArrayLike, h: int = 1) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Sehr vereinfachter DM-Test für Differenzen der 0/1-Loss-Reihe (h=1).\n",
    "    Gibt (DM-Statistik, p-Wert) zurück. Hinweis: Für Bachelor-Zwecke ausreichend.\n",
    "    \"\"\"\n",
    "    d = np.asarray(e1, dtype=float) - np.asarray(e2, dtype=float)\n",
    "    d = d[np.isfinite(d)]\n",
    "    T = len(d)\n",
    "    if T < 8:\n",
    "        return np.nan, np.nan\n",
    "    dbar = d.mean()\n",
    "    var = d.var(ddof=1)  # Newey-West mit Lag 0\n",
    "    if var <= 0:\n",
    "        return np.nan, np.nan\n",
    "    dm = dbar / math.sqrt(var / T)\n",
    "    p = 2.0 * (1.0 - 0.5 * (1 + math.erf(abs(dm) / math.sqrt(2))))\n",
    "    return float(dm), float(p)\n",
    "\n",
    "def mcnemar_test(y_true: ArrayLike, y_pred_a: ArrayLike, y_pred_b: ArrayLike) -> Tuple[int, int, float]:\n",
    "    \"\"\"\n",
    "    McNemar-Test (approx. Chi^2 mit Kontinuitätskorrektur).\n",
    "    Rückgabe: (b, c, p-Wert)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y_true, dtype=int)\n",
    "    a = np.asarray(y_pred_a, dtype=int)\n",
    "    b = np.asarray(y_pred_b, dtype=int)\n",
    "    correct_a = (a == y).astype(int)\n",
    "    correct_b = (b == y).astype(int)\n",
    "    b_count = int(((correct_a == 1) & (correct_b == 0)).sum())\n",
    "    c_count = int(((correct_a == 0) & (correct_b == 1)).sum())\n",
    "    chi2 = (abs(b_count - c_count) - 1) ** 2 / (b_count + c_count + 1e-12)\n",
    "    z = math.sqrt(chi2)\n",
    "    p = 2.0 * (1.0 - 0.5 * (1 + math.erf(z / math.sqrt(2))))\n",
    "    return b_count, c_count, float(p)\n",
    "\n",
    "def evaluate_and_write_metrics(\n",
    "    set_name: str,\n",
    "    oof: OOFResult,\n",
    "    calibrator: PlattCalibrator,\n",
    "    df_forecast: pd.DataFrame,\n",
    "    coef_path: Path,\n",
    "    permimp_path: Path,\n",
    "    cv_details_path: Path,\n",
    ") -> Path:\n",
    "    \"\"\"Berechnet Metriken & Tests, schreibt JSON und gibt Pfad zurück.\"\"\"\n",
    "    # --- OOF (nur gültiger OOF-Ausschnitt) ---\n",
    "    y_oof = df_train.loc[oof.oof_index, \"y_direction_next\"].values.astype(int)\n",
    "    p_oof = oof.oof_proba_cal\n",
    "    yhat_oof = (p_oof >= oof.threshold).astype(int)\n",
    "    # NEU: clip statt eps-Argument\n",
    "    p_oof_clipped = np.clip(p_oof, 1e-15, 1.0 - 1e-15)\n",
    "    oof_auc = float(roc_auc_score(y_oof, p_oof))\n",
    "    oof_f1 = float(f1_score(y_oof, yhat_oof, zero_division=0))\n",
    "    oof_ll = float(log_loss(y_oof, p_oof_clipped))\n",
    "    oof_brier = float(brier_score_loss(y_oof, p_oof))\n",
    "\n",
    "    # --- Test ---\n",
    "    y_te = df_forecast[\"y_true\"].values.astype(int)\n",
    "    p_te = df_forecast[\"proba_cal\"].values.astype(float)\n",
    "    yhat_te = df_forecast[\"y_pred\"].values.astype(int)\n",
    "    # NEU: clip statt eps-Argument\n",
    "    p_te_clipped = np.clip(p_te, 1e-15, 1.0 - 1e-15)\n",
    "    test_auc = float(roc_auc_score(y_te, p_te))\n",
    "    test_f1 = float(f1_score(y_te, yhat_te, zero_division=0))\n",
    "    test_acc = float(accuracy_score(y_te, yhat_te))\n",
    "    test_ll = float(log_loss(y_te, p_te_clipped))\n",
    "    test_brier = float(brier_score_loss(y_te, p_te))\n",
    "\n",
    "    # --- Baselines für Tests ---\n",
    "    yhat_up = np.ones_like(y_te, dtype=int)\n",
    "    y_all = pd.concat([df_train[\"y_direction_next\"], df_test[\"y_direction_next\"]])\n",
    "    y_persist = y_all.shift(1).reindex(df_forecast.index).fillna(method=\"ffill\").astype(int).values\n",
    "\n",
    "    # --- DM-Test (0/1-Loss) ---\n",
    "    loss_model = _binary_loss(y_te, yhat_te)\n",
    "    loss_up = _binary_loss(y_te, yhat_up)\n",
    "    loss_persist = _binary_loss(y_te, y_persist)\n",
    "    dm_up, p_up = diebold_mariano_01loss(loss_model, loss_up, h=1)\n",
    "    dm_pers, p_pers = diebold_mariano_01loss(loss_model, loss_persist, h=1)\n",
    "\n",
    "    # --- McNemar vs Always-Up ---\n",
    "    b_cnt, c_cnt, p_mcnemar = mcnemar_test(y_te, yhat_te, yhat_up)\n",
    "\n",
    "    # --- JSON schreiben ---\n",
    "    metrics = {\n",
    "        \"featureset\": set_name,\n",
    "        \"model_class\": \"Linear\",\n",
    "        \"cv_details_path\": str(cv_details_path),\n",
    "        \"coef_path\": str(coef_path),\n",
    "        \"permimp_path\": str(permimp_path),\n",
    "        \"threshold\": float(oof.threshold),\n",
    "        \"oof_auc\": oof_auc,\n",
    "        \"oof_f1\": oof_f1,\n",
    "        \"oof_logloss\": oof_ll,\n",
    "        \"oof_brier\": oof_brier,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_logloss\": test_ll,\n",
    "        \"test_brier\": test_brier,\n",
    "        \"dm_vs_always_up\": {\"stat\": dm_up, \"pvalue\": p_up},\n",
    "        \"dm_vs_persistence\": {\"stat\": dm_pers, \"pvalue\": p_pers},\n",
    "        \"mcnemar_vs_always_up\": {\"b\": b_cnt, \"c\": c_cnt, \"pvalue\": p_mcnemar},\n",
    "        \"oof_valid_n\": int(len(oof.oof_index)),\n",
    "    }\n",
    "    out_path = METRICS_DIR / f\"linear_logit_{set_name}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    logger.info(f\"[{set_name}] Metrics gespeichert: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede76301-dbdc-4087-80a5-0b5da4584a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step6-explainability]\n",
    "\n",
    "\"\"\"\n",
    "Explainability:\n",
    "\n",
    "* Koeffizienten je Fold + Median -> CSV\n",
    "* Permutation Importance (log-loss) im letzten Train-Fenster -> Top-20 -> CSV\n",
    "\"\"\"\n",
    "def save_coef_profiles(\n",
    "    set_name: str,\n",
    "    coef_folds: pd.DataFrame,\n",
    ") -> Path:\n",
    "    \"\"\"Speichert Fold-Koeffizienten + Median als CSV.\"\"\"\n",
    "    coef_df = coef_folds.copy()\n",
    "    coef_df[\"median\"] = coef_df.median(axis=1)\n",
    "    out_path = METRICS_DIR / f\"linear_logit_{set_name}_coefs.csv\"\n",
    "    coef_df.to_csv(out_path)\n",
    "    logger.info(f\"[{set_name}] Koeffizienten gespeichert: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def compute_perm_importance_topk(\n",
    "    set_name: str,\n",
    "    feature_names: List[str],\n",
    "    df_train: pd.DataFrame,\n",
    "    best_C: float,\n",
    "    k: int = 20,\n",
    ") -> Path:\n",
    "    \"\"\"Permutation Importance im letzten Train-Fenster, scoring=neg_log_loss.\"\"\"\n",
    "    X_tr, _, y_tr = split_Xy(df_train)\n",
    "    pipe = _build_lr_pipeline(C=best_C)\n",
    "    pipe.fit(X_tr[feature_names], y_tr)\n",
    "    r = permutation_importance(\n",
    "        estimator=pipe,\n",
    "        X=X_tr[feature_names],\n",
    "        y=y_tr,\n",
    "        scoring=\"neg_log_loss\",\n",
    "        n_repeats=30,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    imp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_names,\n",
    "            \"importance_mean\": r.importances_mean,\n",
    "            \"importance_std\": r.importances_std,\n",
    "        }\n",
    "    ).sort_values(\"importance_mean\", ascending=False)\n",
    "    imp_top = imp_df.head(k)\n",
    "    out_path = METRICS_DIR / f\"linear_logit_{set_name}_permimp_top{k}.csv\"\n",
    "    imp_top.to_csv(out_path, index=False)\n",
    "    logger.info(f\"[{set_name}] Permutation Importance Top-{k} gespeichert: {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ac40cd-350a-48bf-b3a5-dbccf0822105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step7-plots]\n",
    "\n",
    "\"\"\"\n",
    "Plots (matplotlib-only):\n",
    "\n",
    "* Kumulierte Renditen (Modell vs. Buy&Hold)\n",
    "* Balken: Test-AUC & Test-F1 über Feature-Sets\n",
    "* Top-20 Koef (Median) & Perm-Imp (Top-20)\n",
    "  Dateien werden unter artifacts/reports/20_* gespeichert.\n",
    "\"\"\"\n",
    "def _to_growth(ret_pct: pd.Series) -> pd.Series:\n",
    "    \"\"\"Wandelt Monatsrendite in % in Wachstumsmultiplikator um und kumuliert.\"\"\"\n",
    "    g = (1.0 + ret_pct.fillna(0.0) / 100.0).cumprod()\n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_cum_returns(set_name: str, df_perf: pd.DataFrame) -> Path:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    g_model = _to_growth(df_perf[\"ret_model_pct\"])\n",
    "    g_bh = _to_growth(df_perf[\"ret_bh_pct\"])\n",
    "    ax.plot(g_model.index, g_model.values, label=\"Modell (Long/Cash)\")\n",
    "    ax.plot(g_bh.index, g_bh.values, label=\"Buy&Hold\")\n",
    "    ax.set_title(f\"Kumulierte Rendite – {set_name}\")\n",
    "    ax.set_xlabel(\"Datum\")\n",
    "    ax.set_ylabel(\"Wachstum (Start=1.0)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    out = REPORTS_DIR / f\"20_cumret_{set_name}.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out, dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_bar_metrics_overview(summary: pd.DataFrame) -> Path:\n",
    "    \"\"\"Balken-Plot Test-AUC & Test-F1 je Feature-Set.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(len(summary))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, summary[\"test_auc\"], width, label=\"AUC\")\n",
    "    ax.bar(x + width/2, summary[\"test_f1\"], width, label=\"F1\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(summary.index, rotation=0)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"Test-Metriken je Feature-Set\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    ax.legend()\n",
    "    out = REPORTS_DIR / f\"20_metrics_overview.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out, dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_top20_coefficients(set_name: str, coef_csv: Path) -> Path:\n",
    "    \"\"\"Horizontale Balken der Top-20 |Median-Koeffizienten|.\"\"\"\n",
    "    df = pd.read_csv(coef_csv, index_col=0)\n",
    "    med = df[\"median\"].copy()\n",
    "    top = med.reindex(med.abs().sort_values(ascending=False).head(20).index)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(top.index, top.values)\n",
    "    ax.set_title(f\"Top-20 |Median-Koeffizienten| – {set_name}\")\n",
    "    ax.set_xlabel(\"Koeffizient\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "    out = REPORTS_DIR / f\"20_coef_top20_{set_name}.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out, dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_top20_permimp(set_name: str, permimp_csv: Path) -> Path:\n",
    "    \"\"\"Horizontale Balken der Top-20 Permutation-Importance.\"\"\"\n",
    "    df = pd.read_csv(permimp_csv)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.barh(df[\"feature\"], df[\"importance_mean\"])\n",
    "    ax.set_title(f\"Permutation Importance Top-20 – {set_name}\")\n",
    "    ax.set_xlabel(\"Δ(neg_log_loss)\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "    out = REPORTS_DIR / f\"20_permimp_top20_{set_name}.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out, dpi=150)\n",
    "    plt.close(fig)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f49396-a569-4bcf-9a0b-ba41f70c0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [step8-persistenz]\n",
    "\n",
    "\"\"\"\n",
    "Persistenz:\n",
    "\n",
    "* Speichert finales LR-Modell (auf komplettem Trainingsfenster) und Platt-Kalibrator\n",
    "\"\"\"\n",
    "def persist_model_and_calibrator(\n",
    "    set_name: str,\n",
    "    feature_names: List[str],\n",
    "    df_train: pd.DataFrame,\n",
    "    best_C: float,\n",
    "    calibrator: PlattCalibrator,\n",
    ") -> Tuple[Path, Path]:\n",
    "    X_tr, _, y_tr = split_Xy(df_train)\n",
    "    pipe = _build_lr_pipeline(C=best_C)\n",
    "    pipe.fit(X_tr[feature_names], y_tr)\n",
    "\n",
    "    model_path = MODELS_DIR / f\"linear_logit_{set_name}.pkl\"\n",
    "    cal_path = MODELS_DIR / f\"linear_logit_{set_name}_calibrator.pkl\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(calibrator, cal_path)\n",
    "    logger.info(f\"[{set_name}] Modell gespeichert: {model_path}\")\n",
    "    logger.info(f\"[{set_name}] Kalibrator gespeichert: {cal_path}\")\n",
    "    return model_path, cal_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e9c343-df01-48d0-9578-9cf668277357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 17:53:46,035 | INFO | === Processing Feature-Set: TECH (d=5) ===\n",
      "2025-08-24 17:53:49,773 | INFO | [TECH] Best C=0.1 (CV neg_log_loss=-0.7116)\n",
      "2025-08-24 17:53:49,926 | INFO | [TECH] OOF AUC=0.511 | OOF F1@0.01=0.824 (valid n=60)\n",
      "2025-08-24 17:53:49,952 | INFO | [TECH] SGD OOF AUC=0.388 | F1@0.5=0.479 (valid n=60)\n",
      "2025-08-24 17:53:50,282 | INFO | [TECH] Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\linear_logit_TECH.csv\n",
      "2025-08-24 17:53:50,286 | INFO | [TECH] Koeffizienten gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_TECH_coefs.csv\n",
      "2025-08-24 17:53:50,449 | INFO | [TECH] Permutation Importance Top-20 gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_TECH_permimp_top20.csv\n",
      "C:\\Users\\gamer\\AppData\\Local\\Temp\\ipykernel_6492\\509333387.py:89: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y_persist = y_all.shift(1).reindex(df_forecast.index).fillna(method=\"ffill\").astype(int).values\n",
      "2025-08-24 17:53:50,460 | INFO | [TECH] Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_TECH.json\n",
      "2025-08-24 17:53:50,467 | INFO | [TECH] Modell gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_TECH.pkl\n",
      "2025-08-24 17:53:50,467 | INFO | [TECH] Kalibrator gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_TECH_calibrator.pkl\n",
      "2025-08-24 17:53:50,844 | INFO | === Processing Feature-Set: MACRO (d=9) ===\n",
      "2025-08-24 17:53:50,904 | INFO | [MACRO] Best C=0.1 (CV neg_log_loss=-0.9494)\n",
      "2025-08-24 17:53:51,053 | INFO | [MACRO] OOF AUC=0.693 | OOF F1@0.48=0.830 (valid n=60)\n",
      "2025-08-24 17:53:51,079 | INFO | [MACRO] SGD OOF AUC=0.330 | F1@0.5=0.412 (valid n=60)\n",
      "2025-08-24 17:53:51,431 | INFO | [MACRO] Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\linear_logit_MACRO.csv\n",
      "2025-08-24 17:53:51,433 | INFO | [MACRO] Koeffizienten gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_MACRO_coefs.csv\n",
      "2025-08-24 17:53:51,617 | INFO | [MACRO] Permutation Importance Top-20 gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_MACRO_permimp_top20.csv\n",
      "C:\\Users\\gamer\\AppData\\Local\\Temp\\ipykernel_6492\\509333387.py:89: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y_persist = y_all.shift(1).reindex(df_forecast.index).fillna(method=\"ffill\").astype(int).values\n",
      "2025-08-24 17:53:51,627 | INFO | [MACRO] Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_MACRO.json\n",
      "2025-08-24 17:53:51,634 | INFO | [MACRO] Modell gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_MACRO.pkl\n",
      "2025-08-24 17:53:51,636 | INFO | [MACRO] Kalibrator gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_MACRO_calibrator.pkl\n",
      "2025-08-24 17:53:52,039 | INFO | === Processing Feature-Set: INTEGRATED (d=14) ===\n",
      "2025-08-24 17:53:52,098 | INFO | [INTEGRATED] Best C=0.1 (CV neg_log_loss=-0.9666)\n",
      "2025-08-24 17:53:52,251 | INFO | [INTEGRATED] OOF AUC=0.642 | OOF F1@0.54=0.826 (valid n=60)\n",
      "2025-08-24 17:53:52,276 | INFO | [INTEGRATED] SGD OOF AUC=0.437 | F1@0.5=0.455 (valid n=60)\n",
      "2025-08-24 17:53:52,637 | INFO | [INTEGRATED] Forecast gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\forecasts\\linear_logit_INTEGRATED.csv\n",
      "2025-08-24 17:53:52,639 | INFO | [INTEGRATED] Koeffizienten gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_INTEGRATED_coefs.csv\n",
      "2025-08-24 17:53:52,855 | INFO | [INTEGRATED] Permutation Importance Top-20 gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_INTEGRATED_permimp_top20.csv\n",
      "C:\\Users\\gamer\\AppData\\Local\\Temp\\ipykernel_6492\\509333387.py:89: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y_persist = y_all.shift(1).reindex(df_forecast.index).fillna(method=\"ffill\").astype(int).values\n",
      "2025-08-24 17:53:52,866 | INFO | [INTEGRATED] Metrics gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_INTEGRATED.json\n",
      "2025-08-24 17:53:52,874 | INFO | [INTEGRATED] Modell gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_INTEGRATED.pkl\n",
      "2025-08-24 17:53:52,874 | INFO | [INTEGRATED] Kalibrator gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\models\\linear_logit_INTEGRATED_calibrator.pkl\n",
      "2025-08-24 17:53:53,326 | INFO | Übersicht gespeichert: C:\\Users\\gamer\\Desktop\\AktienPrognose\\artifacts\\metrics\\linear_logit_overview.csv\n",
      "2025-08-24 17:53:53,417 | INFO | === Done: Lineare Klassifikatoren (Logit) für TECH/MACRO/INTEGRATED ===\n"
     ]
    }
   ],
   "source": [
    "# %% [step9-uebersicht20-driver]\n",
    "\n",
    "\"\"\"\n",
    "DRIVER: Führt alles je Feature-Set (TECH/MACRO/INTEGRATED) aus und erstellt Übersicht (CSV+PNG).\n",
    "\n",
    "* OOF/GS/Kalibrierung/Threshold\n",
    "* Optionaler SGD-Check\n",
    "* Walk-Forward + Forecast-CSV\n",
    "* Explainability CSVs + Plots\n",
    "* Metriken/Signifikanz + JSON\n",
    "* Persistenz\n",
    "* Gesamttabelle\n",
    "\"\"\"\n",
    "results_summary: Dict[str, Dict[str, float]] = {}\n",
    "metrics_paths: Dict[str, Path] = {}\n",
    "\n",
    "for set_name, feature_names in FEATURE_GROUPS.items():\n",
    "    logger.info(f\"=== Processing Feature-Set: {set_name} (d={len(feature_names)}) ===\")\n",
    "\n",
    "    # 1) OOF + Kalibrierung + Threshold\n",
    "    oof_res, platt_cal, _best_pipe = run_oof_training_for_set(set_name, feature_names, df_train)\n",
    "\n",
    "    # 2) Optional: SGD OOF-Check (nicht persistiert)\n",
    "    _ = sgd_oof_check(feature_names, df_train, set_name)\n",
    "\n",
    "    # 3) Walk-Forward-Test\n",
    "    df_forecast, df_perf = run_walk_forward(\n",
    "        set_name=set_name,\n",
    "        feature_names=feature_names,\n",
    "        df_full=df_features,\n",
    "        df_train=df_train,\n",
    "        df_test=df_test,\n",
    "        s_price_full=s_price,\n",
    "        best_C=oof_res.best_C,\n",
    "        calibrator=platt_cal,\n",
    "        threshold=oof_res.threshold,\n",
    "    )\n",
    "\n",
    "    # 4) Explainability\n",
    "    coef_csv = save_coef_profiles(set_name, oof_res.coef_per_fold)\n",
    "    permimp_csv = compute_perm_importance_topk(set_name, feature_names, df_train, best_C=oof_res.best_C, k=20)\n",
    "\n",
    "    # 5) Metriken + Tests + JSON\n",
    "    metrics_json_path = evaluate_and_write_metrics(\n",
    "        set_name=set_name,\n",
    "        oof=oof_res,\n",
    "        calibrator=platt_cal,\n",
    "        df_forecast=df_forecast,\n",
    "        coef_path=coef_csv,\n",
    "        permimp_path=permimp_csv,\n",
    "        cv_details_path=oof_res.cv_details_path,\n",
    "    )\n",
    "    metrics_paths[set_name] = metrics_json_path\n",
    "\n",
    "    # 6) Persistenz (Modell + Kalibrator)\n",
    "    model_path, cal_path = persist_model_and_calibrator(\n",
    "        set_name=set_name,\n",
    "        feature_names=feature_names,\n",
    "        df_train=df_train,\n",
    "        best_C=oof_res.best_C,\n",
    "        calibrator=platt_cal,\n",
    "    )\n",
    "\n",
    "    # 7) Plots\n",
    "    cumret_png = plot_cum_returns(set_name, df_perf)\n",
    "    coef_png = plot_top20_coefficients(set_name, coef_csv)\n",
    "    permimp_png = plot_top20_permimp(set_name, permimp_csv)\n",
    "\n",
    "    # 8) Summary sammeln (AUC/F1 Test + OOF zur Übersicht)\n",
    "    with open(metrics_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = json.load(f)\n",
    "    results_summary[set_name] = {\n",
    "        \"oof_auc\": m[\"oof_auc\"],\n",
    "        \"oof_f1\": m[\"oof_f1\"],\n",
    "        \"test_auc\": m[\"test_auc\"],\n",
    "        \"test_f1\": m[\"test_f1\"],\n",
    "        \"test_acc\": m[\"test_acc\"],\n",
    "    }\n",
    "\n",
    "# Übersicht als CSV + Balkenplot\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary).T.sort_index()\n",
    "summary_csv = METRICS_DIR / \"linear_logit_overview.csv\"\n",
    "summary_df.to_csv(summary_csv)\n",
    "logger.info(f\"Übersicht gespeichert: {summary_csv}\")\n",
    "\n",
    "overview_png = plot_bar_metrics_overview(summary_df)\n",
    "\n",
    "# Fertig\n",
    "\n",
    "logger.info(\"=== Done: Lineare Klassifikatoren (Logit) für TECH/MACRO/INTEGRATED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20229905-4424-4e61-acfa-949ef08a977c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KI-Aktienprognose",
   "language": "python",
   "name": "ki-aktienprognose-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
