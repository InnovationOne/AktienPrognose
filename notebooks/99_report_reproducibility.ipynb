{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9ce1f-d39d-469f-b8ca-9591a0ac4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step0 Setup\n",
    "from pathlib import Path\n",
    "import sys, json, yaml, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Find project root (expects 'config' and 'src' directories)\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p/\"config\").exists() and (p/\"src\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Project root with 'config' and 'src' not found.\")\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "\n",
    "# Artifact directories\n",
    "ARTIFACT_DIRS = [\n",
    "    ROOT/\"artifacts/data\",\n",
    "    ROOT/\"artifacts/models\",\n",
    "    ROOT/\"artifacts/forecasts\",\n",
    "    ROOT/\"artifacts/metrics\",\n",
    "    ROOT/\"artifacts/reports\",\n",
    "    ROOT/\"artifacts/tmp\"\n",
    "]\n",
    "for d in ARTIFACT_DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Utility to load YAML config\n",
    "def _load_config(cfg_path: Path) -> dict:\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Config not found: {cfg_path}\")\n",
    "    return yaml.safe_load(cfg_path.read_text()) or {}\n",
    "\n",
    "# Load data and training config for metadata\n",
    "data_cfg = _load_config(ROOT/\"config\"/\"data_config.yaml\")\n",
    "train_cfg = _load_config(ROOT/\"config\"/\"train_config.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedb0d7-35ab-4b03-80aa-bea7e8caab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1 Artefakte laden & normalisieren\n",
    "# Load feature groups (TECH, MACRO etc)\n",
    "feat_groups = yaml.safe_load((ROOT/\"artifacts/config\"/\"feature_groups.yaml\").read_text())\n",
    "\n",
    "# Load and consolidate metrics JSONs\n",
    "metrics_list = []\n",
    "metrics_dir = ROOT/\"artifacts\"/\"metrics\"\n",
    "for f in metrics_dir.glob(\"*.json\"):\n",
    "    data = json.load(f.open())\n",
    "    # If nested, get test metrics (assumption)\n",
    "    if \"test\" in data:\n",
    "        m = data[\"test\"]\n",
    "    else:\n",
    "        m = data\n",
    "    # Add model identifier\n",
    "    m[\"model_name\"] = f.stem\n",
    "    metrics_list.append(m)\n",
    "metrics_df = pd.json_normalize(metrics_list)\n",
    "# Ensure relevant columns\n",
    "for col in [\"accuracy\",\"f1\",\"auc\",\"brier\",\"sharpe\",\"sortino\",\"rachev\"]:\n",
    "    if col not in metrics_df.columns:\n",
    "        metrics_df[col] = np.nan\n",
    "\n",
    "# Identify model class by model_name\n",
    "def assign_class(name: str) -> str:\n",
    "    name_low = name.lower()\n",
    "    if any(x in name_low for x in [\"naive\",\"persistence\",\"baseline\",\"randomwalk\"]):\n",
    "        return \"Baseline\"\n",
    "    if \"linear\" in name_low or \"ridge\" in name_low or \"ols\" in name_low:\n",
    "        return \"Linear\"\n",
    "    if \"arima\" in name_low:\n",
    "        return \"ARIMA\"\n",
    "    if \"forest\" in name_low or \"random\" in name_low or \"rf\" in name_low:\n",
    "        return \"RandomForest\"\n",
    "    if \"lstm\" in name_low:\n",
    "        return \"LSTM\"\n",
    "    if \"ensemble\" in name_low:\n",
    "        return \"Ensemble\"\n",
    "    return \"Other\"\n",
    "metrics_df[\"model_class\"] = metrics_df[\"model_name\"].apply(assign_class)\n",
    "\n",
    "# Load forecast CSVs (predictions)\n",
    "forecasts = []\n",
    "for f in (ROOT/\"artifacts\"/\"forecasts\").glob(\"*.csv\"):\n",
    "    df = pd.read_csv(f, parse_dates=True, index_col=0)\n",
    "    df[\"model_name\"] = f.stem\n",
    "    forecasts.append(df)\n",
    "forecasts_df = pd.concat(forecasts, ignore_index=False) if forecasts else pd.DataFrame()\n",
    "\n",
    "# Load feature importances / coefficients\n",
    "fi_list = []\n",
    "fi_dir = ROOT/\"artifacts\"/\"feature_importance\"\n",
    "for f in fi_dir.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"model_name\"] = f.stem\n",
    "    fi_list.append(df)\n",
    "fi_df = pd.concat(fi_list, ignore_index=True) if fi_list else pd.DataFrame()\n",
    "# Normalize importances per model\n",
    "if not fi_df.empty and \"importance\" in fi_df.columns:\n",
    "    fi_df[\"importance\"] = fi_df.groupby(\"model_name\")[\"importance\"].transform(lambda x: x / x.sum())\n",
    "# Assume fi_df columns: feature, importance, model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d8134-22d5-4b0f-9b3f-5fa4e0e3df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 F2 – Beste Modellklasse\n",
    "# Ranking by metrics (higher is better except Brier where lower is better)\n",
    "metrics_sorted = {}\n",
    "for metric in [\"accuracy\",\"f1\",\"auc\",\"sharpe\",\"sortino\",\"rachev\"]:\n",
    "    metrics_sorted[metric] = metrics_df.sort_values(metric, ascending=False)[\"model_name\"].tolist()\n",
    "metrics_sorted[\"brier\"] = metrics_df.sort_values(\"brier\", ascending=True)[\"model_name\"].tolist()\n",
    "\n",
    "# Class aggregation: compute mean, median and bootstrap CIs for each metric\n",
    "agg_stats = []\n",
    "np.random.seed(42)\n",
    "for cls, group in metrics_df.groupby(\"model_class\"):\n",
    "    metrics_vals = group[[\"accuracy\",\"f1\",\"auc\",\"brier\",\"sharpe\",\"sortino\",\"rachev\"]]\n",
    "    row = {\"model_class\": cls}\n",
    "    for col in [\"accuracy\",\"f1\",\"auc\",\"brier\",\"sharpe\",\"sortino\",\"rachev\"]:\n",
    "        vals = group[col].dropna().values\n",
    "        if len(vals)==0:\n",
    "            row.update({f\"{col}_mean\": np.nan, f\"{col}_med\": np.nan, f\"{col}_ci_low\": np.nan, f\"{col}_ci_high\": np.nan})\n",
    "            continue\n",
    "        mean = vals.mean()\n",
    "        med = np.median(vals)\n",
    "        # Bootstrap for mean CI\n",
    "        boots = []\n",
    "        for _ in range(1000):\n",
    "            sample = np.random.choice(vals, size=len(vals), replace=True)\n",
    "            boots.append(sample.mean())\n",
    "        ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n",
    "        row[f\"{col}_mean\"] = mean\n",
    "        row[f\"{col}_med\"] = med\n",
    "        row[f\"{col}_ci_low\"] = ci_low\n",
    "        row[f\"{col}_ci_high\"] = ci_high\n",
    "    agg_stats.append(row)\n",
    "by_class_df = pd.DataFrame(agg_stats)\n",
    "\n",
    "# Save by_model and by_class master tables\n",
    "metrics_export = metrics_df.copy()\n",
    "metrics_export = metrics_export[[\"model_name\",\"model_class\",\"accuracy\",\"f1\",\"auc\",\"brier\",\"sharpe\",\"sortino\",\"rachev\"]]\n",
    "(metrics_export).to_csv(ROOT/\"artifacts\"/\"reports\"/\"99_master\"/\"by_model.csv\", index=False)\n",
    "(by_class_df).to_csv(ROOT/\"artifacts\"/\"reports\"/\"99_master\"/\"by_class.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d36f5c-6cbb-4840-89b7-137d514224e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 F1 – Integriert vs. isoliert\n",
    "# Identify feature set (assuming model names encode this, e.g., containing 'Tech' or 'Macro')\n",
    "def feature_set(name: str) -> str:\n",
    "    nl = name.lower()\n",
    "    if \"tech\" in nl and \"macro\" in nl:\n",
    "        return \"Integrated\"\n",
    "    if \"tech\" in nl:\n",
    "        return \"Tech\"\n",
    "    if \"macro\" in nl:\n",
    "        return \"Macro\"\n",
    "    return \"Integrated\"  # default to integrated if unclear\n",
    "\n",
    "metrics_df[\"featureset\"] = metrics_df[\"model_name\"].apply(feature_set)\n",
    "\n",
    "# Compare integrated vs Tech/Macro for each class\n",
    "f1_diff_rows = []\n",
    "for cls in metrics_df[\"model_class\"].unique():\n",
    "    df_cls = metrics_df[metrics_df[\"model_class\"] == cls]\n",
    "    int_vals = df_cls[df_cls[\"featureset\"]==\"Integrated\"]\n",
    "    tech_vals = df_cls[df_cls[\"featureset\"]==\"Tech\"]\n",
    "    mac_vals = df_cls[df_cls[\"featureset\"]==\"Macro\"]\n",
    "    # Compute means if multiple models per setting\n",
    "    int_mean = int_vals[[\"f1\",\"auc\",\"sharpe\"]].mean()\n",
    "    tech_mean = tech_vals[[\"f1\",\"auc\",\"sharpe\"]].mean()\n",
    "    mac_mean = mac_vals[[\"f1\",\"auc\",\"sharpe\"]].mean()\n",
    "    if not int_mean.empty and not tech_mean.empty:\n",
    "        df_res = {\n",
    "            \"model_class\": cls,\n",
    "            \"ΔF1_vs_Tech\": float(int_mean[\"f1\"] - tech_mean[\"f1\"]),\n",
    "            \"ΔAUC_vs_Tech\": float(int_mean[\"auc\"] - tech_mean[\"auc\"]),\n",
    "            \"ΔSharpe_vs_Tech\": float(int_mean[\"sharpe\"] - tech_mean[\"sharpe\"]),\n",
    "            \"ΔF1_vs_Macro\": float(int_mean[\"f1\"] - mac_mean[\"f1\"]) if not mac_mean.empty else np.nan,\n",
    "            \"ΔAUC_vs_Macro\": float(int_mean[\"auc\"] - mac_mean[\"auc\"]) if not mac_mean.empty else np.nan,\n",
    "            \"ΔSharpe_vs_Macro\": float(int_mean[\"sharpe\"] - mac_mean[\"sharpe\"]) if not mac_mean.empty else np.nan\n",
    "        }\n",
    "    else:\n",
    "        df_res = {\"model_class\": cls,\n",
    "                  \"ΔF1_vs_Tech\": np.nan, \"ΔAUC_vs_Tech\": np.nan, \"ΔSharpe_vs_Tech\": np.nan,\n",
    "                  \"ΔF1_vs_Macro\": np.nan, \"ΔAUC_vs_Macro\": np.nan, \"ΔSharpe_vs_Macro\": np.nan}\n",
    "    # Placeholder for Diebold-Mariano and McNemar p-values\n",
    "    # Actual calculation would require model predictions/losses\n",
    "    df_res.update({\n",
    "        \"DM_pval_Tech\": np.nan,\n",
    "        \"DM_pval_Macro\": np.nan,\n",
    "        \"McNemar_pval_Tech\": np.nan,\n",
    "        \"McNemar_pval_Macro\": np.nan\n",
    "    })\n",
    "    f1_diff_rows.append(df_res)\n",
    "f1_diff_df = pd.DataFrame(f1_diff_rows)\n",
    "f1_diff_df.to_csv(ROOT/\"artifacts\"/\"reports\"/\"99_master\"/\"f1_integrated_vs_isolated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5750564-9379-41d0-8f71-364d37519ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4 F3 – Merkmalsrelevanz\n",
    "if not fi_df.empty:\n",
    "    # Merge model class into feature importances\n",
    "    class_map = metrics_df.set_index(\"model_name\")[\"model_class\"].to_dict()\n",
    "    fi_df[\"model_class\"] = fi_df[\"model_name\"].map(class_map)\n",
    "    fi_df[\"featureset\"] = fi_df[\"model_name\"].apply(feature_set)\n",
    "    # Calculate total tech vs macro importance in integrated models\n",
    "    tech_feats = set(feat_groups.get(\"TECH\", []))\n",
    "    macro_feats = set(feat_groups.get(\"MACRO\", []))\n",
    "    fi_int = fi_df[fi_df[\"featureset\"]==\"Integrated\"]\n",
    "    tech_importance = fi_int[fi_int[\"feature\"].isin(tech_feats)][\"importance\"].sum()\n",
    "    macro_importance = fi_int[fi_int[\"feature\"].isin(macro_feats)][\"importance\"].sum()\n",
    "    macro_vs_tech = (macro_importance / tech_importance) if tech_importance>0 else np.nan\n",
    "\n",
    "    # Aggregate importance by feature and model class\n",
    "    agg_imp = fi_df.groupby([\"feature\",\"model_class\"])[\"importance\"].sum().unstack(fill_value=0)\n",
    "    agg_imp = agg_imp.reset_index().rename_axis(None, axis=1)\n",
    "    # Determine top-10 features per class (Integrated models only)\n",
    "    top_feats = {}\n",
    "    for cls in fi_df[\"model_class\"].unique():\n",
    "        df_cls = fi_df[(fi_df[\"model_class\"]==cls)&(fi_df[\"featureset\"]==\"Integrated\")]\n",
    "        top_feats[cls] = list(df_cls.groupby(\"feature\")[\"importance\"].sum().nlargest(10).index)\n",
    "    # Consensus top features (those appearing in at least 3 classes' top lists)\n",
    "    from collections import Counter\n",
    "    all_top = [f for feats in top_feats.values() for f in feats]\n",
    "    consensus = [f for f,c in Counter(all_top).items() if c >= 3]\n",
    "    # Export aggregated importances\n",
    "    agg_imp.to_csv(ROOT/\"artifacts\"/\"reports\"/\"99_master\"/\"f3_aggregated_importance.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf21d33-815d-4978-89e3-d04053b3e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5 Visualisierungen\n",
    "# 5.1 Heatmap: Model classes vs Metrics (mean of each class)\n",
    "heat_data = by_class_df.set_index(\"model_class\")[[c+\"_mean\" for c in [\"accuracy\",\"f1\",\"auc\",\"brier\",\"sharpe\",\"sortino\",\"rachev\"]]]\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(heat_data, annot=True, fmt=\".3f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Model Class Performance Metrics (Mean ± CI)\")\n",
    "plt.savefig(ROOT/\"artifacts\"/\"reports\"/\"99_metrics_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# 5.2 Cumulative returns: placeholder logic\n",
    "# (Assumes forecasts_df has columns 'model_name','y_return_next_pct','y_direction_next')\n",
    "if not forecasts_df.empty:\n",
    "    # Example: cumulative returns of best single model vs best ensemble vs buy&hold\n",
    "    # Identify best model by Sharpe\n",
    "    best_model = metrics_df.loc[metrics_df[\"sharpe\"].idxmax()][\"model_name\"]\n",
    "    # Compute strategy returns (assuming y_direction next as strategy signal)\n",
    "    forecasts_df[\"strategy_return\"] = forecasts_df[\"y_ret_next\"] * (2*forecasts_df[\"y_direction_next\"]-1)\n",
    "    cum_returns = {}\n",
    "    for model in [best_model, \"Ensemble_Best\", \"BuyHold\"]:\n",
    "        if model in forecasts_df[\"model_name\"].values:\n",
    "            dfm = forecasts_df[forecasts_df[\"model_name\"]==model]\n",
    "            cum_returns[model] = (1+dfm[\"strategy_return\"]).cumprod()\n",
    "    # Plot cumulative returns\n",
    "    plt.figure()\n",
    "    for model, series in cum_returns.items():\n",
    "        plt.plot(series.index, series.values, label=model)\n",
    "    plt.legend(); plt.title(\"Cumulative Returns\")\n",
    "    plt.savefig(ROOT/\"artifacts\"/\"reports\"/\"99_cum_returns.png\")\n",
    "    plt.close()\n",
    "\n",
    "# 5.3 Bar plot: Integrated advantage per class for ΔF1, ΔAUC, ΔSharpe\n",
    "plt.figure(figsize=(6,4))\n",
    "metrics = [\"ΔF1_vs_Tech\",\"ΔAUC_vs_Tech\",\"ΔSharpe_vs_Tech\"]\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar(np.arange(len(f1_diff_df))+i*0.2, f1_diff_df[metric], width=0.2, label=metric)\n",
    "plt.xticks(np.arange(len(f1_diff_df))+0.2, f1_diff_df[\"model_class\"])\n",
    "plt.ylabel(\"Integrated - Tech\")\n",
    "plt.legend(); plt.title(\"Integrated vs Tech Advantage per Class\")\n",
    "plt.savefig(ROOT/\"artifacts\"/\"reports\"/\"99_integrated_advantage.png\")\n",
    "plt.close()\n",
    "\n",
    "# 5.4 Top features (Integrated) aggregated by class\n",
    "# Using consensus features importance\n",
    "plt.figure(figsize=(6,4))\n",
    "mean_imp = agg_imp.copy().set_index(\"feature\")\n",
    "mean_imp = mean_imp.loc[consensus] if consensus else mean_imp\n",
    "mean_imp = mean_imp.div(mean_imp.sum(axis=0), axis=1)  # relative importance\n",
    "mean_imp.plot(kind=\"bar\")\n",
    "plt.ylabel(\"Importance\") \n",
    "plt.title(\"Top Features (Integrated) by Class\")\n",
    "plt.savefig(ROOT/\"artifacts\"/\"reports\"/\"99_top_features.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e46b48-0a45-44aa-b3ad-8397bfcdc7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step6 Repro-Block\n",
    "print(\"Date range:\", data_cfg.get(\"dataset\",{}).get(\"start_date\"), \"→\", data_cfg.get(\"dataset\",{}).get(\"end_date\"))\n",
    "print(\"Seed:\", np.random.get_state()[1][0])\n",
    "print(f\"Library versions: pandas {pd.__version__}, numpy {np.__version__}\")\n",
    "if \"train_years\" in train_cfg:\n",
    "    print(\"Training window (years):\", train_cfg[\"train_years\"])\n",
    "print(\"Validation step (months):\", train_cfg.get(\"step_months\"))\n",
    "print(\"Embargo period (months):\", train_cfg.get(\"embargo_months\"))\n",
    "# List used artifact files\n",
    "used_files = list((ROOT/\"artifacts\"/\"metrics\").glob(\"*.json\")) \\\n",
    "           + list((ROOT/\"artifacts\"/\"forecasts\").glob(\"*.csv\")) \\\n",
    "           + list((ROOT/\"artifacts\"/\"feature_importance\").glob(\"*.csv\"))\n",
    "print(\"Loaded artifact files:\")\n",
    "for f in used_files:\n",
    "    print(f.relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505a3ab-e6d1-4001-b5f6-8ea35944e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7 Übersicht 99 & Exporte\n",
    "# Determine best model class (highest mean AUC)\n",
    "best_class = by_class_df.loc[by_class_df[\"auc_mean\"].idxmax()][\"model_class\"]\n",
    "integrated_gains = {}\n",
    "for metric in [\"f1\",\"auc\",\"sharpe\"]:\n",
    "    col = \"Δ\" + metric.upper() + \"_vs_Tech\"\n",
    "    integrated_gains[metric] = f1_diff_df[col].mean()\n",
    "top_feats_integrated = consensus  # from earlier consensus list\n",
    "macro_vs_tech_contrib = macro_vs_tech\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"best_model_class\": best_class,\n",
    "    \"integrated_gain_f1\": integrated_gains.get(\"f1\"),\n",
    "    \"integrated_gain_auc\": integrated_gains.get(\"auc\"),\n",
    "    \"integrated_gain_sharpe\": integrated_gains.get(\"sharpe\"),\n",
    "    \"top_features_integrated\": \";\".join(top_feats_integrated) if top_feats_integrated else \"\",\n",
    "    \"macro_vs_tech_contrib\": macro_vs_tech_contrib\n",
    "}])\n",
    "summary.to_csv(ROOT/\"artifacts\"/\"reports\"/\"99_summary_overview.csv\", index=False)\n",
    "# Save summary as image (table)\n",
    "fig, ax = plt.subplots(figsize=(6,1))\n",
    "ax.axis(\"off\"); ax.table(cellText=summary.values, colLabels=summary.columns, loc=\"center\")\n",
    "plt.savefig(ROOT/\"artifacts\"/\"reports\"/\"99_summary_overview.png\")\n",
    "plt.close()\n",
    "\n",
    "# (Optional) Generate a minimal HTML report\n",
    "html_content = f\"\"\"<html><body>\n",
    "<h2>Model Evaluation Summary</h2>\n",
    "{summary.to_html(index=False)}\n",
    "</body></html>\"\"\"\n",
    "with open(ROOT/\"artifacts\"/\"reports\"/\"99_summary.html\", \"w\") as f:\n",
    "    f.write(html_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
